{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZezVHg_Mtibd"
      },
      "source": [
        "# Dataset CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "Ak79WoG3MSLC",
        "outputId": "34c4aea9-14e4-4cc7-baf6-5874cbb0085f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          plane             cat             car             cat             cat             car           truck            deer\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 1150,
              "height": 179
            },
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(32, 4),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]), download=True),\n",
        "        batch_size=128, shuffle=True,\n",
        "        num_workers=4, pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])),\n",
        "        batch_size=128, shuffle=False,\n",
        "        num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# functions to show an image\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "plt.figure(figsize=(20,10)) \n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images[0:8,:,:]))\n",
        "# print labels\n",
        "print(' '.join('%15s' % classes[labels[j]] for j in range(8)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIZ3VjmtrDGN"
      },
      "source": [
        "# ResNet20 Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3NNDgGK9-FF"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "    \n",
        "    # return 3x3 Conv2d\n",
        "\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3,stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    # Initialize basic ResidualBlock with forward propogation\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(out_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \n",
        "    # Initialize  ResNet with forward propogation\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv = conv3x3(3, 16)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
        "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
        "        self.avg_pool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if (stride != 1) or (self.in_channels != out_channels):\n",
        "            downsample = nn.Sequential(\n",
        "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels))\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1QjCJxctt5Z"
      },
      "source": [
        "# Accuracy function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IEJyaFfu67X"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "\n",
        "    #Computes and stores the average and current value\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "\n",
        "    #Computes the accuracy\n",
        "    \n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "R4O7J7PZA-CG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiFq2l26kFkk"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    \n",
        "    # Run one train epoch\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        target = target.cuda()\n",
        "        input_var = input.cuda()\n",
        "        target_var = target\n",
        "\n",
        "        # compute output\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        output = output.float()\n",
        "        loss = loss.float()\n",
        "        \n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target)[0]\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 55 == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                      data_time=data_time, loss=losses, top1=top1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation loop"
      ],
      "metadata": {
        "id": "Pr2ETY-tBKd3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSvEeiZwkFhL"
      },
      "outputs": [],
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    \n",
        "    # Run evaluation\n",
        "  \n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "\n",
        "            target = target.cuda()\n",
        "            input_var = input.cuda()\n",
        "            target_var = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input_var)\n",
        "            loss = criterion(output, target_var)\n",
        "\n",
        "            output = output.float()\n",
        "            loss = loss.float()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(output.data, target)[0]\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "\n",
        "    print('Test\\t  Prec@1: {top1.avg:.3f} (Err: {error:.3f} )\\n'\n",
        "          .format(top1=top1,error=100-top1.avg))\n",
        "\n",
        "    return top1.avg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "7s_J5dhtBQhM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qopdhRqHkFea",
        "outputId": "01e6b6ca-d377-4648-ca9b-b7ef7c5807ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [0][0/391]\tLoss 2.3094 (2.3094)\tPrec@1 10.938 (10.938)\n",
            "Epoch: [0][55/391]\tLoss 1.8240 (2.0695)\tPrec@1 26.562 (22.238)\n",
            "Epoch: [0][110/391]\tLoss 1.8549 (1.9614)\tPrec@1 39.062 (25.612)\n",
            "Epoch: [0][165/391]\tLoss 1.6789 (1.9017)\tPrec@1 34.375 (27.923)\n",
            "Epoch: [0][220/391]\tLoss 1.4253 (1.8494)\tPrec@1 48.438 (29.931)\n",
            "Epoch: [0][275/391]\tLoss 1.5943 (1.8005)\tPrec@1 46.094 (32.088)\n",
            "Epoch: [0][330/391]\tLoss 1.4946 (1.7613)\tPrec@1 43.750 (33.691)\n",
            "Epoch: [0][385/391]\tLoss 1.5170 (1.7190)\tPrec@1 41.406 (35.304)\n",
            "Test\t  Prec@1: 44.320 (Err: 55.680 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [1][0/391]\tLoss 1.3306 (1.3306)\tPrec@1 50.781 (50.781)\n",
            "Epoch: [1][55/391]\tLoss 1.3508 (1.3631)\tPrec@1 50.000 (50.321)\n",
            "Epoch: [1][110/391]\tLoss 1.4697 (1.3276)\tPrec@1 45.312 (51.858)\n",
            "Epoch: [1][165/391]\tLoss 1.2923 (1.2939)\tPrec@1 50.000 (53.389)\n",
            "Epoch: [1][220/391]\tLoss 1.3838 (1.2671)\tPrec@1 49.219 (54.550)\n",
            "Epoch: [1][275/391]\tLoss 1.0213 (1.2389)\tPrec@1 58.594 (55.511)\n",
            "Epoch: [1][330/391]\tLoss 1.0209 (1.2147)\tPrec@1 61.719 (56.377)\n",
            "Epoch: [1][385/391]\tLoss 0.8953 (1.1862)\tPrec@1 66.406 (57.438)\n",
            "Test\t  Prec@1: 60.140 (Err: 39.860 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [2][0/391]\tLoss 0.8766 (0.8766)\tPrec@1 68.750 (68.750)\n",
            "Epoch: [2][55/391]\tLoss 1.0104 (0.9820)\tPrec@1 69.531 (65.346)\n",
            "Epoch: [2][110/391]\tLoss 0.7983 (0.9456)\tPrec@1 67.188 (66.610)\n",
            "Epoch: [2][165/391]\tLoss 0.8374 (0.9425)\tPrec@1 75.781 (66.764)\n",
            "Epoch: [2][220/391]\tLoss 1.0269 (0.9319)\tPrec@1 64.844 (67.166)\n",
            "Epoch: [2][275/391]\tLoss 0.9110 (0.9125)\tPrec@1 72.656 (67.856)\n",
            "Epoch: [2][330/391]\tLoss 0.8204 (0.9024)\tPrec@1 74.219 (68.228)\n",
            "Epoch: [2][385/391]\tLoss 0.7300 (0.8878)\tPrec@1 71.875 (68.720)\n",
            "Test\t  Prec@1: 70.580 (Err: 29.420 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [3][0/391]\tLoss 0.6573 (0.6573)\tPrec@1 77.344 (77.344)\n",
            "Epoch: [3][55/391]\tLoss 0.6916 (0.7689)\tPrec@1 78.906 (73.270)\n",
            "Epoch: [3][110/391]\tLoss 0.9166 (0.7710)\tPrec@1 66.406 (73.283)\n",
            "Epoch: [3][165/391]\tLoss 0.7845 (0.7552)\tPrec@1 74.219 (73.762)\n",
            "Epoch: [3][220/391]\tLoss 0.7063 (0.7550)\tPrec@1 73.438 (73.671)\n",
            "Epoch: [3][275/391]\tLoss 0.8154 (0.7490)\tPrec@1 69.531 (73.899)\n",
            "Epoch: [3][330/391]\tLoss 0.6569 (0.7426)\tPrec@1 79.688 (74.039)\n",
            "Epoch: [3][385/391]\tLoss 0.6087 (0.7363)\tPrec@1 79.688 (74.281)\n",
            "Test\t  Prec@1: 75.320 (Err: 24.680 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [4][0/391]\tLoss 0.6410 (0.6410)\tPrec@1 75.000 (75.000)\n",
            "Epoch: [4][55/391]\tLoss 0.4514 (0.6728)\tPrec@1 83.594 (76.535)\n",
            "Epoch: [4][110/391]\tLoss 0.6421 (0.6635)\tPrec@1 78.906 (77.076)\n",
            "Epoch: [4][165/391]\tLoss 0.6743 (0.6566)\tPrec@1 76.562 (77.160)\n",
            "Epoch: [4][220/391]\tLoss 0.6669 (0.6543)\tPrec@1 82.031 (77.393)\n",
            "Epoch: [4][275/391]\tLoss 0.5512 (0.6508)\tPrec@1 79.688 (77.440)\n",
            "Epoch: [4][330/391]\tLoss 0.4215 (0.6442)\tPrec@1 85.938 (77.639)\n",
            "Epoch: [4][385/391]\tLoss 0.7456 (0.6397)\tPrec@1 75.781 (77.819)\n",
            "Test\t  Prec@1: 65.880 (Err: 34.120 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [5][0/391]\tLoss 0.5540 (0.5540)\tPrec@1 77.344 (77.344)\n",
            "Epoch: [5][55/391]\tLoss 0.5573 (0.6121)\tPrec@1 78.906 (78.934)\n",
            "Epoch: [5][110/391]\tLoss 0.5763 (0.5947)\tPrec@1 83.594 (79.413)\n",
            "Epoch: [5][165/391]\tLoss 0.6018 (0.5933)\tPrec@1 78.125 (79.438)\n",
            "Epoch: [5][220/391]\tLoss 0.8444 (0.5898)\tPrec@1 74.219 (79.525)\n",
            "Epoch: [5][275/391]\tLoss 0.5042 (0.5865)\tPrec@1 81.250 (79.690)\n",
            "Epoch: [5][330/391]\tLoss 0.5925 (0.5866)\tPrec@1 83.594 (79.756)\n",
            "Epoch: [5][385/391]\tLoss 0.5767 (0.5846)\tPrec@1 77.344 (79.866)\n",
            "Test\t  Prec@1: 75.580 (Err: 24.420 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [6][0/391]\tLoss 0.4783 (0.4783)\tPrec@1 82.031 (82.031)\n",
            "Epoch: [6][55/391]\tLoss 0.5055 (0.5301)\tPrec@1 84.375 (81.390)\n",
            "Epoch: [6][110/391]\tLoss 0.6528 (0.5459)\tPrec@1 74.219 (80.771)\n",
            "Epoch: [6][165/391]\tLoss 0.7411 (0.5488)\tPrec@1 77.344 (80.836)\n",
            "Epoch: [6][220/391]\tLoss 0.4773 (0.5490)\tPrec@1 83.594 (80.914)\n",
            "Epoch: [6][275/391]\tLoss 0.6740 (0.5497)\tPrec@1 79.688 (80.825)\n",
            "Epoch: [6][330/391]\tLoss 0.7480 (0.5517)\tPrec@1 72.656 (80.790)\n",
            "Epoch: [6][385/391]\tLoss 0.6660 (0.5461)\tPrec@1 75.781 (80.961)\n",
            "Test\t  Prec@1: 77.620 (Err: 22.380 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [7][0/391]\tLoss 0.5811 (0.5811)\tPrec@1 80.469 (80.469)\n",
            "Epoch: [7][55/391]\tLoss 0.4793 (0.5392)\tPrec@1 83.594 (81.362)\n",
            "Epoch: [7][110/391]\tLoss 0.5753 (0.5232)\tPrec@1 82.031 (81.954)\n",
            "Epoch: [7][165/391]\tLoss 0.4205 (0.5189)\tPrec@1 87.500 (81.984)\n",
            "Epoch: [7][220/391]\tLoss 0.6421 (0.5208)\tPrec@1 79.688 (82.060)\n",
            "Epoch: [7][275/391]\tLoss 0.5948 (0.5175)\tPrec@1 78.906 (82.181)\n",
            "Epoch: [7][330/391]\tLoss 0.5183 (0.5141)\tPrec@1 78.125 (82.199)\n",
            "Epoch: [7][385/391]\tLoss 0.4426 (0.5115)\tPrec@1 84.375 (82.260)\n",
            "Test\t  Prec@1: 80.840 (Err: 19.160 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [8][0/391]\tLoss 0.5736 (0.5736)\tPrec@1 80.469 (80.469)\n",
            "Epoch: [8][55/391]\tLoss 0.4695 (0.4751)\tPrec@1 82.031 (83.468)\n",
            "Epoch: [8][110/391]\tLoss 0.4059 (0.4782)\tPrec@1 90.625 (83.340)\n",
            "Epoch: [8][165/391]\tLoss 0.6853 (0.4814)\tPrec@1 77.344 (83.213)\n",
            "Epoch: [8][220/391]\tLoss 0.4956 (0.4869)\tPrec@1 86.719 (83.021)\n",
            "Epoch: [8][275/391]\tLoss 0.3613 (0.4838)\tPrec@1 87.500 (83.149)\n",
            "Epoch: [8][330/391]\tLoss 0.5121 (0.4835)\tPrec@1 78.906 (83.195)\n",
            "Epoch: [8][385/391]\tLoss 0.5827 (0.4804)\tPrec@1 78.125 (83.341)\n",
            "Test\t  Prec@1: 79.500 (Err: 20.500 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [9][0/391]\tLoss 0.5341 (0.5341)\tPrec@1 82.031 (82.031)\n",
            "Epoch: [9][55/391]\tLoss 0.5154 (0.4662)\tPrec@1 80.469 (84.222)\n",
            "Epoch: [9][110/391]\tLoss 0.4000 (0.4650)\tPrec@1 84.375 (83.925)\n",
            "Epoch: [9][165/391]\tLoss 0.4352 (0.4613)\tPrec@1 85.156 (84.111)\n",
            "Epoch: [9][220/391]\tLoss 0.5038 (0.4611)\tPrec@1 82.812 (84.110)\n",
            "Epoch: [9][275/391]\tLoss 0.4840 (0.4609)\tPrec@1 79.688 (84.146)\n",
            "Epoch: [9][330/391]\tLoss 0.3842 (0.4591)\tPrec@1 85.938 (84.205)\n",
            "Epoch: [9][385/391]\tLoss 0.4132 (0.4600)\tPrec@1 83.594 (84.146)\n",
            "Test\t  Prec@1: 81.560 (Err: 18.440 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [10][0/391]\tLoss 0.3289 (0.3289)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [10][55/391]\tLoss 0.3313 (0.4054)\tPrec@1 89.062 (85.784)\n",
            "Epoch: [10][110/391]\tLoss 0.5055 (0.4321)\tPrec@1 86.719 (84.966)\n",
            "Epoch: [10][165/391]\tLoss 0.5501 (0.4321)\tPrec@1 81.250 (84.973)\n",
            "Epoch: [10][220/391]\tLoss 0.5470 (0.4408)\tPrec@1 79.688 (84.654)\n",
            "Epoch: [10][275/391]\tLoss 0.6380 (0.4389)\tPrec@1 81.250 (84.785)\n",
            "Epoch: [10][330/391]\tLoss 0.4919 (0.4418)\tPrec@1 80.469 (84.698)\n",
            "Epoch: [10][385/391]\tLoss 0.3153 (0.4402)\tPrec@1 89.062 (84.735)\n",
            "Test\t  Prec@1: 81.790 (Err: 18.210 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [11][0/391]\tLoss 0.4293 (0.4293)\tPrec@1 84.375 (84.375)\n",
            "Epoch: [11][55/391]\tLoss 0.5029 (0.4419)\tPrec@1 85.938 (85.017)\n",
            "Epoch: [11][110/391]\tLoss 0.4409 (0.4324)\tPrec@1 82.031 (84.882)\n",
            "Epoch: [11][165/391]\tLoss 0.4160 (0.4318)\tPrec@1 88.281 (85.001)\n",
            "Epoch: [11][220/391]\tLoss 0.3721 (0.4308)\tPrec@1 89.062 (85.001)\n",
            "Epoch: [11][275/391]\tLoss 0.3589 (0.4268)\tPrec@1 89.844 (85.111)\n",
            "Epoch: [11][330/391]\tLoss 0.4314 (0.4252)\tPrec@1 82.031 (85.161)\n",
            "Epoch: [11][385/391]\tLoss 0.4546 (0.4248)\tPrec@1 82.812 (85.174)\n",
            "Test\t  Prec@1: 79.920 (Err: 20.080 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [12][0/391]\tLoss 0.3429 (0.3429)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [12][55/391]\tLoss 0.4479 (0.3981)\tPrec@1 85.938 (86.091)\n",
            "Epoch: [12][110/391]\tLoss 0.5019 (0.4081)\tPrec@1 82.812 (86.113)\n",
            "Epoch: [12][165/391]\tLoss 0.2733 (0.4125)\tPrec@1 91.406 (85.857)\n",
            "Epoch: [12][220/391]\tLoss 0.4264 (0.4099)\tPrec@1 86.719 (85.920)\n",
            "Epoch: [12][275/391]\tLoss 0.4906 (0.4103)\tPrec@1 85.938 (85.901)\n",
            "Epoch: [12][330/391]\tLoss 0.3442 (0.4099)\tPrec@1 87.500 (85.881)\n",
            "Epoch: [12][385/391]\tLoss 0.3564 (0.4080)\tPrec@1 89.844 (85.911)\n",
            "Test\t  Prec@1: 82.100 (Err: 17.900 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [13][0/391]\tLoss 0.2750 (0.2750)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [13][55/391]\tLoss 0.4054 (0.3891)\tPrec@1 85.938 (86.705)\n",
            "Epoch: [13][110/391]\tLoss 0.4137 (0.3953)\tPrec@1 85.938 (86.318)\n",
            "Epoch: [13][165/391]\tLoss 0.2304 (0.3927)\tPrec@1 92.188 (86.385)\n",
            "Epoch: [13][220/391]\tLoss 0.3841 (0.3904)\tPrec@1 90.625 (86.471)\n",
            "Epoch: [13][275/391]\tLoss 0.3774 (0.3928)\tPrec@1 87.500 (86.407)\n",
            "Epoch: [13][330/391]\tLoss 0.5292 (0.3938)\tPrec@1 82.031 (86.386)\n",
            "Epoch: [13][385/391]\tLoss 0.3465 (0.3960)\tPrec@1 86.719 (86.356)\n",
            "Test\t  Prec@1: 83.750 (Err: 16.250 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [14][0/391]\tLoss 0.3664 (0.3664)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [14][55/391]\tLoss 0.4428 (0.3701)\tPrec@1 83.594 (87.081)\n",
            "Epoch: [14][110/391]\tLoss 0.4194 (0.3746)\tPrec@1 83.594 (87.028)\n",
            "Epoch: [14][165/391]\tLoss 0.2413 (0.3798)\tPrec@1 91.406 (86.742)\n",
            "Epoch: [14][220/391]\tLoss 0.3340 (0.3824)\tPrec@1 89.062 (86.645)\n",
            "Epoch: [14][275/391]\tLoss 0.4638 (0.3811)\tPrec@1 85.156 (86.733)\n",
            "Epoch: [14][330/391]\tLoss 0.4380 (0.3831)\tPrec@1 85.156 (86.702)\n",
            "Epoch: [14][385/391]\tLoss 0.4021 (0.3848)\tPrec@1 87.500 (86.705)\n",
            "Test\t  Prec@1: 83.650 (Err: 16.350 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [15][0/391]\tLoss 0.2850 (0.2850)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [15][55/391]\tLoss 0.2766 (0.3588)\tPrec@1 92.969 (87.570)\n",
            "Epoch: [15][110/391]\tLoss 0.3494 (0.3729)\tPrec@1 87.500 (87.064)\n",
            "Epoch: [15][165/391]\tLoss 0.4825 (0.3757)\tPrec@1 84.375 (86.954)\n",
            "Epoch: [15][220/391]\tLoss 0.3188 (0.3737)\tPrec@1 91.406 (87.076)\n",
            "Epoch: [15][275/391]\tLoss 0.3695 (0.3749)\tPrec@1 88.281 (87.030)\n",
            "Epoch: [15][330/391]\tLoss 0.3953 (0.3756)\tPrec@1 85.156 (87.026)\n",
            "Epoch: [15][385/391]\tLoss 0.3863 (0.3777)\tPrec@1 85.156 (86.943)\n",
            "Test\t  Prec@1: 83.960 (Err: 16.040 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [16][0/391]\tLoss 0.3078 (0.3078)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [16][55/391]\tLoss 0.3649 (0.3548)\tPrec@1 88.281 (87.709)\n",
            "Epoch: [16][110/391]\tLoss 0.3061 (0.3550)\tPrec@1 88.281 (87.873)\n",
            "Epoch: [16][165/391]\tLoss 0.5460 (0.3630)\tPrec@1 84.375 (87.448)\n",
            "Epoch: [16][220/391]\tLoss 0.4597 (0.3650)\tPrec@1 85.938 (87.302)\n",
            "Epoch: [16][275/391]\tLoss 0.4460 (0.3635)\tPrec@1 82.812 (87.356)\n",
            "Epoch: [16][330/391]\tLoss 0.2338 (0.3647)\tPrec@1 92.188 (87.321)\n",
            "Epoch: [16][385/391]\tLoss 0.3398 (0.3658)\tPrec@1 89.062 (87.312)\n",
            "Test\t  Prec@1: 81.530 (Err: 18.470 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [17][0/391]\tLoss 0.3397 (0.3397)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [17][55/391]\tLoss 0.4723 (0.3515)\tPrec@1 86.719 (88.058)\n",
            "Epoch: [17][110/391]\tLoss 0.3204 (0.3478)\tPrec@1 88.281 (87.950)\n",
            "Epoch: [17][165/391]\tLoss 0.2921 (0.3428)\tPrec@1 91.406 (88.182)\n",
            "Epoch: [17][220/391]\tLoss 0.4084 (0.3447)\tPrec@1 85.156 (87.998)\n",
            "Epoch: [17][275/391]\tLoss 0.3397 (0.3514)\tPrec@1 88.281 (87.707)\n",
            "Epoch: [17][330/391]\tLoss 0.3593 (0.3536)\tPrec@1 87.500 (87.635)\n",
            "Epoch: [17][385/391]\tLoss 0.4281 (0.3555)\tPrec@1 85.938 (87.553)\n",
            "Test\t  Prec@1: 81.330 (Err: 18.670 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [18][0/391]\tLoss 0.4433 (0.4433)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [18][55/391]\tLoss 0.2773 (0.3373)\tPrec@1 90.625 (88.058)\n",
            "Epoch: [18][110/391]\tLoss 0.3393 (0.3339)\tPrec@1 86.719 (88.415)\n",
            "Epoch: [18][165/391]\tLoss 0.2704 (0.3358)\tPrec@1 91.406 (88.206)\n",
            "Epoch: [18][220/391]\tLoss 0.3730 (0.3403)\tPrec@1 89.062 (88.175)\n",
            "Epoch: [18][275/391]\tLoss 0.4611 (0.3409)\tPrec@1 89.062 (88.168)\n",
            "Epoch: [18][330/391]\tLoss 0.4256 (0.3433)\tPrec@1 85.156 (88.069)\n",
            "Epoch: [18][385/391]\tLoss 0.3021 (0.3461)\tPrec@1 87.500 (88.018)\n",
            "Test\t  Prec@1: 85.720 (Err: 14.280 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [19][0/391]\tLoss 0.3399 (0.3399)\tPrec@1 84.375 (84.375)\n",
            "Epoch: [19][55/391]\tLoss 0.3617 (0.3100)\tPrec@1 88.281 (89.481)\n",
            "Epoch: [19][110/391]\tLoss 0.3434 (0.3136)\tPrec@1 88.281 (89.274)\n",
            "Epoch: [19][165/391]\tLoss 0.3351 (0.3272)\tPrec@1 85.938 (88.658)\n",
            "Epoch: [19][220/391]\tLoss 0.3131 (0.3320)\tPrec@1 89.062 (88.497)\n",
            "Epoch: [19][275/391]\tLoss 0.3301 (0.3367)\tPrec@1 89.844 (88.335)\n",
            "Epoch: [19][330/391]\tLoss 0.2660 (0.3369)\tPrec@1 91.406 (88.300)\n",
            "Epoch: [19][385/391]\tLoss 0.2811 (0.3375)\tPrec@1 92.188 (88.275)\n",
            "Test\t  Prec@1: 82.780 (Err: 17.220 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [20][0/391]\tLoss 0.2963 (0.2963)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [20][55/391]\tLoss 0.5227 (0.3214)\tPrec@1 82.812 (88.630)\n",
            "Epoch: [20][110/391]\tLoss 0.3704 (0.3266)\tPrec@1 90.625 (88.619)\n",
            "Epoch: [20][165/391]\tLoss 0.3161 (0.3336)\tPrec@1 89.062 (88.328)\n",
            "Epoch: [20][220/391]\tLoss 0.2422 (0.3333)\tPrec@1 89.062 (88.341)\n",
            "Epoch: [20][275/391]\tLoss 0.2572 (0.3346)\tPrec@1 91.406 (88.361)\n",
            "Epoch: [20][330/391]\tLoss 0.3743 (0.3384)\tPrec@1 88.281 (88.222)\n",
            "Epoch: [20][385/391]\tLoss 0.2287 (0.3391)\tPrec@1 92.969 (88.219)\n",
            "Test\t  Prec@1: 84.850 (Err: 15.150 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [21][0/391]\tLoss 0.2705 (0.2705)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [21][55/391]\tLoss 0.3923 (0.3262)\tPrec@1 85.156 (88.686)\n",
            "Epoch: [21][110/391]\tLoss 0.3249 (0.3201)\tPrec@1 90.625 (88.753)\n",
            "Epoch: [21][165/391]\tLoss 0.3067 (0.3211)\tPrec@1 89.062 (88.714)\n",
            "Epoch: [21][220/391]\tLoss 0.4154 (0.3253)\tPrec@1 84.375 (88.660)\n",
            "Epoch: [21][275/391]\tLoss 0.4335 (0.3282)\tPrec@1 84.375 (88.530)\n",
            "Epoch: [21][330/391]\tLoss 0.2595 (0.3271)\tPrec@1 92.188 (88.574)\n",
            "Epoch: [21][385/391]\tLoss 0.3342 (0.3274)\tPrec@1 87.500 (88.611)\n",
            "Test\t  Prec@1: 83.330 (Err: 16.670 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [22][0/391]\tLoss 0.3942 (0.3942)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [22][55/391]\tLoss 0.2268 (0.3124)\tPrec@1 90.625 (89.118)\n",
            "Epoch: [22][110/391]\tLoss 0.3764 (0.3197)\tPrec@1 86.719 (88.858)\n",
            "Epoch: [22][165/391]\tLoss 0.2876 (0.3232)\tPrec@1 89.062 (88.761)\n",
            "Epoch: [22][220/391]\tLoss 0.2290 (0.3198)\tPrec@1 92.969 (88.981)\n",
            "Epoch: [22][275/391]\tLoss 0.2479 (0.3199)\tPrec@1 90.625 (89.023)\n",
            "Epoch: [22][330/391]\tLoss 0.3352 (0.3232)\tPrec@1 85.938 (88.775)\n",
            "Epoch: [22][385/391]\tLoss 0.3829 (0.3246)\tPrec@1 84.375 (88.716)\n",
            "Test\t  Prec@1: 83.570 (Err: 16.430 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [23][0/391]\tLoss 0.2306 (0.2306)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [23][55/391]\tLoss 0.2895 (0.3113)\tPrec@1 91.406 (89.049)\n",
            "Epoch: [23][110/391]\tLoss 0.4007 (0.2974)\tPrec@1 86.719 (89.464)\n",
            "Epoch: [23][165/391]\tLoss 0.3269 (0.3049)\tPrec@1 88.281 (89.321)\n",
            "Epoch: [23][220/391]\tLoss 0.4187 (0.3090)\tPrec@1 86.719 (89.172)\n",
            "Epoch: [23][275/391]\tLoss 0.2760 (0.3094)\tPrec@1 90.625 (89.153)\n",
            "Epoch: [23][330/391]\tLoss 0.2552 (0.3098)\tPrec@1 89.062 (89.162)\n",
            "Epoch: [23][385/391]\tLoss 0.3101 (0.3098)\tPrec@1 87.500 (89.184)\n",
            "Test\t  Prec@1: 84.870 (Err: 15.130 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [24][0/391]\tLoss 0.3569 (0.3569)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [24][55/391]\tLoss 0.2693 (0.3042)\tPrec@1 89.844 (89.607)\n",
            "Epoch: [24][110/391]\tLoss 0.2642 (0.2985)\tPrec@1 89.062 (89.626)\n",
            "Epoch: [24][165/391]\tLoss 0.2259 (0.2951)\tPrec@1 93.750 (89.792)\n",
            "Epoch: [24][220/391]\tLoss 0.3386 (0.2999)\tPrec@1 88.281 (89.526)\n",
            "Epoch: [24][275/391]\tLoss 0.3407 (0.3042)\tPrec@1 91.406 (89.422)\n",
            "Epoch: [24][330/391]\tLoss 0.4786 (0.3068)\tPrec@1 81.250 (89.237)\n",
            "Epoch: [24][385/391]\tLoss 0.3217 (0.3074)\tPrec@1 88.281 (89.245)\n",
            "Test\t  Prec@1: 84.080 (Err: 15.920 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [25][0/391]\tLoss 0.2694 (0.2694)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [25][55/391]\tLoss 0.2444 (0.2872)\tPrec@1 91.406 (90.081)\n",
            "Epoch: [25][110/391]\tLoss 0.3342 (0.2942)\tPrec@1 89.844 (89.963)\n",
            "Epoch: [25][165/391]\tLoss 0.3037 (0.3006)\tPrec@1 89.062 (89.674)\n",
            "Epoch: [25][220/391]\tLoss 0.2550 (0.3022)\tPrec@1 92.188 (89.508)\n",
            "Epoch: [25][275/391]\tLoss 0.3079 (0.3072)\tPrec@1 85.156 (89.340)\n",
            "Epoch: [25][330/391]\tLoss 0.2957 (0.3099)\tPrec@1 87.500 (89.265)\n",
            "Epoch: [25][385/391]\tLoss 0.2887 (0.3089)\tPrec@1 89.844 (89.346)\n",
            "Test\t  Prec@1: 85.160 (Err: 14.840 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [26][0/391]\tLoss 0.2278 (0.2278)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [26][55/391]\tLoss 0.3290 (0.2857)\tPrec@1 89.062 (90.025)\n",
            "Epoch: [26][110/391]\tLoss 0.3111 (0.2942)\tPrec@1 88.281 (89.837)\n",
            "Epoch: [26][165/391]\tLoss 0.2963 (0.2974)\tPrec@1 85.938 (89.547)\n",
            "Epoch: [26][220/391]\tLoss 0.2685 (0.2953)\tPrec@1 89.844 (89.540)\n",
            "Epoch: [26][275/391]\tLoss 0.2504 (0.2984)\tPrec@1 89.844 (89.479)\n",
            "Epoch: [26][330/391]\tLoss 0.2758 (0.3011)\tPrec@1 92.969 (89.405)\n",
            "Epoch: [26][385/391]\tLoss 0.3746 (0.3025)\tPrec@1 87.500 (89.417)\n",
            "Test\t  Prec@1: 82.730 (Err: 17.270 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [27][0/391]\tLoss 0.2816 (0.2816)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [27][55/391]\tLoss 0.2880 (0.2945)\tPrec@1 93.750 (89.634)\n",
            "Epoch: [27][110/391]\tLoss 0.2835 (0.2898)\tPrec@1 90.625 (89.865)\n",
            "Epoch: [27][165/391]\tLoss 0.3653 (0.2936)\tPrec@1 84.375 (89.750)\n",
            "Epoch: [27][220/391]\tLoss 0.2902 (0.2934)\tPrec@1 88.281 (89.748)\n",
            "Epoch: [27][275/391]\tLoss 0.2789 (0.2959)\tPrec@1 89.062 (89.699)\n",
            "Epoch: [27][330/391]\tLoss 0.3165 (0.2986)\tPrec@1 85.938 (89.537)\n",
            "Epoch: [27][385/391]\tLoss 0.3503 (0.3002)\tPrec@1 88.281 (89.429)\n",
            "Test\t  Prec@1: 84.780 (Err: 15.220 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [28][0/391]\tLoss 0.3525 (0.3525)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [28][55/391]\tLoss 0.3215 (0.2650)\tPrec@1 90.625 (90.583)\n",
            "Epoch: [28][110/391]\tLoss 0.2850 (0.2800)\tPrec@1 88.281 (90.336)\n",
            "Epoch: [28][165/391]\tLoss 0.3672 (0.2876)\tPrec@1 88.281 (89.961)\n",
            "Epoch: [28][220/391]\tLoss 0.2332 (0.2868)\tPrec@1 91.406 (90.031)\n",
            "Epoch: [28][275/391]\tLoss 0.3602 (0.2906)\tPrec@1 89.062 (89.841)\n",
            "Epoch: [28][330/391]\tLoss 0.3524 (0.2885)\tPrec@1 89.062 (89.877)\n",
            "Epoch: [28][385/391]\tLoss 0.4532 (0.2924)\tPrec@1 84.375 (89.734)\n",
            "Test\t  Prec@1: 83.580 (Err: 16.420 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [29][0/391]\tLoss 0.2204 (0.2204)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [29][55/391]\tLoss 0.2061 (0.2780)\tPrec@1 92.969 (90.569)\n",
            "Epoch: [29][110/391]\tLoss 0.2176 (0.2703)\tPrec@1 92.188 (90.646)\n",
            "Epoch: [29][165/391]\tLoss 0.2684 (0.2745)\tPrec@1 92.188 (90.489)\n",
            "Epoch: [29][220/391]\tLoss 0.3623 (0.2827)\tPrec@1 85.156 (90.102)\n",
            "Epoch: [29][275/391]\tLoss 0.5271 (0.2878)\tPrec@1 82.812 (89.915)\n",
            "Epoch: [29][330/391]\tLoss 0.2529 (0.2873)\tPrec@1 91.406 (89.950)\n",
            "Epoch: [29][385/391]\tLoss 0.3140 (0.2886)\tPrec@1 87.500 (89.874)\n",
            "Test\t  Prec@1: 84.100 (Err: 15.900 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [30][0/391]\tLoss 0.3104 (0.3104)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [30][55/391]\tLoss 0.1996 (0.2749)\tPrec@1 93.750 (90.485)\n",
            "Epoch: [30][110/391]\tLoss 0.3801 (0.2777)\tPrec@1 87.500 (90.379)\n",
            "Epoch: [30][165/391]\tLoss 0.4074 (0.2745)\tPrec@1 86.719 (90.281)\n",
            "Epoch: [30][220/391]\tLoss 0.2786 (0.2807)\tPrec@1 89.062 (90.077)\n",
            "Epoch: [30][275/391]\tLoss 0.4272 (0.2872)\tPrec@1 83.594 (89.787)\n",
            "Epoch: [30][330/391]\tLoss 0.1871 (0.2893)\tPrec@1 96.875 (89.738)\n",
            "Epoch: [30][385/391]\tLoss 0.2111 (0.2902)\tPrec@1 91.406 (89.743)\n",
            "Test\t  Prec@1: 84.310 (Err: 15.690 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [31][0/391]\tLoss 0.3799 (0.3799)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [31][55/391]\tLoss 0.2678 (0.2752)\tPrec@1 89.844 (90.248)\n",
            "Epoch: [31][110/391]\tLoss 0.3201 (0.2666)\tPrec@1 90.625 (90.611)\n",
            "Epoch: [31][165/391]\tLoss 0.2140 (0.2622)\tPrec@1 91.406 (90.832)\n",
            "Epoch: [31][220/391]\tLoss 0.2646 (0.2665)\tPrec@1 87.500 (90.749)\n",
            "Epoch: [31][275/391]\tLoss 0.2676 (0.2769)\tPrec@1 92.969 (90.447)\n",
            "Epoch: [31][330/391]\tLoss 0.3660 (0.2802)\tPrec@1 86.719 (90.323)\n",
            "Epoch: [31][385/391]\tLoss 0.3003 (0.2799)\tPrec@1 92.188 (90.340)\n",
            "Test\t  Prec@1: 82.910 (Err: 17.090 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [32][0/391]\tLoss 0.2547 (0.2547)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [32][55/391]\tLoss 0.2623 (0.2602)\tPrec@1 88.281 (90.444)\n",
            "Epoch: [32][110/391]\tLoss 0.2857 (0.2637)\tPrec@1 90.625 (90.583)\n",
            "Epoch: [32][165/391]\tLoss 0.2985 (0.2670)\tPrec@1 89.844 (90.611)\n",
            "Epoch: [32][220/391]\tLoss 0.2675 (0.2722)\tPrec@1 89.062 (90.342)\n",
            "Epoch: [32][275/391]\tLoss 0.3715 (0.2739)\tPrec@1 85.156 (90.370)\n",
            "Epoch: [32][330/391]\tLoss 0.2426 (0.2781)\tPrec@1 90.625 (90.262)\n",
            "Epoch: [32][385/391]\tLoss 0.3210 (0.2810)\tPrec@1 88.281 (90.119)\n",
            "Test\t  Prec@1: 83.890 (Err: 16.110 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [33][0/391]\tLoss 0.2251 (0.2251)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [33][55/391]\tLoss 0.2446 (0.2663)\tPrec@1 90.625 (90.876)\n",
            "Epoch: [33][110/391]\tLoss 0.2418 (0.2709)\tPrec@1 89.844 (90.660)\n",
            "Epoch: [33][165/391]\tLoss 0.3588 (0.2696)\tPrec@1 86.719 (90.696)\n",
            "Epoch: [33][220/391]\tLoss 0.2807 (0.2718)\tPrec@1 92.969 (90.512)\n",
            "Epoch: [33][275/391]\tLoss 0.3330 (0.2751)\tPrec@1 87.500 (90.339)\n",
            "Epoch: [33][330/391]\tLoss 0.2800 (0.2783)\tPrec@1 93.750 (90.257)\n",
            "Epoch: [33][385/391]\tLoss 0.2770 (0.2785)\tPrec@1 89.844 (90.228)\n",
            "Test\t  Prec@1: 85.750 (Err: 14.250 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [34][0/391]\tLoss 0.2517 (0.2517)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [34][55/391]\tLoss 0.2261 (0.2364)\tPrec@1 90.625 (91.532)\n",
            "Epoch: [34][110/391]\tLoss 0.2364 (0.2472)\tPrec@1 91.406 (91.090)\n",
            "Epoch: [34][165/391]\tLoss 0.3948 (0.2565)\tPrec@1 85.156 (90.733)\n",
            "Epoch: [34][220/391]\tLoss 0.2491 (0.2634)\tPrec@1 91.406 (90.568)\n",
            "Epoch: [34][275/391]\tLoss 0.3540 (0.2686)\tPrec@1 88.281 (90.424)\n",
            "Epoch: [34][330/391]\tLoss 0.2288 (0.2679)\tPrec@1 91.406 (90.481)\n",
            "Epoch: [34][385/391]\tLoss 0.2114 (0.2705)\tPrec@1 92.969 (90.384)\n",
            "Test\t  Prec@1: 85.420 (Err: 14.580 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [35][0/391]\tLoss 0.1751 (0.1751)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [35][55/391]\tLoss 0.1683 (0.2618)\tPrec@1 91.406 (90.611)\n",
            "Epoch: [35][110/391]\tLoss 0.2180 (0.2619)\tPrec@1 92.188 (90.702)\n",
            "Epoch: [35][165/391]\tLoss 0.2185 (0.2676)\tPrec@1 92.969 (90.470)\n",
            "Epoch: [35][220/391]\tLoss 0.2941 (0.2710)\tPrec@1 89.062 (90.356)\n",
            "Epoch: [35][275/391]\tLoss 0.2890 (0.2688)\tPrec@1 89.844 (90.466)\n",
            "Epoch: [35][330/391]\tLoss 0.2434 (0.2688)\tPrec@1 90.625 (90.483)\n",
            "Epoch: [35][385/391]\tLoss 0.3141 (0.2699)\tPrec@1 86.719 (90.485)\n",
            "Test\t  Prec@1: 84.690 (Err: 15.310 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [36][0/391]\tLoss 0.2924 (0.2924)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [36][55/391]\tLoss 0.2804 (0.2639)\tPrec@1 89.062 (90.737)\n",
            "Epoch: [36][110/391]\tLoss 0.3011 (0.2611)\tPrec@1 90.625 (90.829)\n",
            "Epoch: [36][165/391]\tLoss 0.2216 (0.2639)\tPrec@1 91.406 (90.733)\n",
            "Epoch: [36][220/391]\tLoss 0.2327 (0.2646)\tPrec@1 92.188 (90.728)\n",
            "Epoch: [36][275/391]\tLoss 0.2396 (0.2694)\tPrec@1 92.188 (90.512)\n",
            "Epoch: [36][330/391]\tLoss 0.3151 (0.2723)\tPrec@1 89.844 (90.420)\n",
            "Epoch: [36][385/391]\tLoss 0.2344 (0.2713)\tPrec@1 89.844 (90.512)\n",
            "Test\t  Prec@1: 86.550 (Err: 13.450 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [37][0/391]\tLoss 0.1888 (0.1888)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [37][55/391]\tLoss 0.2674 (0.2439)\tPrec@1 92.188 (91.350)\n",
            "Epoch: [37][110/391]\tLoss 0.2412 (0.2484)\tPrec@1 92.969 (91.399)\n",
            "Epoch: [37][165/391]\tLoss 0.2936 (0.2520)\tPrec@1 93.750 (91.237)\n",
            "Epoch: [37][220/391]\tLoss 0.3875 (0.2578)\tPrec@1 85.938 (91.021)\n",
            "Epoch: [37][275/391]\tLoss 0.2809 (0.2572)\tPrec@1 89.844 (91.044)\n",
            "Epoch: [37][330/391]\tLoss 0.2052 (0.2607)\tPrec@1 91.406 (90.929)\n",
            "Epoch: [37][385/391]\tLoss 0.1882 (0.2650)\tPrec@1 95.312 (90.781)\n",
            "Test\t  Prec@1: 85.940 (Err: 14.060 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [38][0/391]\tLoss 0.2273 (0.2273)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [38][55/391]\tLoss 0.3044 (0.2456)\tPrec@1 88.281 (91.336)\n",
            "Epoch: [38][110/391]\tLoss 0.2485 (0.2549)\tPrec@1 92.188 (91.005)\n",
            "Epoch: [38][165/391]\tLoss 0.2885 (0.2567)\tPrec@1 91.406 (90.903)\n",
            "Epoch: [38][220/391]\tLoss 0.3086 (0.2579)\tPrec@1 90.625 (90.865)\n",
            "Epoch: [38][275/391]\tLoss 0.3199 (0.2602)\tPrec@1 87.500 (90.772)\n",
            "Epoch: [38][330/391]\tLoss 0.2439 (0.2644)\tPrec@1 89.062 (90.630)\n",
            "Epoch: [38][385/391]\tLoss 0.2907 (0.2639)\tPrec@1 90.625 (90.688)\n",
            "Test\t  Prec@1: 85.800 (Err: 14.200 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [39][0/391]\tLoss 0.1678 (0.1678)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [39][55/391]\tLoss 0.2517 (0.2587)\tPrec@1 89.062 (91.071)\n",
            "Epoch: [39][110/391]\tLoss 0.3687 (0.2543)\tPrec@1 85.938 (91.258)\n",
            "Epoch: [39][165/391]\tLoss 0.2300 (0.2613)\tPrec@1 93.750 (91.034)\n",
            "Epoch: [39][220/391]\tLoss 0.2687 (0.2571)\tPrec@1 89.062 (91.180)\n",
            "Epoch: [39][275/391]\tLoss 0.2158 (0.2598)\tPrec@1 92.188 (91.050)\n",
            "Epoch: [39][330/391]\tLoss 0.2691 (0.2598)\tPrec@1 89.844 (91.022)\n",
            "Epoch: [39][385/391]\tLoss 0.3991 (0.2629)\tPrec@1 87.500 (90.880)\n",
            "Test\t  Prec@1: 83.940 (Err: 16.060 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [40][0/391]\tLoss 0.3266 (0.3266)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [40][55/391]\tLoss 0.2305 (0.2313)\tPrec@1 91.406 (91.783)\n",
            "Epoch: [40][110/391]\tLoss 0.2051 (0.2354)\tPrec@1 92.188 (91.695)\n",
            "Epoch: [40][165/391]\tLoss 0.3259 (0.2443)\tPrec@1 89.062 (91.345)\n",
            "Epoch: [40][220/391]\tLoss 0.1905 (0.2541)\tPrec@1 93.750 (90.918)\n",
            "Epoch: [40][275/391]\tLoss 0.1995 (0.2570)\tPrec@1 93.750 (90.767)\n",
            "Epoch: [40][330/391]\tLoss 0.2975 (0.2567)\tPrec@1 90.625 (90.833)\n",
            "Epoch: [40][385/391]\tLoss 0.3584 (0.2576)\tPrec@1 88.281 (90.823)\n",
            "Test\t  Prec@1: 84.870 (Err: 15.130 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [41][0/391]\tLoss 0.2207 (0.2207)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [41][55/391]\tLoss 0.2764 (0.2417)\tPrec@1 88.281 (91.755)\n",
            "Epoch: [41][110/391]\tLoss 0.2282 (0.2388)\tPrec@1 92.188 (91.709)\n",
            "Epoch: [41][165/391]\tLoss 0.1788 (0.2395)\tPrec@1 92.969 (91.580)\n",
            "Epoch: [41][220/391]\tLoss 0.2388 (0.2502)\tPrec@1 90.625 (91.219)\n",
            "Epoch: [41][275/391]\tLoss 0.1585 (0.2516)\tPrec@1 93.750 (91.154)\n",
            "Epoch: [41][330/391]\tLoss 0.2408 (0.2540)\tPrec@1 92.969 (91.043)\n",
            "Epoch: [41][385/391]\tLoss 0.2931 (0.2581)\tPrec@1 91.406 (90.912)\n",
            "Test\t  Prec@1: 85.700 (Err: 14.300 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [42][0/391]\tLoss 0.3730 (0.3730)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [42][55/391]\tLoss 0.2356 (0.2527)\tPrec@1 93.750 (91.336)\n",
            "Epoch: [42][110/391]\tLoss 0.2708 (0.2400)\tPrec@1 89.844 (91.646)\n",
            "Epoch: [42][165/391]\tLoss 0.2664 (0.2501)\tPrec@1 88.281 (91.213)\n",
            "Epoch: [42][220/391]\tLoss 0.1807 (0.2535)\tPrec@1 92.188 (91.063)\n",
            "Epoch: [42][275/391]\tLoss 0.2182 (0.2521)\tPrec@1 92.188 (91.115)\n",
            "Epoch: [42][330/391]\tLoss 0.3392 (0.2506)\tPrec@1 88.281 (91.203)\n",
            "Epoch: [42][385/391]\tLoss 0.1351 (0.2547)\tPrec@1 95.312 (91.026)\n",
            "Test\t  Prec@1: 86.790 (Err: 13.210 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [43][0/391]\tLoss 0.1886 (0.1886)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [43][55/391]\tLoss 0.2680 (0.2416)\tPrec@1 88.281 (91.574)\n",
            "Epoch: [43][110/391]\tLoss 0.3104 (0.2458)\tPrec@1 89.062 (91.392)\n",
            "Epoch: [43][165/391]\tLoss 0.3037 (0.2500)\tPrec@1 89.062 (91.298)\n",
            "Epoch: [43][220/391]\tLoss 0.2443 (0.2523)\tPrec@1 92.188 (91.159)\n",
            "Epoch: [43][275/391]\tLoss 0.2459 (0.2503)\tPrec@1 91.406 (91.163)\n",
            "Epoch: [43][330/391]\tLoss 0.3467 (0.2546)\tPrec@1 89.062 (91.055)\n",
            "Epoch: [43][385/391]\tLoss 0.2630 (0.2534)\tPrec@1 90.625 (91.091)\n",
            "Test\t  Prec@1: 85.150 (Err: 14.850 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [44][0/391]\tLoss 0.1724 (0.1724)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [44][55/391]\tLoss 0.2172 (0.2269)\tPrec@1 92.188 (92.076)\n",
            "Epoch: [44][110/391]\tLoss 0.1341 (0.2437)\tPrec@1 95.312 (91.519)\n",
            "Epoch: [44][165/391]\tLoss 0.2692 (0.2511)\tPrec@1 90.625 (91.157)\n",
            "Epoch: [44][220/391]\tLoss 0.2462 (0.2553)\tPrec@1 92.188 (90.957)\n",
            "Epoch: [44][275/391]\tLoss 0.2550 (0.2559)\tPrec@1 92.969 (90.982)\n",
            "Epoch: [44][330/391]\tLoss 0.1735 (0.2546)\tPrec@1 96.094 (90.993)\n",
            "Epoch: [44][385/391]\tLoss 0.3635 (0.2557)\tPrec@1 87.500 (90.910)\n",
            "Test\t  Prec@1: 84.410 (Err: 15.590 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [45][0/391]\tLoss 0.3201 (0.3201)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [45][55/391]\tLoss 0.2620 (0.2289)\tPrec@1 89.844 (92.188)\n",
            "Epoch: [45][110/391]\tLoss 0.2866 (0.2336)\tPrec@1 91.406 (91.906)\n",
            "Epoch: [45][165/391]\tLoss 0.2284 (0.2372)\tPrec@1 89.844 (91.745)\n",
            "Epoch: [45][220/391]\tLoss 0.2306 (0.2424)\tPrec@1 91.406 (91.519)\n",
            "Epoch: [45][275/391]\tLoss 0.2403 (0.2440)\tPrec@1 91.406 (91.443)\n",
            "Epoch: [45][330/391]\tLoss 0.2008 (0.2457)\tPrec@1 94.531 (91.449)\n",
            "Epoch: [45][385/391]\tLoss 0.2480 (0.2459)\tPrec@1 93.750 (91.453)\n",
            "Test\t  Prec@1: 85.160 (Err: 14.840 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [46][0/391]\tLoss 0.1529 (0.1529)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [46][55/391]\tLoss 0.1916 (0.2205)\tPrec@1 92.969 (92.118)\n",
            "Epoch: [46][110/391]\tLoss 0.2758 (0.2185)\tPrec@1 88.281 (92.082)\n",
            "Epoch: [46][165/391]\tLoss 0.2217 (0.2289)\tPrec@1 89.844 (91.773)\n",
            "Epoch: [46][220/391]\tLoss 0.2563 (0.2315)\tPrec@1 91.406 (91.749)\n",
            "Epoch: [46][275/391]\tLoss 0.3139 (0.2387)\tPrec@1 91.406 (91.505)\n",
            "Epoch: [46][330/391]\tLoss 0.2237 (0.2428)\tPrec@1 92.969 (91.366)\n",
            "Epoch: [46][385/391]\tLoss 0.2924 (0.2450)\tPrec@1 91.406 (91.362)\n",
            "Test\t  Prec@1: 86.140 (Err: 13.860 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [47][0/391]\tLoss 0.2526 (0.2526)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [47][55/391]\tLoss 0.2096 (0.2343)\tPrec@1 93.750 (91.895)\n",
            "Epoch: [47][110/391]\tLoss 0.2964 (0.2357)\tPrec@1 87.500 (91.737)\n",
            "Epoch: [47][165/391]\tLoss 0.1890 (0.2426)\tPrec@1 94.531 (91.444)\n",
            "Epoch: [47][220/391]\tLoss 0.2953 (0.2448)\tPrec@1 92.188 (91.378)\n",
            "Epoch: [47][275/391]\tLoss 0.2215 (0.2432)\tPrec@1 90.625 (91.517)\n",
            "Epoch: [47][330/391]\tLoss 0.2651 (0.2439)\tPrec@1 89.844 (91.531)\n",
            "Epoch: [47][385/391]\tLoss 0.1617 (0.2456)\tPrec@1 93.750 (91.475)\n",
            "Test\t  Prec@1: 85.470 (Err: 14.530 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [48][0/391]\tLoss 0.0781 (0.0781)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [48][55/391]\tLoss 0.3291 (0.2185)\tPrec@1 86.719 (92.257)\n",
            "Epoch: [48][110/391]\tLoss 0.1759 (0.2258)\tPrec@1 92.969 (92.096)\n",
            "Epoch: [48][165/391]\tLoss 0.3481 (0.2346)\tPrec@1 86.719 (91.651)\n",
            "Epoch: [48][220/391]\tLoss 0.2893 (0.2379)\tPrec@1 87.500 (91.565)\n",
            "Epoch: [48][275/391]\tLoss 0.2524 (0.2398)\tPrec@1 91.406 (91.491)\n",
            "Epoch: [48][330/391]\tLoss 0.2142 (0.2422)\tPrec@1 94.531 (91.463)\n",
            "Epoch: [48][385/391]\tLoss 0.3180 (0.2432)\tPrec@1 92.188 (91.469)\n",
            "Test\t  Prec@1: 85.750 (Err: 14.250 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [49][0/391]\tLoss 0.2204 (0.2204)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [49][55/391]\tLoss 0.2264 (0.2289)\tPrec@1 93.750 (91.881)\n",
            "Epoch: [49][110/391]\tLoss 0.2239 (0.2354)\tPrec@1 90.625 (91.667)\n",
            "Epoch: [49][165/391]\tLoss 0.4207 (0.2368)\tPrec@1 86.719 (91.665)\n",
            "Epoch: [49][220/391]\tLoss 0.1853 (0.2367)\tPrec@1 92.969 (91.710)\n",
            "Epoch: [49][275/391]\tLoss 0.2986 (0.2411)\tPrec@1 89.844 (91.491)\n",
            "Epoch: [49][330/391]\tLoss 0.2078 (0.2427)\tPrec@1 92.969 (91.458)\n",
            "Epoch: [49][385/391]\tLoss 0.1637 (0.2451)\tPrec@1 94.531 (91.372)\n",
            "Test\t  Prec@1: 86.990 (Err: 13.010 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [50][0/391]\tLoss 0.1459 (0.1459)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [50][55/391]\tLoss 0.3494 (0.2310)\tPrec@1 86.719 (91.922)\n",
            "Epoch: [50][110/391]\tLoss 0.1824 (0.2307)\tPrec@1 94.531 (91.744)\n",
            "Epoch: [50][165/391]\tLoss 0.2819 (0.2274)\tPrec@1 89.844 (91.825)\n",
            "Epoch: [50][220/391]\tLoss 0.1557 (0.2346)\tPrec@1 93.750 (91.650)\n",
            "Epoch: [50][275/391]\tLoss 0.2408 (0.2378)\tPrec@1 90.625 (91.593)\n",
            "Epoch: [50][330/391]\tLoss 0.3109 (0.2392)\tPrec@1 89.844 (91.574)\n",
            "Epoch: [50][385/391]\tLoss 0.1938 (0.2399)\tPrec@1 93.750 (91.540)\n",
            "Test\t  Prec@1: 85.980 (Err: 14.020 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [51][0/391]\tLoss 0.2008 (0.2008)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [51][55/391]\tLoss 0.1872 (0.2410)\tPrec@1 93.750 (91.685)\n",
            "Epoch: [51][110/391]\tLoss 0.3378 (0.2355)\tPrec@1 92.969 (91.864)\n",
            "Epoch: [51][165/391]\tLoss 0.1502 (0.2372)\tPrec@1 93.750 (91.660)\n",
            "Epoch: [51][220/391]\tLoss 0.2734 (0.2389)\tPrec@1 89.844 (91.608)\n",
            "Epoch: [51][275/391]\tLoss 0.2766 (0.2420)\tPrec@1 87.500 (91.500)\n",
            "Epoch: [51][330/391]\tLoss 0.1951 (0.2419)\tPrec@1 94.531 (91.541)\n",
            "Epoch: [51][385/391]\tLoss 0.2261 (0.2438)\tPrec@1 90.625 (91.477)\n",
            "Test\t  Prec@1: 84.970 (Err: 15.030 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [52][0/391]\tLoss 0.2184 (0.2184)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [52][55/391]\tLoss 0.1709 (0.2072)\tPrec@1 92.188 (92.550)\n",
            "Epoch: [52][110/391]\tLoss 0.1642 (0.2116)\tPrec@1 94.531 (92.406)\n",
            "Epoch: [52][165/391]\tLoss 0.2280 (0.2174)\tPrec@1 94.531 (92.357)\n",
            "Epoch: [52][220/391]\tLoss 0.2108 (0.2229)\tPrec@1 92.188 (92.195)\n",
            "Epoch: [52][275/391]\tLoss 0.3908 (0.2294)\tPrec@1 84.375 (91.970)\n",
            "Epoch: [52][330/391]\tLoss 0.3245 (0.2328)\tPrec@1 89.844 (91.824)\n",
            "Epoch: [52][385/391]\tLoss 0.1979 (0.2342)\tPrec@1 91.406 (91.801)\n",
            "Test\t  Prec@1: 85.450 (Err: 14.550 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [53][0/391]\tLoss 0.2308 (0.2308)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [53][55/391]\tLoss 0.2357 (0.2235)\tPrec@1 90.625 (92.271)\n",
            "Epoch: [53][110/391]\tLoss 0.1861 (0.2358)\tPrec@1 94.531 (91.814)\n",
            "Epoch: [53][165/391]\tLoss 0.2497 (0.2348)\tPrec@1 88.281 (91.755)\n",
            "Epoch: [53][220/391]\tLoss 0.1352 (0.2336)\tPrec@1 95.312 (91.742)\n",
            "Epoch: [53][275/391]\tLoss 0.2286 (0.2327)\tPrec@1 92.188 (91.839)\n",
            "Epoch: [53][330/391]\tLoss 0.4217 (0.2354)\tPrec@1 88.281 (91.741)\n",
            "Epoch: [53][385/391]\tLoss 0.3175 (0.2356)\tPrec@1 88.281 (91.769)\n",
            "Test\t  Prec@1: 84.310 (Err: 15.690 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [54][0/391]\tLoss 0.2273 (0.2273)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [54][55/391]\tLoss 0.2387 (0.2106)\tPrec@1 89.062 (92.843)\n",
            "Epoch: [54][110/391]\tLoss 0.2599 (0.2177)\tPrec@1 89.844 (92.427)\n",
            "Epoch: [54][165/391]\tLoss 0.1791 (0.2219)\tPrec@1 92.188 (92.310)\n",
            "Epoch: [54][220/391]\tLoss 0.2091 (0.2307)\tPrec@1 92.188 (91.944)\n",
            "Epoch: [54][275/391]\tLoss 0.1760 (0.2280)\tPrec@1 94.531 (92.063)\n",
            "Epoch: [54][330/391]\tLoss 0.3080 (0.2287)\tPrec@1 88.281 (92.027)\n",
            "Epoch: [54][385/391]\tLoss 0.2072 (0.2329)\tPrec@1 91.406 (91.854)\n",
            "Test\t  Prec@1: 85.710 (Err: 14.290 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [55][0/391]\tLoss 0.1373 (0.1373)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [55][55/391]\tLoss 0.2980 (0.2071)\tPrec@1 89.844 (92.913)\n",
            "Epoch: [55][110/391]\tLoss 0.1375 (0.2187)\tPrec@1 95.312 (92.539)\n",
            "Epoch: [55][165/391]\tLoss 0.3651 (0.2220)\tPrec@1 89.844 (92.343)\n",
            "Epoch: [55][220/391]\tLoss 0.2176 (0.2223)\tPrec@1 89.844 (92.272)\n",
            "Epoch: [55][275/391]\tLoss 0.2008 (0.2243)\tPrec@1 90.625 (92.261)\n",
            "Epoch: [55][330/391]\tLoss 0.2866 (0.2269)\tPrec@1 89.062 (92.119)\n",
            "Epoch: [55][385/391]\tLoss 0.2193 (0.2293)\tPrec@1 95.312 (92.054)\n",
            "Test\t  Prec@1: 85.840 (Err: 14.160 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [56][0/391]\tLoss 0.2663 (0.2663)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [56][55/391]\tLoss 0.2359 (0.2469)\tPrec@1 89.844 (90.848)\n",
            "Epoch: [56][110/391]\tLoss 0.2052 (0.2360)\tPrec@1 92.188 (91.371)\n",
            "Epoch: [56][165/391]\tLoss 0.3032 (0.2328)\tPrec@1 89.844 (91.707)\n",
            "Epoch: [56][220/391]\tLoss 0.2978 (0.2337)\tPrec@1 90.625 (91.710)\n",
            "Epoch: [56][275/391]\tLoss 0.2388 (0.2313)\tPrec@1 92.969 (91.746)\n",
            "Epoch: [56][330/391]\tLoss 0.1955 (0.2331)\tPrec@1 92.969 (91.753)\n",
            "Epoch: [56][385/391]\tLoss 0.2214 (0.2342)\tPrec@1 89.844 (91.710)\n",
            "Test\t  Prec@1: 83.990 (Err: 16.010 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [57][0/391]\tLoss 0.1422 (0.1422)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [57][55/391]\tLoss 0.1888 (0.2296)\tPrec@1 92.188 (91.839)\n",
            "Epoch: [57][110/391]\tLoss 0.1889 (0.2216)\tPrec@1 93.750 (92.131)\n",
            "Epoch: [57][165/391]\tLoss 0.2155 (0.2258)\tPrec@1 92.188 (91.985)\n",
            "Epoch: [57][220/391]\tLoss 0.2780 (0.2260)\tPrec@1 89.062 (92.078)\n",
            "Epoch: [57][275/391]\tLoss 0.2479 (0.2253)\tPrec@1 91.406 (92.077)\n",
            "Epoch: [57][330/391]\tLoss 0.2291 (0.2268)\tPrec@1 92.188 (92.084)\n",
            "Epoch: [57][385/391]\tLoss 0.2878 (0.2265)\tPrec@1 90.625 (92.105)\n",
            "Test\t  Prec@1: 85.860 (Err: 14.140 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [58][0/391]\tLoss 0.1493 (0.1493)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [58][55/391]\tLoss 0.1782 (0.2252)\tPrec@1 94.531 (92.160)\n",
            "Epoch: [58][110/391]\tLoss 0.2533 (0.2262)\tPrec@1 87.500 (92.188)\n",
            "Epoch: [58][165/391]\tLoss 0.2137 (0.2282)\tPrec@1 94.531 (92.145)\n",
            "Epoch: [58][220/391]\tLoss 0.2850 (0.2269)\tPrec@1 90.625 (92.180)\n",
            "Epoch: [58][275/391]\tLoss 0.1807 (0.2247)\tPrec@1 93.750 (92.188)\n",
            "Epoch: [58][330/391]\tLoss 0.1909 (0.2251)\tPrec@1 92.969 (92.093)\n",
            "Epoch: [58][385/391]\tLoss 0.4633 (0.2297)\tPrec@1 84.375 (91.916)\n",
            "Test\t  Prec@1: 86.230 (Err: 13.770 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [59][0/391]\tLoss 0.1982 (0.1982)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [59][55/391]\tLoss 0.1533 (0.2175)\tPrec@1 95.312 (92.327)\n",
            "Epoch: [59][110/391]\tLoss 0.1438 (0.2196)\tPrec@1 95.312 (92.180)\n",
            "Epoch: [59][165/391]\tLoss 0.4189 (0.2259)\tPrec@1 88.281 (91.839)\n",
            "Epoch: [59][220/391]\tLoss 0.2019 (0.2277)\tPrec@1 92.969 (91.742)\n",
            "Epoch: [59][275/391]\tLoss 0.1785 (0.2269)\tPrec@1 93.750 (91.837)\n",
            "Epoch: [59][330/391]\tLoss 0.1341 (0.2273)\tPrec@1 96.094 (91.859)\n",
            "Epoch: [59][385/391]\tLoss 0.2464 (0.2303)\tPrec@1 90.625 (91.785)\n",
            "Test\t  Prec@1: 86.220 (Err: 13.780 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [60][0/391]\tLoss 0.2475 (0.2475)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [60][55/391]\tLoss 0.2745 (0.2242)\tPrec@1 89.844 (92.299)\n",
            "Epoch: [60][110/391]\tLoss 0.2400 (0.2164)\tPrec@1 92.188 (92.427)\n",
            "Epoch: [60][165/391]\tLoss 0.2283 (0.2218)\tPrec@1 92.969 (92.268)\n",
            "Epoch: [60][220/391]\tLoss 0.2669 (0.2233)\tPrec@1 89.844 (92.166)\n",
            "Epoch: [60][275/391]\tLoss 0.3431 (0.2267)\tPrec@1 85.938 (92.052)\n",
            "Epoch: [60][330/391]\tLoss 0.1443 (0.2275)\tPrec@1 96.094 (92.029)\n",
            "Epoch: [60][385/391]\tLoss 0.2149 (0.2307)\tPrec@1 91.406 (91.890)\n",
            "Test\t  Prec@1: 86.250 (Err: 13.750 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [61][0/391]\tLoss 0.2345 (0.2345)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [61][55/391]\tLoss 0.2182 (0.1997)\tPrec@1 93.750 (92.983)\n",
            "Epoch: [61][110/391]\tLoss 0.1344 (0.2194)\tPrec@1 95.312 (92.251)\n",
            "Epoch: [61][165/391]\tLoss 0.3599 (0.2198)\tPrec@1 87.500 (92.206)\n",
            "Epoch: [61][220/391]\tLoss 0.1702 (0.2215)\tPrec@1 93.750 (92.255)\n",
            "Epoch: [61][275/391]\tLoss 0.1763 (0.2232)\tPrec@1 93.750 (92.156)\n",
            "Epoch: [61][330/391]\tLoss 0.2176 (0.2228)\tPrec@1 89.844 (92.195)\n",
            "Epoch: [61][385/391]\tLoss 0.2349 (0.2255)\tPrec@1 93.750 (92.086)\n",
            "Test\t  Prec@1: 86.530 (Err: 13.470 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [62][0/391]\tLoss 0.2232 (0.2232)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [62][55/391]\tLoss 0.2185 (0.2244)\tPrec@1 93.750 (92.118)\n",
            "Epoch: [62][110/391]\tLoss 0.1936 (0.2201)\tPrec@1 91.406 (92.293)\n",
            "Epoch: [62][165/391]\tLoss 0.2305 (0.2242)\tPrec@1 92.188 (92.150)\n",
            "Epoch: [62][220/391]\tLoss 0.1545 (0.2213)\tPrec@1 96.094 (92.233)\n",
            "Epoch: [62][275/391]\tLoss 0.1557 (0.2227)\tPrec@1 96.094 (92.207)\n",
            "Epoch: [62][330/391]\tLoss 0.2889 (0.2272)\tPrec@1 89.844 (92.032)\n",
            "Epoch: [62][385/391]\tLoss 0.2395 (0.2304)\tPrec@1 89.844 (91.949)\n",
            "Test\t  Prec@1: 86.510 (Err: 13.490 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [63][0/391]\tLoss 0.2661 (0.2661)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [63][55/391]\tLoss 0.1832 (0.2138)\tPrec@1 93.750 (92.578)\n",
            "Epoch: [63][110/391]\tLoss 0.2175 (0.2154)\tPrec@1 92.188 (92.504)\n",
            "Epoch: [63][165/391]\tLoss 0.1519 (0.2193)\tPrec@1 96.875 (92.296)\n",
            "Epoch: [63][220/391]\tLoss 0.0777 (0.2214)\tPrec@1 97.656 (92.209)\n",
            "Epoch: [63][275/391]\tLoss 0.1997 (0.2234)\tPrec@1 92.969 (92.196)\n",
            "Epoch: [63][330/391]\tLoss 0.2194 (0.2247)\tPrec@1 92.969 (92.188)\n",
            "Epoch: [63][385/391]\tLoss 0.1992 (0.2275)\tPrec@1 93.750 (92.072)\n",
            "Test\t  Prec@1: 84.200 (Err: 15.800 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [64][0/391]\tLoss 0.1706 (0.1706)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [64][55/391]\tLoss 0.2892 (0.2082)\tPrec@1 89.062 (92.704)\n",
            "Epoch: [64][110/391]\tLoss 0.2168 (0.2174)\tPrec@1 92.188 (92.293)\n",
            "Epoch: [64][165/391]\tLoss 0.1945 (0.2162)\tPrec@1 92.969 (92.413)\n",
            "Epoch: [64][220/391]\tLoss 0.2581 (0.2184)\tPrec@1 90.625 (92.378)\n",
            "Epoch: [64][275/391]\tLoss 0.2542 (0.2203)\tPrec@1 92.969 (92.301)\n",
            "Epoch: [64][330/391]\tLoss 0.1672 (0.2239)\tPrec@1 95.312 (92.169)\n",
            "Epoch: [64][385/391]\tLoss 0.1908 (0.2253)\tPrec@1 93.750 (92.092)\n",
            "Test\t  Prec@1: 84.280 (Err: 15.720 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [65][0/391]\tLoss 0.2892 (0.2892)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [65][55/391]\tLoss 0.2018 (0.2198)\tPrec@1 95.312 (92.160)\n",
            "Epoch: [65][110/391]\tLoss 0.2693 (0.2253)\tPrec@1 92.188 (92.047)\n",
            "Epoch: [65][165/391]\tLoss 0.2994 (0.2268)\tPrec@1 89.844 (92.070)\n",
            "Epoch: [65][220/391]\tLoss 0.2306 (0.2284)\tPrec@1 90.625 (91.982)\n",
            "Epoch: [65][275/391]\tLoss 0.2393 (0.2250)\tPrec@1 93.750 (92.108)\n",
            "Epoch: [65][330/391]\tLoss 0.2426 (0.2224)\tPrec@1 91.406 (92.192)\n",
            "Epoch: [65][385/391]\tLoss 0.2992 (0.2214)\tPrec@1 89.062 (92.188)\n",
            "Test\t  Prec@1: 85.820 (Err: 14.180 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [66][0/391]\tLoss 0.1666 (0.1666)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [66][55/391]\tLoss 0.2276 (0.2094)\tPrec@1 92.188 (92.871)\n",
            "Epoch: [66][110/391]\tLoss 0.2413 (0.2184)\tPrec@1 92.969 (92.307)\n",
            "Epoch: [66][165/391]\tLoss 0.2427 (0.2194)\tPrec@1 88.281 (92.188)\n",
            "Epoch: [66][220/391]\tLoss 0.3027 (0.2198)\tPrec@1 88.281 (92.166)\n",
            "Epoch: [66][275/391]\tLoss 0.2328 (0.2215)\tPrec@1 90.625 (92.185)\n",
            "Epoch: [66][330/391]\tLoss 0.2994 (0.2225)\tPrec@1 90.625 (92.131)\n",
            "Epoch: [66][385/391]\tLoss 0.2697 (0.2236)\tPrec@1 92.188 (92.072)\n",
            "Test\t  Prec@1: 84.460 (Err: 15.540 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [67][0/391]\tLoss 0.1774 (0.1774)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [67][55/391]\tLoss 0.2422 (0.2131)\tPrec@1 92.188 (92.662)\n",
            "Epoch: [67][110/391]\tLoss 0.2514 (0.2141)\tPrec@1 92.188 (92.539)\n",
            "Epoch: [67][165/391]\tLoss 0.1984 (0.2147)\tPrec@1 92.969 (92.413)\n",
            "Epoch: [67][220/391]\tLoss 0.2168 (0.2166)\tPrec@1 92.188 (92.325)\n",
            "Epoch: [67][275/391]\tLoss 0.2547 (0.2207)\tPrec@1 89.844 (92.151)\n",
            "Epoch: [67][330/391]\tLoss 0.2359 (0.2222)\tPrec@1 92.969 (92.060)\n",
            "Epoch: [67][385/391]\tLoss 0.2017 (0.2228)\tPrec@1 90.625 (92.054)\n",
            "Test\t  Prec@1: 87.060 (Err: 12.940 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [68][0/391]\tLoss 0.1939 (0.1939)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [68][55/391]\tLoss 0.2106 (0.2010)\tPrec@1 92.969 (92.913)\n",
            "Epoch: [68][110/391]\tLoss 0.2052 (0.1983)\tPrec@1 96.094 (93.117)\n",
            "Epoch: [68][165/391]\tLoss 0.1317 (0.2011)\tPrec@1 96.094 (93.049)\n",
            "Epoch: [68][220/391]\tLoss 0.2316 (0.2071)\tPrec@1 92.969 (92.827)\n",
            "Epoch: [68][275/391]\tLoss 0.1894 (0.2110)\tPrec@1 91.406 (92.629)\n",
            "Epoch: [68][330/391]\tLoss 0.3491 (0.2139)\tPrec@1 85.156 (92.530)\n",
            "Epoch: [68][385/391]\tLoss 0.1816 (0.2167)\tPrec@1 91.406 (92.412)\n",
            "Test\t  Prec@1: 84.860 (Err: 15.140 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [69][0/391]\tLoss 0.2150 (0.2150)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [69][55/391]\tLoss 0.1265 (0.2128)\tPrec@1 96.094 (92.690)\n",
            "Epoch: [69][110/391]\tLoss 0.1057 (0.2093)\tPrec@1 96.094 (92.708)\n",
            "Epoch: [69][165/391]\tLoss 0.1672 (0.2114)\tPrec@1 95.312 (92.597)\n",
            "Epoch: [69][220/391]\tLoss 0.2481 (0.2160)\tPrec@1 92.188 (92.414)\n",
            "Epoch: [69][275/391]\tLoss 0.1996 (0.2196)\tPrec@1 93.750 (92.326)\n",
            "Epoch: [69][330/391]\tLoss 0.2166 (0.2210)\tPrec@1 91.406 (92.280)\n",
            "Epoch: [69][385/391]\tLoss 0.2853 (0.2215)\tPrec@1 91.406 (92.246)\n",
            "Test\t  Prec@1: 86.460 (Err: 13.540 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [70][0/391]\tLoss 0.2330 (0.2330)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [70][55/391]\tLoss 0.1575 (0.1990)\tPrec@1 94.531 (93.136)\n",
            "Epoch: [70][110/391]\tLoss 0.2676 (0.2052)\tPrec@1 92.188 (92.955)\n",
            "Epoch: [70][165/391]\tLoss 0.2250 (0.2093)\tPrec@1 92.188 (92.668)\n",
            "Epoch: [70][220/391]\tLoss 0.2825 (0.2169)\tPrec@1 92.188 (92.393)\n",
            "Epoch: [70][275/391]\tLoss 0.1876 (0.2216)\tPrec@1 92.969 (92.272)\n",
            "Epoch: [70][330/391]\tLoss 0.3002 (0.2207)\tPrec@1 90.625 (92.357)\n",
            "Epoch: [70][385/391]\tLoss 0.1998 (0.2199)\tPrec@1 94.531 (92.358)\n",
            "Test\t  Prec@1: 86.510 (Err: 13.490 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [71][0/391]\tLoss 0.2457 (0.2457)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [71][55/391]\tLoss 0.1668 (0.2011)\tPrec@1 92.969 (92.997)\n",
            "Epoch: [71][110/391]\tLoss 0.1606 (0.2106)\tPrec@1 95.312 (92.610)\n",
            "Epoch: [71][165/391]\tLoss 0.1728 (0.2124)\tPrec@1 94.531 (92.470)\n",
            "Epoch: [71][220/391]\tLoss 0.1827 (0.2159)\tPrec@1 96.094 (92.347)\n",
            "Epoch: [71][275/391]\tLoss 0.2758 (0.2182)\tPrec@1 89.062 (92.272)\n",
            "Epoch: [71][330/391]\tLoss 0.2110 (0.2212)\tPrec@1 92.188 (92.169)\n",
            "Epoch: [71][385/391]\tLoss 0.1811 (0.2215)\tPrec@1 95.312 (92.175)\n",
            "Test\t  Prec@1: 88.140 (Err: 11.860 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [72][0/391]\tLoss 0.1977 (0.1977)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [72][55/391]\tLoss 0.1660 (0.1940)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [72][110/391]\tLoss 0.2112 (0.2097)\tPrec@1 92.969 (92.462)\n",
            "Epoch: [72][165/391]\tLoss 0.2833 (0.2098)\tPrec@1 89.844 (92.489)\n",
            "Epoch: [72][220/391]\tLoss 0.3593 (0.2093)\tPrec@1 91.406 (92.569)\n",
            "Epoch: [72][275/391]\tLoss 0.2631 (0.2096)\tPrec@1 86.719 (92.581)\n",
            "Epoch: [72][330/391]\tLoss 0.2345 (0.2106)\tPrec@1 92.188 (92.551)\n",
            "Epoch: [72][385/391]\tLoss 0.2284 (0.2138)\tPrec@1 92.188 (92.489)\n",
            "Test\t  Prec@1: 87.040 (Err: 12.960 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [73][0/391]\tLoss 0.1776 (0.1776)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [73][55/391]\tLoss 0.2141 (0.2008)\tPrec@1 92.188 (92.801)\n",
            "Epoch: [73][110/391]\tLoss 0.2567 (0.2044)\tPrec@1 90.625 (92.765)\n",
            "Epoch: [73][165/391]\tLoss 0.2657 (0.2151)\tPrec@1 90.625 (92.338)\n",
            "Epoch: [73][220/391]\tLoss 0.1960 (0.2164)\tPrec@1 89.844 (92.340)\n",
            "Epoch: [73][275/391]\tLoss 0.2398 (0.2169)\tPrec@1 90.625 (92.309)\n",
            "Epoch: [73][330/391]\tLoss 0.1204 (0.2155)\tPrec@1 97.656 (92.369)\n",
            "Epoch: [73][385/391]\tLoss 0.2655 (0.2149)\tPrec@1 85.938 (92.430)\n",
            "Test\t  Prec@1: 85.000 (Err: 15.000 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [74][0/391]\tLoss 0.1631 (0.1631)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [74][55/391]\tLoss 0.1197 (0.1844)\tPrec@1 95.312 (93.359)\n",
            "Epoch: [74][110/391]\tLoss 0.1866 (0.1950)\tPrec@1 94.531 (93.039)\n",
            "Epoch: [74][165/391]\tLoss 0.2018 (0.1987)\tPrec@1 92.188 (92.861)\n",
            "Epoch: [74][220/391]\tLoss 0.1792 (0.2051)\tPrec@1 92.188 (92.661)\n",
            "Epoch: [74][275/391]\tLoss 0.1601 (0.2071)\tPrec@1 96.094 (92.536)\n",
            "Epoch: [74][330/391]\tLoss 0.3257 (0.2071)\tPrec@1 87.500 (92.601)\n",
            "Epoch: [74][385/391]\tLoss 0.2738 (0.2120)\tPrec@1 89.844 (92.438)\n",
            "Test\t  Prec@1: 86.250 (Err: 13.750 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [75][0/391]\tLoss 0.1653 (0.1653)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [75][55/391]\tLoss 0.1210 (0.2042)\tPrec@1 95.312 (92.857)\n",
            "Epoch: [75][110/391]\tLoss 0.2127 (0.2052)\tPrec@1 92.188 (92.877)\n",
            "Epoch: [75][165/391]\tLoss 0.2560 (0.2043)\tPrec@1 90.625 (92.941)\n",
            "Epoch: [75][220/391]\tLoss 0.2974 (0.2082)\tPrec@1 90.625 (92.859)\n",
            "Epoch: [75][275/391]\tLoss 0.2525 (0.2109)\tPrec@1 92.188 (92.759)\n",
            "Epoch: [75][330/391]\tLoss 0.1598 (0.2126)\tPrec@1 94.531 (92.676)\n",
            "Epoch: [75][385/391]\tLoss 0.2022 (0.2141)\tPrec@1 90.625 (92.641)\n",
            "Test\t  Prec@1: 85.690 (Err: 14.310 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [76][0/391]\tLoss 0.2428 (0.2428)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [76][55/391]\tLoss 0.2618 (0.1858)\tPrec@1 91.406 (93.569)\n",
            "Epoch: [76][110/391]\tLoss 0.2237 (0.1884)\tPrec@1 92.188 (93.511)\n",
            "Epoch: [76][165/391]\tLoss 0.1433 (0.1928)\tPrec@1 94.531 (93.199)\n",
            "Epoch: [76][220/391]\tLoss 0.1710 (0.2003)\tPrec@1 96.875 (92.944)\n",
            "Epoch: [76][275/391]\tLoss 0.1812 (0.2072)\tPrec@1 94.531 (92.739)\n",
            "Epoch: [76][330/391]\tLoss 0.2031 (0.2116)\tPrec@1 91.406 (92.520)\n",
            "Epoch: [76][385/391]\tLoss 0.1881 (0.2143)\tPrec@1 92.969 (92.424)\n",
            "Test\t  Prec@1: 85.820 (Err: 14.180 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [77][0/391]\tLoss 0.2402 (0.2402)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [77][55/391]\tLoss 0.1410 (0.1976)\tPrec@1 95.312 (93.290)\n",
            "Epoch: [77][110/391]\tLoss 0.2349 (0.2050)\tPrec@1 89.844 (92.905)\n",
            "Epoch: [77][165/391]\tLoss 0.2422 (0.2086)\tPrec@1 90.625 (92.715)\n",
            "Epoch: [77][220/391]\tLoss 0.2277 (0.2079)\tPrec@1 92.969 (92.750)\n",
            "Epoch: [77][275/391]\tLoss 0.3294 (0.2090)\tPrec@1 90.625 (92.601)\n",
            "Epoch: [77][330/391]\tLoss 0.2643 (0.2076)\tPrec@1 89.062 (92.660)\n",
            "Epoch: [77][385/391]\tLoss 0.2278 (0.2115)\tPrec@1 91.406 (92.554)\n",
            "Test\t  Prec@1: 86.240 (Err: 13.760 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [78][0/391]\tLoss 0.1747 (0.1747)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [78][55/391]\tLoss 0.2210 (0.1949)\tPrec@1 89.844 (93.318)\n",
            "Epoch: [78][110/391]\tLoss 0.1909 (0.1997)\tPrec@1 92.969 (92.884)\n",
            "Epoch: [78][165/391]\tLoss 0.1933 (0.2079)\tPrec@1 95.312 (92.710)\n",
            "Epoch: [78][220/391]\tLoss 0.2515 (0.2101)\tPrec@1 89.062 (92.686)\n",
            "Epoch: [78][275/391]\tLoss 0.1574 (0.2126)\tPrec@1 93.750 (92.601)\n",
            "Epoch: [78][330/391]\tLoss 0.1288 (0.2137)\tPrec@1 94.531 (92.624)\n",
            "Epoch: [78][385/391]\tLoss 0.3675 (0.2141)\tPrec@1 82.812 (92.576)\n",
            "Test\t  Prec@1: 84.680 (Err: 15.320 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [79][0/391]\tLoss 0.1198 (0.1198)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [79][55/391]\tLoss 0.3398 (0.1992)\tPrec@1 86.719 (92.899)\n",
            "Epoch: [79][110/391]\tLoss 0.2805 (0.2112)\tPrec@1 90.625 (92.420)\n",
            "Epoch: [79][165/391]\tLoss 0.1896 (0.2096)\tPrec@1 92.969 (92.498)\n",
            "Epoch: [79][220/391]\tLoss 0.3590 (0.2095)\tPrec@1 88.281 (92.545)\n",
            "Epoch: [79][275/391]\tLoss 0.2306 (0.2130)\tPrec@1 92.969 (92.405)\n",
            "Epoch: [79][330/391]\tLoss 0.2798 (0.2156)\tPrec@1 88.281 (92.291)\n",
            "Epoch: [79][385/391]\tLoss 0.2376 (0.2172)\tPrec@1 94.531 (92.264)\n",
            "Test\t  Prec@1: 87.650 (Err: 12.350 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [80][0/391]\tLoss 0.1186 (0.1186)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [80][55/391]\tLoss 0.1730 (0.1931)\tPrec@1 95.312 (93.345)\n",
            "Epoch: [80][110/391]\tLoss 0.2198 (0.1898)\tPrec@1 92.188 (93.468)\n",
            "Epoch: [80][165/391]\tLoss 0.1302 (0.1980)\tPrec@1 94.531 (93.213)\n",
            "Epoch: [80][220/391]\tLoss 0.1360 (0.2014)\tPrec@1 95.312 (93.022)\n",
            "Epoch: [80][275/391]\tLoss 0.2466 (0.2046)\tPrec@1 92.969 (92.918)\n",
            "Epoch: [80][330/391]\tLoss 0.2051 (0.2083)\tPrec@1 92.188 (92.778)\n",
            "Epoch: [80][385/391]\tLoss 0.1342 (0.2111)\tPrec@1 96.094 (92.710)\n",
            "Test\t  Prec@1: 85.840 (Err: 14.160 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [81][0/391]\tLoss 0.2107 (0.2107)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [81][55/391]\tLoss 0.2417 (0.2073)\tPrec@1 92.188 (92.885)\n",
            "Epoch: [81][110/391]\tLoss 0.1607 (0.2008)\tPrec@1 92.969 (93.011)\n",
            "Epoch: [81][165/391]\tLoss 0.1964 (0.2039)\tPrec@1 93.750 (92.969)\n",
            "Epoch: [81][220/391]\tLoss 0.2681 (0.2040)\tPrec@1 91.406 (92.912)\n",
            "Epoch: [81][275/391]\tLoss 0.2942 (0.2080)\tPrec@1 88.281 (92.689)\n",
            "Epoch: [81][330/391]\tLoss 0.2575 (0.2126)\tPrec@1 92.969 (92.556)\n",
            "Epoch: [81][385/391]\tLoss 0.2558 (0.2118)\tPrec@1 89.844 (92.592)\n",
            "Test\t  Prec@1: 88.190 (Err: 11.810 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [82][0/391]\tLoss 0.1425 (0.1425)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [82][55/391]\tLoss 0.2437 (0.1892)\tPrec@1 91.406 (93.122)\n",
            "Epoch: [82][110/391]\tLoss 0.1154 (0.1964)\tPrec@1 96.875 (93.018)\n",
            "Epoch: [82][165/391]\tLoss 0.1946 (0.1993)\tPrec@1 94.531 (92.917)\n",
            "Epoch: [82][220/391]\tLoss 0.1532 (0.2006)\tPrec@1 96.094 (92.926)\n",
            "Epoch: [82][275/391]\tLoss 0.2916 (0.2042)\tPrec@1 89.844 (92.805)\n",
            "Epoch: [82][330/391]\tLoss 0.1730 (0.2050)\tPrec@1 94.531 (92.766)\n",
            "Epoch: [82][385/391]\tLoss 0.0847 (0.2061)\tPrec@1 98.438 (92.689)\n",
            "Test\t  Prec@1: 88.770 (Err: 11.230 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [83][0/391]\tLoss 0.3036 (0.3036)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [83][55/391]\tLoss 0.1888 (0.2079)\tPrec@1 90.625 (92.439)\n",
            "Epoch: [83][110/391]\tLoss 0.1480 (0.2085)\tPrec@1 95.312 (92.589)\n",
            "Epoch: [83][165/391]\tLoss 0.3125 (0.2092)\tPrec@1 91.406 (92.625)\n",
            "Epoch: [83][220/391]\tLoss 0.1804 (0.2083)\tPrec@1 94.531 (92.700)\n",
            "Epoch: [83][275/391]\tLoss 0.3028 (0.2082)\tPrec@1 90.625 (92.771)\n",
            "Epoch: [83][330/391]\tLoss 0.2961 (0.2131)\tPrec@1 89.844 (92.634)\n",
            "Epoch: [83][385/391]\tLoss 0.1917 (0.2152)\tPrec@1 93.750 (92.598)\n",
            "Test\t  Prec@1: 88.230 (Err: 11.770 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [84][0/391]\tLoss 0.1826 (0.1826)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [84][55/391]\tLoss 0.1379 (0.1889)\tPrec@1 95.312 (93.331)\n",
            "Epoch: [84][110/391]\tLoss 0.2104 (0.1921)\tPrec@1 93.750 (93.342)\n",
            "Epoch: [84][165/391]\tLoss 0.1966 (0.2001)\tPrec@1 92.188 (93.030)\n",
            "Epoch: [84][220/391]\tLoss 0.2560 (0.2033)\tPrec@1 90.625 (92.873)\n",
            "Epoch: [84][275/391]\tLoss 0.2386 (0.2055)\tPrec@1 91.406 (92.785)\n",
            "Epoch: [84][330/391]\tLoss 0.3797 (0.2090)\tPrec@1 85.156 (92.610)\n",
            "Epoch: [84][385/391]\tLoss 0.1878 (0.2088)\tPrec@1 94.531 (92.600)\n",
            "Test\t  Prec@1: 85.590 (Err: 14.410 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [85][0/391]\tLoss 0.2803 (0.2803)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [85][55/391]\tLoss 0.2196 (0.2043)\tPrec@1 92.188 (92.941)\n",
            "Epoch: [85][110/391]\tLoss 0.1711 (0.1994)\tPrec@1 95.312 (93.124)\n",
            "Epoch: [85][165/391]\tLoss 0.2209 (0.1985)\tPrec@1 90.625 (93.133)\n",
            "Epoch: [85][220/391]\tLoss 0.2229 (0.2024)\tPrec@1 92.969 (93.047)\n",
            "Epoch: [85][275/391]\tLoss 0.1024 (0.2057)\tPrec@1 96.875 (92.861)\n",
            "Epoch: [85][330/391]\tLoss 0.1941 (0.2068)\tPrec@1 92.969 (92.834)\n",
            "Epoch: [85][385/391]\tLoss 0.2590 (0.2087)\tPrec@1 89.062 (92.754)\n",
            "Test\t  Prec@1: 86.920 (Err: 13.080 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [86][0/391]\tLoss 0.1635 (0.1635)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [86][55/391]\tLoss 0.2350 (0.1945)\tPrec@1 91.406 (93.345)\n",
            "Epoch: [86][110/391]\tLoss 0.2214 (0.2010)\tPrec@1 89.844 (92.870)\n",
            "Epoch: [86][165/391]\tLoss 0.2086 (0.2022)\tPrec@1 92.969 (92.762)\n",
            "Epoch: [86][220/391]\tLoss 0.1480 (0.2030)\tPrec@1 94.531 (92.845)\n",
            "Epoch: [86][275/391]\tLoss 0.2825 (0.2040)\tPrec@1 91.406 (92.734)\n",
            "Epoch: [86][330/391]\tLoss 0.2394 (0.2096)\tPrec@1 94.531 (92.612)\n",
            "Epoch: [86][385/391]\tLoss 0.2149 (0.2094)\tPrec@1 92.969 (92.615)\n",
            "Test\t  Prec@1: 86.760 (Err: 13.240 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [87][0/391]\tLoss 0.1083 (0.1083)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [87][55/391]\tLoss 0.1174 (0.1933)\tPrec@1 95.312 (93.094)\n",
            "Epoch: [87][110/391]\tLoss 0.2461 (0.1936)\tPrec@1 91.406 (93.208)\n",
            "Epoch: [87][165/391]\tLoss 0.2018 (0.1956)\tPrec@1 90.625 (93.072)\n",
            "Epoch: [87][220/391]\tLoss 0.1816 (0.1965)\tPrec@1 92.969 (93.039)\n",
            "Epoch: [87][275/391]\tLoss 0.1304 (0.2002)\tPrec@1 95.312 (92.847)\n",
            "Epoch: [87][330/391]\tLoss 0.2396 (0.2012)\tPrec@1 92.188 (92.837)\n",
            "Epoch: [87][385/391]\tLoss 0.3055 (0.2058)\tPrec@1 89.844 (92.704)\n",
            "Test\t  Prec@1: 86.830 (Err: 13.170 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [88][0/391]\tLoss 0.1980 (0.1980)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [88][55/391]\tLoss 0.1766 (0.2028)\tPrec@1 94.531 (92.815)\n",
            "Epoch: [88][110/391]\tLoss 0.1124 (0.1978)\tPrec@1 96.875 (93.131)\n",
            "Epoch: [88][165/391]\tLoss 0.1888 (0.1998)\tPrec@1 94.531 (93.148)\n",
            "Epoch: [88][220/391]\tLoss 0.1115 (0.1988)\tPrec@1 97.656 (93.146)\n",
            "Epoch: [88][275/391]\tLoss 0.1612 (0.2023)\tPrec@1 95.312 (93.037)\n",
            "Epoch: [88][330/391]\tLoss 0.2633 (0.2022)\tPrec@1 92.188 (92.976)\n",
            "Epoch: [88][385/391]\tLoss 0.2344 (0.2029)\tPrec@1 89.844 (92.967)\n",
            "Test\t  Prec@1: 87.530 (Err: 12.470 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [89][0/391]\tLoss 0.1989 (0.1989)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [89][55/391]\tLoss 0.2470 (0.1965)\tPrec@1 93.750 (92.955)\n",
            "Epoch: [89][110/391]\tLoss 0.1929 (0.2055)\tPrec@1 92.969 (92.722)\n",
            "Epoch: [89][165/391]\tLoss 0.1375 (0.2063)\tPrec@1 93.750 (92.559)\n",
            "Epoch: [89][220/391]\tLoss 0.1780 (0.2087)\tPrec@1 95.312 (92.435)\n",
            "Epoch: [89][275/391]\tLoss 0.2845 (0.2084)\tPrec@1 88.281 (92.507)\n",
            "Epoch: [89][330/391]\tLoss 0.1793 (0.2082)\tPrec@1 96.094 (92.579)\n",
            "Epoch: [89][385/391]\tLoss 0.2440 (0.2088)\tPrec@1 92.188 (92.574)\n",
            "Test\t  Prec@1: 87.270 (Err: 12.730 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [90][0/391]\tLoss 0.2114 (0.2114)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [90][55/391]\tLoss 0.2124 (0.1880)\tPrec@1 91.406 (93.457)\n",
            "Epoch: [90][110/391]\tLoss 0.2195 (0.1932)\tPrec@1 92.969 (93.229)\n",
            "Epoch: [90][165/391]\tLoss 0.2281 (0.1935)\tPrec@1 92.188 (93.279)\n",
            "Epoch: [90][220/391]\tLoss 0.2049 (0.2004)\tPrec@1 93.750 (93.018)\n",
            "Epoch: [90][275/391]\tLoss 0.2060 (0.2023)\tPrec@1 92.969 (92.887)\n",
            "Epoch: [90][330/391]\tLoss 0.1803 (0.2081)\tPrec@1 91.406 (92.634)\n",
            "Epoch: [90][385/391]\tLoss 0.2435 (0.2118)\tPrec@1 89.844 (92.461)\n",
            "Test\t  Prec@1: 85.600 (Err: 14.400 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [91][0/391]\tLoss 0.2552 (0.2552)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [91][55/391]\tLoss 0.1688 (0.2060)\tPrec@1 94.531 (92.829)\n",
            "Epoch: [91][110/391]\tLoss 0.1131 (0.2022)\tPrec@1 95.312 (92.962)\n",
            "Epoch: [91][165/391]\tLoss 0.1607 (0.1974)\tPrec@1 96.094 (93.072)\n",
            "Epoch: [91][220/391]\tLoss 0.1923 (0.1977)\tPrec@1 91.406 (93.036)\n",
            "Epoch: [91][275/391]\tLoss 0.2608 (0.1998)\tPrec@1 93.750 (93.020)\n",
            "Epoch: [91][330/391]\tLoss 0.2328 (0.2021)\tPrec@1 89.844 (92.926)\n",
            "Epoch: [91][385/391]\tLoss 0.2345 (0.2057)\tPrec@1 91.406 (92.829)\n",
            "Test\t  Prec@1: 85.790 (Err: 14.210 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [92][0/391]\tLoss 0.2685 (0.2685)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [92][55/391]\tLoss 0.1280 (0.1936)\tPrec@1 95.312 (93.094)\n",
            "Epoch: [92][110/391]\tLoss 0.2697 (0.1975)\tPrec@1 89.844 (93.032)\n",
            "Epoch: [92][165/391]\tLoss 0.4227 (0.2027)\tPrec@1 86.719 (92.912)\n",
            "Epoch: [92][220/391]\tLoss 0.1391 (0.2031)\tPrec@1 93.750 (92.887)\n",
            "Epoch: [92][275/391]\tLoss 0.2038 (0.2049)\tPrec@1 92.969 (92.788)\n",
            "Epoch: [92][330/391]\tLoss 0.2450 (0.2071)\tPrec@1 90.625 (92.700)\n",
            "Epoch: [92][385/391]\tLoss 0.1949 (0.2072)\tPrec@1 92.188 (92.689)\n",
            "Test\t  Prec@1: 86.110 (Err: 13.890 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [93][0/391]\tLoss 0.2218 (0.2218)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [93][55/391]\tLoss 0.1644 (0.1891)\tPrec@1 94.531 (93.331)\n",
            "Epoch: [93][110/391]\tLoss 0.1341 (0.1919)\tPrec@1 94.531 (93.229)\n",
            "Epoch: [93][165/391]\tLoss 0.1777 (0.1917)\tPrec@1 94.531 (93.237)\n",
            "Epoch: [93][220/391]\tLoss 0.1313 (0.1933)\tPrec@1 96.094 (93.170)\n",
            "Epoch: [93][275/391]\tLoss 0.2202 (0.1992)\tPrec@1 91.406 (92.943)\n",
            "Epoch: [93][330/391]\tLoss 0.1803 (0.1998)\tPrec@1 92.969 (92.886)\n",
            "Epoch: [93][385/391]\tLoss 0.1864 (0.2025)\tPrec@1 92.969 (92.774)\n",
            "Test\t  Prec@1: 86.850 (Err: 13.150 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [94][0/391]\tLoss 0.1985 (0.1985)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [94][55/391]\tLoss 0.1436 (0.1845)\tPrec@1 92.969 (93.499)\n",
            "Epoch: [94][110/391]\tLoss 0.2374 (0.1928)\tPrec@1 93.750 (93.124)\n",
            "Epoch: [94][165/391]\tLoss 0.2609 (0.2005)\tPrec@1 91.406 (93.006)\n",
            "Epoch: [94][220/391]\tLoss 0.2893 (0.2039)\tPrec@1 89.844 (92.905)\n",
            "Epoch: [94][275/391]\tLoss 0.1460 (0.2029)\tPrec@1 95.312 (92.912)\n",
            "Epoch: [94][330/391]\tLoss 0.1779 (0.2035)\tPrec@1 93.750 (92.896)\n",
            "Epoch: [94][385/391]\tLoss 0.1159 (0.2057)\tPrec@1 96.094 (92.778)\n",
            "Test\t  Prec@1: 84.890 (Err: 15.110 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [95][0/391]\tLoss 0.1989 (0.1989)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [95][55/391]\tLoss 0.1566 (0.1779)\tPrec@1 95.312 (94.015)\n",
            "Epoch: [95][110/391]\tLoss 0.1415 (0.1966)\tPrec@1 96.875 (93.342)\n",
            "Epoch: [95][165/391]\tLoss 0.1835 (0.1954)\tPrec@1 96.094 (93.265)\n",
            "Epoch: [95][220/391]\tLoss 0.1538 (0.1984)\tPrec@1 93.750 (93.153)\n",
            "Epoch: [95][275/391]\tLoss 0.1768 (0.2037)\tPrec@1 95.312 (92.952)\n",
            "Epoch: [95][330/391]\tLoss 0.1707 (0.2040)\tPrec@1 94.531 (92.929)\n",
            "Epoch: [95][385/391]\tLoss 0.2963 (0.2034)\tPrec@1 90.625 (92.967)\n",
            "Test\t  Prec@1: 87.330 (Err: 12.670 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [96][0/391]\tLoss 0.2132 (0.2132)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [96][55/391]\tLoss 0.3046 (0.2084)\tPrec@1 91.406 (92.453)\n",
            "Epoch: [96][110/391]\tLoss 0.2096 (0.2020)\tPrec@1 94.531 (92.793)\n",
            "Epoch: [96][165/391]\tLoss 0.2210 (0.2051)\tPrec@1 93.750 (92.686)\n",
            "Epoch: [96][220/391]\tLoss 0.2106 (0.2073)\tPrec@1 91.406 (92.573)\n",
            "Epoch: [96][275/391]\tLoss 0.1924 (0.2067)\tPrec@1 91.406 (92.598)\n",
            "Epoch: [96][330/391]\tLoss 0.2282 (0.2064)\tPrec@1 93.750 (92.586)\n",
            "Epoch: [96][385/391]\tLoss 0.2105 (0.2068)\tPrec@1 92.969 (92.582)\n",
            "Test\t  Prec@1: 86.110 (Err: 13.890 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [97][0/391]\tLoss 0.1318 (0.1318)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [97][55/391]\tLoss 0.1830 (0.2087)\tPrec@1 92.188 (92.453)\n",
            "Epoch: [97][110/391]\tLoss 0.1185 (0.2036)\tPrec@1 96.875 (92.786)\n",
            "Epoch: [97][165/391]\tLoss 0.3166 (0.1986)\tPrec@1 88.281 (93.096)\n",
            "Epoch: [97][220/391]\tLoss 0.1446 (0.1959)\tPrec@1 96.094 (93.174)\n",
            "Epoch: [97][275/391]\tLoss 0.1316 (0.1955)\tPrec@1 94.531 (93.184)\n",
            "Epoch: [97][330/391]\tLoss 0.2498 (0.1973)\tPrec@1 91.406 (93.099)\n",
            "Epoch: [97][385/391]\tLoss 0.1888 (0.2009)\tPrec@1 92.969 (92.967)\n",
            "Test\t  Prec@1: 86.960 (Err: 13.040 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [98][0/391]\tLoss 0.2677 (0.2677)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [98][55/391]\tLoss 0.1560 (0.1938)\tPrec@1 95.312 (93.276)\n",
            "Epoch: [98][110/391]\tLoss 0.1703 (0.1824)\tPrec@1 95.312 (93.602)\n",
            "Epoch: [98][165/391]\tLoss 0.3417 (0.1880)\tPrec@1 85.938 (93.496)\n",
            "Epoch: [98][220/391]\tLoss 0.1210 (0.1964)\tPrec@1 96.875 (93.177)\n",
            "Epoch: [98][275/391]\tLoss 0.1604 (0.1987)\tPrec@1 93.750 (93.099)\n",
            "Epoch: [98][330/391]\tLoss 0.2258 (0.2015)\tPrec@1 92.188 (93.023)\n",
            "Epoch: [98][385/391]\tLoss 0.2133 (0.2002)\tPrec@1 92.969 (93.031)\n",
            "Test\t  Prec@1: 87.320 (Err: 12.680 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [99][0/391]\tLoss 0.1652 (0.1652)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [99][55/391]\tLoss 0.2992 (0.1774)\tPrec@1 85.938 (93.652)\n",
            "Epoch: [99][110/391]\tLoss 0.3369 (0.1819)\tPrec@1 89.844 (93.490)\n",
            "Epoch: [99][165/391]\tLoss 0.1576 (0.1893)\tPrec@1 96.094 (93.284)\n",
            "Epoch: [99][220/391]\tLoss 0.1627 (0.1919)\tPrec@1 92.969 (93.220)\n",
            "Epoch: [99][275/391]\tLoss 0.2768 (0.1934)\tPrec@1 91.406 (93.156)\n",
            "Epoch: [99][330/391]\tLoss 0.2174 (0.1961)\tPrec@1 92.969 (93.051)\n",
            "Epoch: [99][385/391]\tLoss 0.1059 (0.1993)\tPrec@1 97.656 (92.940)\n",
            "Test\t  Prec@1: 86.430 (Err: 13.570 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [100][0/391]\tLoss 0.1691 (0.1691)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [100][55/391]\tLoss 0.1116 (0.1654)\tPrec@1 95.312 (94.448)\n",
            "Epoch: [100][110/391]\tLoss 0.0916 (0.1500)\tPrec@1 96.094 (94.947)\n",
            "Epoch: [100][165/391]\tLoss 0.1168 (0.1378)\tPrec@1 95.312 (95.407)\n",
            "Epoch: [100][220/391]\tLoss 0.0740 (0.1329)\tPrec@1 100.000 (95.549)\n",
            "Epoch: [100][275/391]\tLoss 0.0774 (0.1278)\tPrec@1 97.656 (95.709)\n",
            "Epoch: [100][330/391]\tLoss 0.1319 (0.1258)\tPrec@1 94.531 (95.723)\n",
            "Epoch: [100][385/391]\tLoss 0.0638 (0.1229)\tPrec@1 98.438 (95.825)\n",
            "Test\t  Prec@1: 90.950 (Err: 9.050 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [101][0/391]\tLoss 0.0682 (0.0682)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [101][55/391]\tLoss 0.0591 (0.0955)\tPrec@1 97.656 (96.973)\n",
            "Epoch: [101][110/391]\tLoss 0.0883 (0.0978)\tPrec@1 97.656 (96.784)\n",
            "Epoch: [101][165/391]\tLoss 0.0913 (0.0963)\tPrec@1 96.875 (96.847)\n",
            "Epoch: [101][220/391]\tLoss 0.1140 (0.0952)\tPrec@1 96.094 (96.871)\n",
            "Epoch: [101][275/391]\tLoss 0.0886 (0.0940)\tPrec@1 96.094 (96.900)\n",
            "Epoch: [101][330/391]\tLoss 0.0828 (0.0945)\tPrec@1 96.875 (96.866)\n",
            "Epoch: [101][385/391]\tLoss 0.1103 (0.0949)\tPrec@1 94.531 (96.841)\n",
            "Test\t  Prec@1: 91.230 (Err: 8.770 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [102][0/391]\tLoss 0.0778 (0.0778)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [102][55/391]\tLoss 0.1170 (0.0899)\tPrec@1 96.875 (96.931)\n",
            "Epoch: [102][110/391]\tLoss 0.0570 (0.0922)\tPrec@1 97.656 (96.903)\n",
            "Epoch: [102][165/391]\tLoss 0.0844 (0.0875)\tPrec@1 96.875 (97.101)\n",
            "Epoch: [102][220/391]\tLoss 0.0621 (0.0847)\tPrec@1 96.875 (97.218)\n",
            "Epoch: [102][275/391]\tLoss 0.0577 (0.0838)\tPrec@1 98.438 (97.263)\n",
            "Epoch: [102][330/391]\tLoss 0.0476 (0.0843)\tPrec@1 98.438 (97.241)\n",
            "Epoch: [102][385/391]\tLoss 0.0767 (0.0849)\tPrec@1 97.656 (97.187)\n",
            "Test\t  Prec@1: 91.430 (Err: 8.570 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [103][0/391]\tLoss 0.0872 (0.0872)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [103][55/391]\tLoss 0.0367 (0.0736)\tPrec@1 99.219 (97.726)\n",
            "Epoch: [103][110/391]\tLoss 0.0868 (0.0742)\tPrec@1 96.875 (97.621)\n",
            "Epoch: [103][165/391]\tLoss 0.0666 (0.0753)\tPrec@1 97.656 (97.609)\n",
            "Epoch: [103][220/391]\tLoss 0.0556 (0.0770)\tPrec@1 98.438 (97.522)\n",
            "Epoch: [103][275/391]\tLoss 0.0636 (0.0765)\tPrec@1 99.219 (97.518)\n",
            "Epoch: [103][330/391]\tLoss 0.1246 (0.0774)\tPrec@1 96.094 (97.484)\n",
            "Epoch: [103][385/391]\tLoss 0.0585 (0.0765)\tPrec@1 99.219 (97.509)\n",
            "Test\t  Prec@1: 91.550 (Err: 8.450 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [104][0/391]\tLoss 0.0656 (0.0656)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [104][55/391]\tLoss 0.0480 (0.0708)\tPrec@1 98.438 (97.712)\n",
            "Epoch: [104][110/391]\tLoss 0.0887 (0.0747)\tPrec@1 96.875 (97.523)\n",
            "Epoch: [104][165/391]\tLoss 0.0713 (0.0739)\tPrec@1 98.438 (97.529)\n",
            "Epoch: [104][220/391]\tLoss 0.0255 (0.0734)\tPrec@1 99.219 (97.547)\n",
            "Epoch: [104][275/391]\tLoss 0.0629 (0.0722)\tPrec@1 98.438 (97.574)\n",
            "Epoch: [104][330/391]\tLoss 0.1011 (0.0727)\tPrec@1 97.656 (97.557)\n",
            "Epoch: [104][385/391]\tLoss 0.0421 (0.0728)\tPrec@1 98.438 (97.557)\n",
            "Test\t  Prec@1: 91.640 (Err: 8.360 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [105][0/391]\tLoss 0.0775 (0.0775)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [105][55/391]\tLoss 0.0995 (0.0706)\tPrec@1 96.094 (97.503)\n",
            "Epoch: [105][110/391]\tLoss 0.0462 (0.0706)\tPrec@1 98.438 (97.656)\n",
            "Epoch: [105][165/391]\tLoss 0.1070 (0.0719)\tPrec@1 94.531 (97.548)\n",
            "Epoch: [105][220/391]\tLoss 0.0971 (0.0723)\tPrec@1 96.094 (97.522)\n",
            "Epoch: [105][275/391]\tLoss 0.0328 (0.0711)\tPrec@1 99.219 (97.608)\n",
            "Epoch: [105][330/391]\tLoss 0.0421 (0.0705)\tPrec@1 98.438 (97.630)\n",
            "Epoch: [105][385/391]\tLoss 0.0682 (0.0706)\tPrec@1 96.875 (97.630)\n",
            "Test\t  Prec@1: 91.560 (Err: 8.440 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [106][0/391]\tLoss 0.0484 (0.0484)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [106][55/391]\tLoss 0.0786 (0.0680)\tPrec@1 98.438 (97.810)\n",
            "Epoch: [106][110/391]\tLoss 0.0402 (0.0666)\tPrec@1 99.219 (97.727)\n",
            "Epoch: [106][165/391]\tLoss 0.0621 (0.0661)\tPrec@1 97.656 (97.797)\n",
            "Epoch: [106][220/391]\tLoss 0.1149 (0.0659)\tPrec@1 97.656 (97.837)\n",
            "Epoch: [106][275/391]\tLoss 0.0648 (0.0657)\tPrec@1 96.094 (97.860)\n",
            "Epoch: [106][330/391]\tLoss 0.0330 (0.0654)\tPrec@1 99.219 (97.876)\n",
            "Epoch: [106][385/391]\tLoss 0.0561 (0.0654)\tPrec@1 97.656 (97.857)\n",
            "Test\t  Prec@1: 91.620 (Err: 8.380 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [107][0/391]\tLoss 0.0664 (0.0664)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [107][55/391]\tLoss 0.0402 (0.0618)\tPrec@1 99.219 (97.879)\n",
            "Epoch: [107][110/391]\tLoss 0.0391 (0.0633)\tPrec@1 99.219 (97.797)\n",
            "Epoch: [107][165/391]\tLoss 0.0498 (0.0610)\tPrec@1 97.656 (97.910)\n",
            "Epoch: [107][220/391]\tLoss 0.1182 (0.0593)\tPrec@1 95.312 (97.981)\n",
            "Epoch: [107][275/391]\tLoss 0.1152 (0.0605)\tPrec@1 95.312 (97.931)\n",
            "Epoch: [107][330/391]\tLoss 0.0572 (0.0610)\tPrec@1 97.656 (97.928)\n",
            "Epoch: [107][385/391]\tLoss 0.0776 (0.0611)\tPrec@1 96.094 (97.927)\n",
            "Test\t  Prec@1: 91.520 (Err: 8.480 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [108][0/391]\tLoss 0.0398 (0.0398)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [108][55/391]\tLoss 0.0845 (0.0556)\tPrec@1 96.875 (98.354)\n",
            "Epoch: [108][110/391]\tLoss 0.0332 (0.0568)\tPrec@1 99.219 (98.205)\n",
            "Epoch: [108][165/391]\tLoss 0.0460 (0.0590)\tPrec@1 98.438 (98.094)\n",
            "Epoch: [108][220/391]\tLoss 0.0489 (0.0599)\tPrec@1 99.219 (98.063)\n",
            "Epoch: [108][275/391]\tLoss 0.0321 (0.0592)\tPrec@1 99.219 (98.120)\n",
            "Epoch: [108][330/391]\tLoss 0.0767 (0.0593)\tPrec@1 96.875 (98.081)\n",
            "Epoch: [108][385/391]\tLoss 0.0659 (0.0602)\tPrec@1 96.875 (98.031)\n",
            "Test\t  Prec@1: 91.520 (Err: 8.480 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [109][0/391]\tLoss 0.0314 (0.0314)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [109][55/391]\tLoss 0.1206 (0.0610)\tPrec@1 94.531 (97.991)\n",
            "Epoch: [109][110/391]\tLoss 0.0450 (0.0590)\tPrec@1 99.219 (98.079)\n",
            "Epoch: [109][165/391]\tLoss 0.0355 (0.0580)\tPrec@1 98.438 (98.089)\n",
            "Epoch: [109][220/391]\tLoss 0.0679 (0.0573)\tPrec@1 98.438 (98.088)\n",
            "Epoch: [109][275/391]\tLoss 0.0762 (0.0568)\tPrec@1 97.656 (98.137)\n",
            "Epoch: [109][330/391]\tLoss 0.0802 (0.0571)\tPrec@1 97.656 (98.133)\n",
            "Epoch: [109][385/391]\tLoss 0.0744 (0.0573)\tPrec@1 97.656 (98.124)\n",
            "Test\t  Prec@1: 91.670 (Err: 8.330 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [110][0/391]\tLoss 0.0410 (0.0410)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [110][55/391]\tLoss 0.0579 (0.0526)\tPrec@1 97.656 (98.200)\n",
            "Epoch: [110][110/391]\tLoss 0.0361 (0.0557)\tPrec@1 98.438 (98.149)\n",
            "Epoch: [110][165/391]\tLoss 0.0895 (0.0574)\tPrec@1 96.094 (98.028)\n",
            "Epoch: [110][220/391]\tLoss 0.0542 (0.0580)\tPrec@1 98.438 (98.006)\n",
            "Epoch: [110][275/391]\tLoss 0.0747 (0.0575)\tPrec@1 97.656 (98.044)\n",
            "Epoch: [110][330/391]\tLoss 0.0553 (0.0580)\tPrec@1 97.656 (98.017)\n",
            "Epoch: [110][385/391]\tLoss 0.0347 (0.0570)\tPrec@1 99.219 (98.067)\n",
            "Test\t  Prec@1: 91.640 (Err: 8.360 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [111][0/391]\tLoss 0.0355 (0.0355)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [111][55/391]\tLoss 0.0354 (0.0563)\tPrec@1 98.438 (98.158)\n",
            "Epoch: [111][110/391]\tLoss 0.0692 (0.0559)\tPrec@1 96.875 (98.156)\n",
            "Epoch: [111][165/391]\tLoss 0.0258 (0.0535)\tPrec@1 99.219 (98.339)\n",
            "Epoch: [111][220/391]\tLoss 0.0377 (0.0531)\tPrec@1 98.438 (98.317)\n",
            "Epoch: [111][275/391]\tLoss 0.0582 (0.0534)\tPrec@1 96.875 (98.248)\n",
            "Epoch: [111][330/391]\tLoss 0.0427 (0.0539)\tPrec@1 99.219 (98.218)\n",
            "Epoch: [111][385/391]\tLoss 0.0349 (0.0538)\tPrec@1 99.219 (98.209)\n",
            "Test\t  Prec@1: 91.520 (Err: 8.480 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [112][0/391]\tLoss 0.0230 (0.0230)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [112][55/391]\tLoss 0.0320 (0.0517)\tPrec@1 99.219 (98.298)\n",
            "Epoch: [112][110/391]\tLoss 0.0318 (0.0507)\tPrec@1 98.438 (98.283)\n",
            "Epoch: [112][165/391]\tLoss 0.0461 (0.0506)\tPrec@1 98.438 (98.320)\n",
            "Epoch: [112][220/391]\tLoss 0.0735 (0.0513)\tPrec@1 96.875 (98.300)\n",
            "Epoch: [112][275/391]\tLoss 0.0750 (0.0515)\tPrec@1 97.656 (98.273)\n",
            "Epoch: [112][330/391]\tLoss 0.0205 (0.0519)\tPrec@1 99.219 (98.303)\n",
            "Epoch: [112][385/391]\tLoss 0.0626 (0.0520)\tPrec@1 97.656 (98.280)\n",
            "Test\t  Prec@1: 91.650 (Err: 8.350 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [113][0/391]\tLoss 0.0200 (0.0200)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [113][55/391]\tLoss 0.0917 (0.0468)\tPrec@1 97.656 (98.465)\n",
            "Epoch: [113][110/391]\tLoss 0.0628 (0.0467)\tPrec@1 98.438 (98.473)\n",
            "Epoch: [113][165/391]\tLoss 0.0398 (0.0475)\tPrec@1 97.656 (98.494)\n",
            "Epoch: [113][220/391]\tLoss 0.0229 (0.0474)\tPrec@1 99.219 (98.483)\n",
            "Epoch: [113][275/391]\tLoss 0.0313 (0.0485)\tPrec@1 99.219 (98.421)\n",
            "Epoch: [113][330/391]\tLoss 0.1072 (0.0484)\tPrec@1 96.875 (98.428)\n",
            "Epoch: [113][385/391]\tLoss 0.0506 (0.0486)\tPrec@1 97.656 (98.431)\n",
            "Test\t  Prec@1: 91.880 (Err: 8.120 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [114][0/391]\tLoss 0.0727 (0.0727)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [114][55/391]\tLoss 0.0467 (0.0429)\tPrec@1 98.438 (98.758)\n",
            "Epoch: [114][110/391]\tLoss 0.0782 (0.0458)\tPrec@1 96.094 (98.592)\n",
            "Epoch: [114][165/391]\tLoss 0.0750 (0.0490)\tPrec@1 98.438 (98.456)\n",
            "Epoch: [114][220/391]\tLoss 0.0344 (0.0494)\tPrec@1 98.438 (98.420)\n",
            "Epoch: [114][275/391]\tLoss 0.0377 (0.0499)\tPrec@1 98.438 (98.389)\n",
            "Epoch: [114][330/391]\tLoss 0.0218 (0.0495)\tPrec@1 99.219 (98.397)\n",
            "Epoch: [114][385/391]\tLoss 0.0356 (0.0487)\tPrec@1 98.438 (98.419)\n",
            "Test\t  Prec@1: 91.710 (Err: 8.290 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [115][0/391]\tLoss 0.0898 (0.0898)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [115][55/391]\tLoss 0.0234 (0.0480)\tPrec@1 99.219 (98.493)\n",
            "Epoch: [115][110/391]\tLoss 0.0279 (0.0463)\tPrec@1 100.000 (98.501)\n",
            "Epoch: [115][165/391]\tLoss 0.0370 (0.0456)\tPrec@1 97.656 (98.532)\n",
            "Epoch: [115][220/391]\tLoss 0.0541 (0.0460)\tPrec@1 98.438 (98.505)\n",
            "Epoch: [115][275/391]\tLoss 0.0329 (0.0457)\tPrec@1 99.219 (98.505)\n",
            "Epoch: [115][330/391]\tLoss 0.0679 (0.0467)\tPrec@1 96.875 (98.454)\n",
            "Epoch: [115][385/391]\tLoss 0.0211 (0.0468)\tPrec@1 100.000 (98.454)\n",
            "Test\t  Prec@1: 91.580 (Err: 8.420 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [116][0/391]\tLoss 0.0400 (0.0400)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [116][55/391]\tLoss 0.0582 (0.0448)\tPrec@1 97.656 (98.424)\n",
            "Epoch: [116][110/391]\tLoss 0.0435 (0.0439)\tPrec@1 97.656 (98.501)\n",
            "Epoch: [116][165/391]\tLoss 0.0544 (0.0434)\tPrec@1 98.438 (98.532)\n",
            "Epoch: [116][220/391]\tLoss 0.0545 (0.0439)\tPrec@1 99.219 (98.533)\n",
            "Epoch: [116][275/391]\tLoss 0.0543 (0.0435)\tPrec@1 96.875 (98.539)\n",
            "Epoch: [116][330/391]\tLoss 0.0645 (0.0443)\tPrec@1 97.656 (98.532)\n",
            "Epoch: [116][385/391]\tLoss 0.0317 (0.0452)\tPrec@1 98.438 (98.480)\n",
            "Test\t  Prec@1: 91.730 (Err: 8.270 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [117][0/391]\tLoss 0.0374 (0.0374)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [117][55/391]\tLoss 0.0325 (0.0383)\tPrec@1 99.219 (98.647)\n",
            "Epoch: [117][110/391]\tLoss 0.0254 (0.0396)\tPrec@1 100.000 (98.768)\n",
            "Epoch: [117][165/391]\tLoss 0.0344 (0.0418)\tPrec@1 99.219 (98.649)\n",
            "Epoch: [117][220/391]\tLoss 0.0368 (0.0429)\tPrec@1 100.000 (98.607)\n",
            "Epoch: [117][330/391]\tLoss 0.1214 (0.0431)\tPrec@1 93.750 (98.605)\n",
            "Epoch: [117][385/391]\tLoss 0.0192 (0.0425)\tPrec@1 99.219 (98.622)\n",
            "Test\t  Prec@1: 91.610 (Err: 8.390 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [118][0/391]\tLoss 0.0239 (0.0239)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [118][55/391]\tLoss 0.0197 (0.0373)\tPrec@1 100.000 (98.856)\n",
            "Epoch: [118][110/391]\tLoss 0.0750 (0.0420)\tPrec@1 97.656 (98.670)\n",
            "Epoch: [118][165/391]\tLoss 0.0706 (0.0428)\tPrec@1 97.656 (98.621)\n",
            "Epoch: [118][220/391]\tLoss 0.0163 (0.0431)\tPrec@1 100.000 (98.590)\n",
            "Epoch: [118][275/391]\tLoss 0.0827 (0.0430)\tPrec@1 97.656 (98.590)\n",
            "Epoch: [118][330/391]\tLoss 0.0631 (0.0434)\tPrec@1 98.438 (98.570)\n",
            "Epoch: [118][385/391]\tLoss 0.0383 (0.0438)\tPrec@1 99.219 (98.563)\n",
            "Test\t  Prec@1: 91.690 (Err: 8.310 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [119][0/391]\tLoss 0.0212 (0.0212)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [119][55/391]\tLoss 0.0823 (0.0384)\tPrec@1 96.094 (98.814)\n",
            "Epoch: [119][110/391]\tLoss 0.0524 (0.0376)\tPrec@1 97.656 (98.881)\n",
            "Epoch: [119][165/391]\tLoss 0.0542 (0.0392)\tPrec@1 97.656 (98.805)\n",
            "Epoch: [119][220/391]\tLoss 0.0501 (0.0396)\tPrec@1 98.438 (98.791)\n",
            "Epoch: [119][275/391]\tLoss 0.0297 (0.0410)\tPrec@1 98.438 (98.701)\n",
            "Epoch: [119][330/391]\tLoss 0.0429 (0.0413)\tPrec@1 98.438 (98.671)\n",
            "Epoch: [119][385/391]\tLoss 0.0503 (0.0411)\tPrec@1 96.875 (98.676)\n",
            "Test\t  Prec@1: 91.630 (Err: 8.370 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [120][0/391]\tLoss 0.0608 (0.0608)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [120][55/391]\tLoss 0.0215 (0.0392)\tPrec@1 100.000 (98.744)\n",
            "Epoch: [120][110/391]\tLoss 0.0392 (0.0415)\tPrec@1 100.000 (98.719)\n",
            "Epoch: [120][165/391]\tLoss 0.0580 (0.0421)\tPrec@1 98.438 (98.692)\n",
            "Epoch: [120][220/391]\tLoss 0.0280 (0.0422)\tPrec@1 98.438 (98.660)\n",
            "Epoch: [120][275/391]\tLoss 0.0130 (0.0425)\tPrec@1 100.000 (98.636)\n",
            "Epoch: [120][330/391]\tLoss 0.0802 (0.0423)\tPrec@1 97.656 (98.648)\n",
            "Epoch: [120][385/391]\tLoss 0.0277 (0.0417)\tPrec@1 99.219 (98.680)\n",
            "Test\t  Prec@1: 91.570 (Err: 8.430 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [121][0/391]\tLoss 0.0171 (0.0171)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [121][55/391]\tLoss 0.0166 (0.0360)\tPrec@1 100.000 (98.898)\n",
            "Epoch: [121][110/391]\tLoss 0.0236 (0.0387)\tPrec@1 100.000 (98.846)\n",
            "Epoch: [121][165/391]\tLoss 0.0357 (0.0382)\tPrec@1 98.438 (98.847)\n",
            "Epoch: [121][220/391]\tLoss 0.0400 (0.0384)\tPrec@1 99.219 (98.773)\n",
            "Epoch: [121][275/391]\tLoss 0.0508 (0.0391)\tPrec@1 97.656 (98.723)\n",
            "Epoch: [121][330/391]\tLoss 0.0745 (0.0399)\tPrec@1 97.656 (98.690)\n",
            "Epoch: [121][385/391]\tLoss 0.0502 (0.0401)\tPrec@1 97.656 (98.682)\n",
            "Test\t  Prec@1: 91.670 (Err: 8.330 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [122][0/391]\tLoss 0.0326 (0.0326)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [122][55/391]\tLoss 0.0470 (0.0340)\tPrec@1 97.656 (98.856)\n",
            "Epoch: [122][110/391]\tLoss 0.0592 (0.0363)\tPrec@1 97.656 (98.803)\n",
            "Epoch: [122][165/391]\tLoss 0.0264 (0.0372)\tPrec@1 100.000 (98.805)\n",
            "Epoch: [122][220/391]\tLoss 0.0598 (0.0388)\tPrec@1 96.094 (98.734)\n",
            "Epoch: [122][275/391]\tLoss 0.0351 (0.0383)\tPrec@1 99.219 (98.746)\n",
            "Epoch: [122][330/391]\tLoss 0.0403 (0.0388)\tPrec@1 99.219 (98.744)\n",
            "Epoch: [122][385/391]\tLoss 0.0395 (0.0385)\tPrec@1 98.438 (98.759)\n",
            "Test\t  Prec@1: 91.610 (Err: 8.390 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [123][0/391]\tLoss 0.0338 (0.0338)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [123][55/391]\tLoss 0.0334 (0.0352)\tPrec@1 97.656 (98.856)\n",
            "Epoch: [123][110/391]\tLoss 0.0317 (0.0349)\tPrec@1 99.219 (98.923)\n",
            "Epoch: [123][165/391]\tLoss 0.0091 (0.0338)\tPrec@1 100.000 (98.946)\n",
            "Epoch: [123][220/391]\tLoss 0.0230 (0.0339)\tPrec@1 99.219 (98.925)\n",
            "Epoch: [123][275/391]\tLoss 0.0341 (0.0342)\tPrec@1 99.219 (98.916)\n",
            "Epoch: [123][330/391]\tLoss 0.0223 (0.0341)\tPrec@1 99.219 (98.898)\n",
            "Epoch: [123][385/391]\tLoss 0.0202 (0.0345)\tPrec@1 100.000 (98.889)\n",
            "Test\t  Prec@1: 91.830 (Err: 8.170 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [124][0/391]\tLoss 0.0245 (0.0245)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [124][55/391]\tLoss 0.0073 (0.0303)\tPrec@1 100.000 (99.135)\n",
            "Epoch: [124][110/391]\tLoss 0.0257 (0.0337)\tPrec@1 99.219 (98.965)\n",
            "Epoch: [124][165/391]\tLoss 0.0196 (0.0343)\tPrec@1 100.000 (98.960)\n",
            "Epoch: [124][220/391]\tLoss 0.0250 (0.0350)\tPrec@1 100.000 (98.936)\n",
            "Epoch: [124][275/391]\tLoss 0.0257 (0.0356)\tPrec@1 98.438 (98.888)\n",
            "Epoch: [124][330/391]\tLoss 0.0579 (0.0363)\tPrec@1 97.656 (98.867)\n",
            "Epoch: [124][385/391]\tLoss 0.0489 (0.0366)\tPrec@1 98.438 (98.848)\n",
            "Test\t  Prec@1: 91.710 (Err: 8.290 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [125][0/391]\tLoss 0.0323 (0.0323)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [125][55/391]\tLoss 0.0509 (0.0360)\tPrec@1 97.656 (98.689)\n",
            "Epoch: [125][110/391]\tLoss 0.0179 (0.0347)\tPrec@1 100.000 (98.839)\n",
            "Epoch: [125][165/391]\tLoss 0.0352 (0.0342)\tPrec@1 98.438 (98.885)\n",
            "Epoch: [125][220/391]\tLoss 0.0247 (0.0350)\tPrec@1 99.219 (98.858)\n",
            "Epoch: [125][275/391]\tLoss 0.0478 (0.0354)\tPrec@1 98.438 (98.831)\n",
            "Epoch: [125][330/391]\tLoss 0.0316 (0.0358)\tPrec@1 99.219 (98.858)\n",
            "Epoch: [125][385/391]\tLoss 0.0540 (0.0351)\tPrec@1 98.438 (98.921)\n",
            "Test\t  Prec@1: 91.770 (Err: 8.230 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [126][0/391]\tLoss 0.0251 (0.0251)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [126][55/391]\tLoss 0.0512 (0.0348)\tPrec@1 97.656 (98.884)\n",
            "Epoch: [126][110/391]\tLoss 0.0352 (0.0354)\tPrec@1 99.219 (98.867)\n",
            "Epoch: [126][165/391]\tLoss 0.0210 (0.0355)\tPrec@1 100.000 (98.870)\n",
            "Epoch: [126][220/391]\tLoss 0.0661 (0.0355)\tPrec@1 97.656 (98.851)\n",
            "Epoch: [126][275/391]\tLoss 0.0251 (0.0353)\tPrec@1 99.219 (98.865)\n",
            "Epoch: [126][330/391]\tLoss 0.0498 (0.0356)\tPrec@1 99.219 (98.855)\n",
            "Epoch: [126][385/391]\tLoss 0.0356 (0.0361)\tPrec@1 99.219 (98.846)\n",
            "Test\t  Prec@1: 91.530 (Err: 8.470 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [127][0/391]\tLoss 0.0266 (0.0266)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [127][55/391]\tLoss 0.0276 (0.0318)\tPrec@1 98.438 (99.009)\n",
            "Epoch: [127][110/391]\tLoss 0.0350 (0.0327)\tPrec@1 99.219 (98.994)\n",
            "Epoch: [127][165/391]\tLoss 0.0164 (0.0339)\tPrec@1 100.000 (98.960)\n",
            "Epoch: [127][220/391]\tLoss 0.0263 (0.0336)\tPrec@1 99.219 (98.950)\n",
            "Epoch: [127][275/391]\tLoss 0.0303 (0.0332)\tPrec@1 100.000 (98.947)\n",
            "Epoch: [127][330/391]\tLoss 0.0107 (0.0335)\tPrec@1 100.000 (98.936)\n",
            "Epoch: [127][385/391]\tLoss 0.0235 (0.0340)\tPrec@1 100.000 (98.919)\n",
            "Test\t  Prec@1: 91.410 (Err: 8.590 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [128][0/391]\tLoss 0.0643 (0.0643)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [128][55/391]\tLoss 0.0562 (0.0340)\tPrec@1 98.438 (99.009)\n",
            "Epoch: [128][110/391]\tLoss 0.0697 (0.0351)\tPrec@1 99.219 (98.916)\n",
            "Epoch: [128][165/391]\tLoss 0.0279 (0.0341)\tPrec@1 99.219 (98.946)\n",
            "Epoch: [128][220/391]\tLoss 0.0258 (0.0330)\tPrec@1 99.219 (98.996)\n",
            "Epoch: [128][275/391]\tLoss 0.0139 (0.0325)\tPrec@1 100.000 (99.001)\n",
            "Epoch: [128][330/391]\tLoss 0.0203 (0.0318)\tPrec@1 100.000 (99.028)\n",
            "Epoch: [128][385/391]\tLoss 0.0516 (0.0319)\tPrec@1 98.438 (99.012)\n",
            "Test\t  Prec@1: 91.800 (Err: 8.200 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [129][0/391]\tLoss 0.0446 (0.0446)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [129][55/391]\tLoss 0.0172 (0.0306)\tPrec@1 100.000 (99.051)\n",
            "Epoch: [129][110/391]\tLoss 0.0197 (0.0314)\tPrec@1 100.000 (99.008)\n",
            "Epoch: [129][165/391]\tLoss 0.0590 (0.0310)\tPrec@1 96.875 (99.007)\n",
            "Epoch: [129][220/391]\tLoss 0.0777 (0.0308)\tPrec@1 97.656 (99.028)\n",
            "Epoch: [129][275/391]\tLoss 0.0426 (0.0315)\tPrec@1 98.438 (98.989)\n",
            "Epoch: [129][330/391]\tLoss 0.0339 (0.0308)\tPrec@1 98.438 (99.023)\n",
            "Epoch: [129][385/391]\tLoss 0.0220 (0.0316)\tPrec@1 100.000 (98.988)\n",
            "Test\t  Prec@1: 91.680 (Err: 8.320 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [130][0/391]\tLoss 0.0245 (0.0245)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [130][55/391]\tLoss 0.0430 (0.0312)\tPrec@1 98.438 (99.107)\n",
            "Epoch: [130][110/391]\tLoss 0.0133 (0.0314)\tPrec@1 100.000 (99.050)\n",
            "Epoch: [130][165/391]\tLoss 0.0262 (0.0318)\tPrec@1 99.219 (99.030)\n",
            "Epoch: [130][220/391]\tLoss 0.0657 (0.0319)\tPrec@1 97.656 (99.003)\n",
            "Epoch: [130][275/391]\tLoss 0.0387 (0.0321)\tPrec@1 98.438 (99.018)\n",
            "Epoch: [130][330/391]\tLoss 0.0166 (0.0318)\tPrec@1 99.219 (99.016)\n",
            "Epoch: [130][385/391]\tLoss 0.0415 (0.0324)\tPrec@1 98.438 (98.976)\n",
            "Test\t  Prec@1: 91.760 (Err: 8.240 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [131][0/391]\tLoss 0.0153 (0.0153)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [131][55/391]\tLoss 0.0278 (0.0305)\tPrec@1 99.219 (99.023)\n",
            "Epoch: [131][110/391]\tLoss 0.0297 (0.0294)\tPrec@1 99.219 (99.127)\n",
            "Epoch: [131][165/391]\tLoss 0.0355 (0.0308)\tPrec@1 98.438 (99.045)\n",
            "Epoch: [131][220/391]\tLoss 0.0136 (0.0301)\tPrec@1 99.219 (99.077)\n",
            "Epoch: [131][275/391]\tLoss 0.0557 (0.0307)\tPrec@1 97.656 (99.043)\n",
            "Epoch: [131][330/391]\tLoss 0.0190 (0.0312)\tPrec@1 99.219 (99.009)\n",
            "Epoch: [131][385/391]\tLoss 0.0165 (0.0320)\tPrec@1 99.219 (98.966)\n",
            "Test\t  Prec@1: 91.440 (Err: 8.560 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [132][0/391]\tLoss 0.0232 (0.0232)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [132][55/391]\tLoss 0.0132 (0.0311)\tPrec@1 100.000 (98.982)\n",
            "Epoch: [132][110/391]\tLoss 0.0185 (0.0298)\tPrec@1 99.219 (99.071)\n",
            "Epoch: [132][165/391]\tLoss 0.0091 (0.0288)\tPrec@1 100.000 (99.115)\n",
            "Epoch: [132][220/391]\tLoss 0.0195 (0.0297)\tPrec@1 99.219 (99.095)\n",
            "Epoch: [132][275/391]\tLoss 0.0319 (0.0293)\tPrec@1 99.219 (99.100)\n",
            "Epoch: [132][330/391]\tLoss 0.0153 (0.0297)\tPrec@1 100.000 (99.087)\n",
            "Epoch: [132][385/391]\tLoss 0.0389 (0.0302)\tPrec@1 98.438 (99.035)\n",
            "Test\t  Prec@1: 91.830 (Err: 8.170 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [133][0/391]\tLoss 0.0629 (0.0629)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [133][55/391]\tLoss 0.0148 (0.0283)\tPrec@1 100.000 (99.093)\n",
            "Epoch: [133][110/391]\tLoss 0.0176 (0.0297)\tPrec@1 100.000 (99.050)\n",
            "Epoch: [133][165/391]\tLoss 0.0207 (0.0291)\tPrec@1 100.000 (99.092)\n",
            "Epoch: [133][220/391]\tLoss 0.0426 (0.0293)\tPrec@1 98.438 (99.095)\n",
            "Epoch: [133][275/391]\tLoss 0.0208 (0.0303)\tPrec@1 99.219 (99.066)\n",
            "Epoch: [133][330/391]\tLoss 0.0112 (0.0301)\tPrec@1 100.000 (99.082)\n",
            "Epoch: [133][385/391]\tLoss 0.0274 (0.0302)\tPrec@1 99.219 (99.069)\n",
            "Test\t  Prec@1: 91.600 (Err: 8.400 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [134][0/391]\tLoss 0.0619 (0.0619)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [134][55/391]\tLoss 0.0255 (0.0233)\tPrec@1 100.000 (99.330)\n",
            "Epoch: [134][110/391]\tLoss 0.0463 (0.0270)\tPrec@1 98.438 (99.191)\n",
            "Epoch: [134][165/391]\tLoss 0.0141 (0.0274)\tPrec@1 100.000 (99.158)\n",
            "Epoch: [134][220/391]\tLoss 0.0155 (0.0283)\tPrec@1 100.000 (99.120)\n",
            "Epoch: [134][275/391]\tLoss 0.0222 (0.0285)\tPrec@1 100.000 (99.108)\n",
            "Epoch: [134][330/391]\tLoss 0.0216 (0.0288)\tPrec@1 99.219 (99.098)\n",
            "Epoch: [134][385/391]\tLoss 0.0314 (0.0290)\tPrec@1 98.438 (99.101)\n",
            "Test\t  Prec@1: 91.740 (Err: 8.260 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [135][0/391]\tLoss 0.0419 (0.0419)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [135][55/391]\tLoss 0.0265 (0.0262)\tPrec@1 99.219 (99.135)\n",
            "Epoch: [135][110/391]\tLoss 0.0205 (0.0258)\tPrec@1 99.219 (99.247)\n",
            "Epoch: [135][165/391]\tLoss 0.0082 (0.0266)\tPrec@1 100.000 (99.200)\n",
            "Epoch: [135][220/391]\tLoss 0.0354 (0.0269)\tPrec@1 97.656 (99.212)\n",
            "Epoch: [135][275/391]\tLoss 0.0440 (0.0270)\tPrec@1 98.438 (99.199)\n",
            "Epoch: [135][330/391]\tLoss 0.0188 (0.0277)\tPrec@1 100.000 (99.186)\n",
            "Epoch: [135][385/391]\tLoss 0.0390 (0.0277)\tPrec@1 98.438 (99.168)\n",
            "Test\t  Prec@1: 91.550 (Err: 8.450 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [136][0/391]\tLoss 0.0105 (0.0105)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [136][55/391]\tLoss 0.0960 (0.0265)\tPrec@1 96.875 (99.149)\n",
            "Epoch: [136][110/391]\tLoss 0.0243 (0.0291)\tPrec@1 99.219 (99.099)\n",
            "Epoch: [136][165/391]\tLoss 0.0138 (0.0281)\tPrec@1 100.000 (99.176)\n",
            "Epoch: [136][220/391]\tLoss 0.0357 (0.0287)\tPrec@1 98.438 (99.162)\n",
            "Epoch: [136][275/391]\tLoss 0.0202 (0.0282)\tPrec@1 100.000 (99.173)\n",
            "Epoch: [136][330/391]\tLoss 0.0179 (0.0282)\tPrec@1 100.000 (99.193)\n",
            "Epoch: [136][385/391]\tLoss 0.0296 (0.0286)\tPrec@1 99.219 (99.184)\n",
            "Test\t  Prec@1: 91.450 (Err: 8.550 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [137][0/391]\tLoss 0.0476 (0.0476)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [137][55/391]\tLoss 0.0489 (0.0291)\tPrec@1 98.438 (99.009)\n",
            "Epoch: [137][110/391]\tLoss 0.0225 (0.0283)\tPrec@1 100.000 (99.092)\n",
            "Epoch: [137][165/391]\tLoss 0.0247 (0.0292)\tPrec@1 99.219 (99.016)\n",
            "Epoch: [137][220/391]\tLoss 0.0252 (0.0297)\tPrec@1 100.000 (99.035)\n",
            "Epoch: [137][275/391]\tLoss 0.0237 (0.0287)\tPrec@1 99.219 (99.074)\n",
            "Epoch: [137][330/391]\tLoss 0.0191 (0.0288)\tPrec@1 100.000 (99.070)\n",
            "Epoch: [137][385/391]\tLoss 0.0356 (0.0290)\tPrec@1 98.438 (99.069)\n",
            "Test\t  Prec@1: 91.440 (Err: 8.560 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [138][0/391]\tLoss 0.0336 (0.0336)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [138][55/391]\tLoss 0.0386 (0.0249)\tPrec@1 98.438 (99.205)\n",
            "Epoch: [138][110/391]\tLoss 0.0158 (0.0260)\tPrec@1 100.000 (99.240)\n",
            "Epoch: [138][165/391]\tLoss 0.0239 (0.0278)\tPrec@1 98.438 (99.181)\n",
            "Epoch: [138][220/391]\tLoss 0.0715 (0.0280)\tPrec@1 96.875 (99.155)\n",
            "Epoch: [138][275/391]\tLoss 0.0135 (0.0275)\tPrec@1 100.000 (99.159)\n",
            "Epoch: [138][330/391]\tLoss 0.0208 (0.0277)\tPrec@1 99.219 (99.143)\n",
            "Epoch: [138][385/391]\tLoss 0.0585 (0.0276)\tPrec@1 98.438 (99.156)\n",
            "Test\t  Prec@1: 91.840 (Err: 8.160 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [139][0/391]\tLoss 0.0192 (0.0192)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [139][55/391]\tLoss 0.0099 (0.0258)\tPrec@1 100.000 (99.261)\n",
            "Epoch: [139][110/391]\tLoss 0.0125 (0.0252)\tPrec@1 100.000 (99.303)\n",
            "Epoch: [139][165/391]\tLoss 0.0245 (0.0256)\tPrec@1 98.438 (99.261)\n",
            "Epoch: [139][220/391]\tLoss 0.0314 (0.0261)\tPrec@1 97.656 (99.233)\n",
            "Epoch: [139][275/391]\tLoss 0.0019 (0.0263)\tPrec@1 100.000 (99.219)\n",
            "Epoch: [139][330/391]\tLoss 0.0215 (0.0260)\tPrec@1 98.438 (99.231)\n",
            "Epoch: [139][385/391]\tLoss 0.0305 (0.0264)\tPrec@1 99.219 (99.207)\n",
            "Test\t  Prec@1: 91.760 (Err: 8.240 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [140][0/391]\tLoss 0.0428 (0.0428)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [140][55/391]\tLoss 0.0269 (0.0241)\tPrec@1 98.438 (99.261)\n",
            "Epoch: [140][110/391]\tLoss 0.0167 (0.0256)\tPrec@1 100.000 (99.134)\n",
            "Epoch: [140][165/391]\tLoss 0.0373 (0.0253)\tPrec@1 98.438 (99.148)\n",
            "Epoch: [140][220/391]\tLoss 0.0142 (0.0262)\tPrec@1 100.000 (99.127)\n",
            "Epoch: [140][275/391]\tLoss 0.0450 (0.0273)\tPrec@1 99.219 (99.094)\n",
            "Epoch: [140][330/391]\tLoss 0.0556 (0.0272)\tPrec@1 97.656 (99.101)\n",
            "Epoch: [140][385/391]\tLoss 0.0160 (0.0280)\tPrec@1 99.219 (99.061)\n",
            "Test\t  Prec@1: 91.740 (Err: 8.260 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [141][0/391]\tLoss 0.0279 (0.0279)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [141][55/391]\tLoss 0.0190 (0.0205)\tPrec@1 99.219 (99.372)\n",
            "Epoch: [141][110/391]\tLoss 0.0142 (0.0226)\tPrec@1 100.000 (99.303)\n",
            "Epoch: [141][165/391]\tLoss 0.0290 (0.0247)\tPrec@1 100.000 (99.266)\n",
            "Epoch: [141][220/391]\tLoss 0.0318 (0.0258)\tPrec@1 99.219 (99.226)\n",
            "Epoch: [141][275/391]\tLoss 0.0349 (0.0265)\tPrec@1 97.656 (99.179)\n",
            "Epoch: [141][330/391]\tLoss 0.0093 (0.0267)\tPrec@1 100.000 (99.169)\n",
            "Epoch: [141][385/391]\tLoss 0.0171 (0.0268)\tPrec@1 100.000 (99.146)\n",
            "Test\t  Prec@1: 91.510 (Err: 8.490 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [142][0/391]\tLoss 0.0152 (0.0152)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [142][55/391]\tLoss 0.0219 (0.0243)\tPrec@1 99.219 (99.289)\n",
            "Epoch: [142][110/391]\tLoss 0.0074 (0.0240)\tPrec@1 100.000 (99.317)\n",
            "Epoch: [142][165/391]\tLoss 0.0253 (0.0246)\tPrec@1 100.000 (99.289)\n",
            "Epoch: [142][220/391]\tLoss 0.0320 (0.0239)\tPrec@1 99.219 (99.289)\n",
            "Epoch: [142][275/391]\tLoss 0.0478 (0.0249)\tPrec@1 98.438 (99.236)\n",
            "Epoch: [142][330/391]\tLoss 0.0493 (0.0257)\tPrec@1 99.219 (99.214)\n",
            "Epoch: [142][385/391]\tLoss 0.0348 (0.0264)\tPrec@1 100.000 (99.170)\n",
            "Test\t  Prec@1: 91.480 (Err: 8.520 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [143][0/391]\tLoss 0.0383 (0.0383)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [143][55/391]\tLoss 0.0288 (0.0221)\tPrec@1 98.438 (99.344)\n",
            "Epoch: [143][110/391]\tLoss 0.0136 (0.0219)\tPrec@1 100.000 (99.381)\n",
            "Epoch: [143][165/391]\tLoss 0.0352 (0.0224)\tPrec@1 98.438 (99.322)\n",
            "Epoch: [143][220/391]\tLoss 0.0397 (0.0236)\tPrec@1 99.219 (99.261)\n",
            "Epoch: [143][275/391]\tLoss 0.0269 (0.0238)\tPrec@1 99.219 (99.261)\n",
            "Epoch: [143][330/391]\tLoss 0.0229 (0.0239)\tPrec@1 99.219 (99.249)\n",
            "Epoch: [143][385/391]\tLoss 0.0077 (0.0242)\tPrec@1 100.000 (99.243)\n",
            "Test\t  Prec@1: 91.410 (Err: 8.590 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [144][0/391]\tLoss 0.0196 (0.0196)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [144][55/391]\tLoss 0.0241 (0.0266)\tPrec@1 99.219 (99.191)\n",
            "Epoch: [144][110/391]\tLoss 0.0096 (0.0265)\tPrec@1 100.000 (99.191)\n",
            "Epoch: [144][165/391]\tLoss 0.0421 (0.0254)\tPrec@1 98.438 (99.252)\n",
            "Epoch: [144][220/391]\tLoss 0.0124 (0.0253)\tPrec@1 100.000 (99.240)\n",
            "Epoch: [144][275/391]\tLoss 0.0111 (0.0257)\tPrec@1 100.000 (99.207)\n",
            "Epoch: [144][330/391]\tLoss 0.0137 (0.0257)\tPrec@1 100.000 (99.193)\n",
            "Epoch: [144][385/391]\tLoss 0.0159 (0.0255)\tPrec@1 100.000 (99.205)\n",
            "Test\t  Prec@1: 91.560 (Err: 8.440 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [145][0/391]\tLoss 0.0466 (0.0466)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [145][55/391]\tLoss 0.0238 (0.0216)\tPrec@1 98.438 (99.372)\n",
            "Epoch: [145][110/391]\tLoss 0.0444 (0.0226)\tPrec@1 98.438 (99.296)\n",
            "Epoch: [145][165/391]\tLoss 0.0630 (0.0241)\tPrec@1 97.656 (99.242)\n",
            "Epoch: [145][220/391]\tLoss 0.0188 (0.0248)\tPrec@1 100.000 (99.215)\n",
            "Epoch: [145][275/391]\tLoss 0.0136 (0.0253)\tPrec@1 100.000 (99.190)\n",
            "Epoch: [145][330/391]\tLoss 0.0512 (0.0257)\tPrec@1 97.656 (99.183)\n",
            "Epoch: [145][385/391]\tLoss 0.0282 (0.0260)\tPrec@1 99.219 (99.184)\n",
            "Test\t  Prec@1: 91.580 (Err: 8.420 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [146][0/391]\tLoss 0.0270 (0.0270)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [146][55/391]\tLoss 0.0338 (0.0233)\tPrec@1 99.219 (99.302)\n",
            "Epoch: [146][110/391]\tLoss 0.0159 (0.0242)\tPrec@1 99.219 (99.282)\n",
            "Epoch: [146][165/391]\tLoss 0.0281 (0.0246)\tPrec@1 99.219 (99.252)\n",
            "Epoch: [146][220/391]\tLoss 0.0212 (0.0255)\tPrec@1 99.219 (99.215)\n",
            "Epoch: [146][275/391]\tLoss 0.0281 (0.0255)\tPrec@1 98.438 (99.193)\n",
            "Epoch: [146][330/391]\tLoss 0.0530 (0.0256)\tPrec@1 98.438 (99.198)\n",
            "Epoch: [146][385/391]\tLoss 0.0068 (0.0260)\tPrec@1 100.000 (99.182)\n",
            "Test\t  Prec@1: 91.490 (Err: 8.510 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [147][0/391]\tLoss 0.0083 (0.0083)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [147][55/391]\tLoss 0.0076 (0.0252)\tPrec@1 100.000 (99.205)\n",
            "Epoch: [147][110/391]\tLoss 0.0038 (0.0254)\tPrec@1 100.000 (99.162)\n",
            "Epoch: [147][165/391]\tLoss 0.0237 (0.0252)\tPrec@1 99.219 (99.200)\n",
            "Epoch: [147][220/391]\tLoss 0.0256 (0.0258)\tPrec@1 99.219 (99.212)\n",
            "Epoch: [147][275/391]\tLoss 0.0130 (0.0256)\tPrec@1 100.000 (99.193)\n",
            "Epoch: [147][330/391]\tLoss 0.0343 (0.0253)\tPrec@1 98.438 (99.207)\n",
            "Epoch: [147][385/391]\tLoss 0.0134 (0.0251)\tPrec@1 100.000 (99.215)\n",
            "Test\t  Prec@1: 91.480 (Err: 8.520 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [148][0/391]\tLoss 0.0108 (0.0108)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [148][55/391]\tLoss 0.0150 (0.0247)\tPrec@1 100.000 (99.275)\n",
            "Epoch: [148][110/391]\tLoss 0.0089 (0.0244)\tPrec@1 100.000 (99.289)\n",
            "Epoch: [148][165/391]\tLoss 0.0242 (0.0245)\tPrec@1 99.219 (99.271)\n",
            "Epoch: [148][220/391]\tLoss 0.0175 (0.0248)\tPrec@1 98.438 (99.268)\n",
            "Epoch: [148][275/391]\tLoss 0.0101 (0.0238)\tPrec@1 100.000 (99.304)\n",
            "Epoch: [148][330/391]\tLoss 0.0356 (0.0245)\tPrec@1 98.438 (99.268)\n",
            "Epoch: [148][385/391]\tLoss 0.0171 (0.0244)\tPrec@1 99.219 (99.267)\n",
            "Test\t  Prec@1: 92.020 (Err: 7.980 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [149][0/391]\tLoss 0.0177 (0.0177)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [149][55/391]\tLoss 0.0480 (0.0231)\tPrec@1 97.656 (99.302)\n",
            "Epoch: [149][110/391]\tLoss 0.0225 (0.0220)\tPrec@1 98.438 (99.317)\n",
            "Epoch: [149][165/391]\tLoss 0.0257 (0.0235)\tPrec@1 99.219 (99.233)\n",
            "Epoch: [149][220/391]\tLoss 0.0040 (0.0241)\tPrec@1 100.000 (99.219)\n",
            "Epoch: [149][275/391]\tLoss 0.0226 (0.0245)\tPrec@1 99.219 (99.213)\n",
            "Epoch: [149][330/391]\tLoss 0.0196 (0.0241)\tPrec@1 99.219 (99.233)\n",
            "Epoch: [149][385/391]\tLoss 0.0132 (0.0240)\tPrec@1 100.000 (99.243)\n",
            "Test\t  Prec@1: 91.930 (Err: 8.070 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [150][0/391]\tLoss 0.0276 (0.0276)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [150][55/391]\tLoss 0.0363 (0.0273)\tPrec@1 98.438 (99.023)\n",
            "Epoch: [150][110/391]\tLoss 0.0572 (0.0246)\tPrec@1 98.438 (99.198)\n",
            "Epoch: [150][165/391]\tLoss 0.0121 (0.0239)\tPrec@1 100.000 (99.247)\n",
            "Epoch: [150][220/391]\tLoss 0.0157 (0.0229)\tPrec@1 100.000 (99.289)\n",
            "Epoch: [150][275/391]\tLoss 0.0342 (0.0224)\tPrec@1 98.438 (99.298)\n",
            "Epoch: [150][330/391]\tLoss 0.0113 (0.0223)\tPrec@1 100.000 (99.323)\n",
            "Epoch: [150][385/391]\tLoss 0.0080 (0.0219)\tPrec@1 100.000 (99.336)\n",
            "Test\t  Prec@1: 91.870 (Err: 8.130 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [151][0/391]\tLoss 0.0771 (0.0771)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [151][55/391]\tLoss 0.0158 (0.0218)\tPrec@1 99.219 (99.330)\n",
            "Epoch: [151][110/391]\tLoss 0.0491 (0.0197)\tPrec@1 98.438 (99.430)\n",
            "Epoch: [151][165/391]\tLoss 0.0388 (0.0206)\tPrec@1 99.219 (99.421)\n",
            "Epoch: [151][220/391]\tLoss 0.0275 (0.0206)\tPrec@1 99.219 (99.403)\n",
            "Epoch: [151][275/391]\tLoss 0.0162 (0.0203)\tPrec@1 99.219 (99.414)\n",
            "Epoch: [151][330/391]\tLoss 0.0279 (0.0199)\tPrec@1 99.219 (99.429)\n",
            "Epoch: [151][385/391]\tLoss 0.0028 (0.0199)\tPrec@1 100.000 (99.417)\n",
            "Test\t  Prec@1: 92.070 (Err: 7.930 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [152][0/391]\tLoss 0.0345 (0.0345)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [152][55/391]\tLoss 0.0196 (0.0172)\tPrec@1 99.219 (99.554)\n",
            "Epoch: [152][110/391]\tLoss 0.0102 (0.0187)\tPrec@1 99.219 (99.472)\n",
            "Epoch: [152][165/391]\tLoss 0.0189 (0.0179)\tPrec@1 98.438 (99.511)\n",
            "Epoch: [152][220/391]\tLoss 0.0352 (0.0182)\tPrec@1 99.219 (99.505)\n",
            "Epoch: [152][275/391]\tLoss 0.0186 (0.0184)\tPrec@1 99.219 (99.490)\n",
            "Epoch: [152][330/391]\tLoss 0.0037 (0.0187)\tPrec@1 100.000 (99.469)\n",
            "Epoch: [152][385/391]\tLoss 0.0288 (0.0183)\tPrec@1 99.219 (99.482)\n",
            "Test\t  Prec@1: 92.070 (Err: 7.930 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [153][0/391]\tLoss 0.0202 (0.0202)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [153][55/391]\tLoss 0.0112 (0.0182)\tPrec@1 100.000 (99.484)\n",
            "Epoch: [153][110/391]\tLoss 0.0286 (0.0166)\tPrec@1 99.219 (99.564)\n",
            "Epoch: [153][165/391]\tLoss 0.0105 (0.0171)\tPrec@1 100.000 (99.553)\n",
            "Epoch: [153][220/391]\tLoss 0.0070 (0.0173)\tPrec@1 100.000 (99.544)\n",
            "Epoch: [153][275/391]\tLoss 0.0162 (0.0176)\tPrec@1 99.219 (99.533)\n",
            "Epoch: [153][330/391]\tLoss 0.0246 (0.0175)\tPrec@1 98.438 (99.521)\n",
            "Epoch: [153][385/391]\tLoss 0.0223 (0.0173)\tPrec@1 99.219 (99.532)\n",
            "Test\t  Prec@1: 92.140 (Err: 7.860 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [154][0/391]\tLoss 0.0065 (0.0065)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [154][55/391]\tLoss 0.0180 (0.0182)\tPrec@1 99.219 (99.470)\n",
            "Epoch: [154][110/391]\tLoss 0.0195 (0.0177)\tPrec@1 99.219 (99.500)\n",
            "Epoch: [154][165/391]\tLoss 0.0157 (0.0183)\tPrec@1 100.000 (99.478)\n",
            "Epoch: [154][220/391]\tLoss 0.0051 (0.0181)\tPrec@1 100.000 (99.477)\n",
            "Epoch: [154][275/391]\tLoss 0.0126 (0.0184)\tPrec@1 100.000 (99.482)\n",
            "Epoch: [154][330/391]\tLoss 0.0280 (0.0181)\tPrec@1 99.219 (99.500)\n",
            "Epoch: [154][385/391]\tLoss 0.0110 (0.0179)\tPrec@1 99.219 (99.506)\n",
            "Test\t  Prec@1: 92.120 (Err: 7.880 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [155][0/391]\tLoss 0.0027 (0.0027)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [155][55/391]\tLoss 0.0072 (0.0174)\tPrec@1 100.000 (99.470)\n",
            "Epoch: [155][110/391]\tLoss 0.0139 (0.0160)\tPrec@1 100.000 (99.571)\n",
            "Epoch: [155][165/391]\tLoss 0.0125 (0.0166)\tPrec@1 100.000 (99.534)\n",
            "Epoch: [155][220/391]\tLoss 0.0300 (0.0178)\tPrec@1 99.219 (99.502)\n",
            "Epoch: [155][275/391]\tLoss 0.0080 (0.0177)\tPrec@1 100.000 (99.499)\n",
            "Epoch: [155][330/391]\tLoss 0.0037 (0.0176)\tPrec@1 100.000 (99.509)\n",
            "Epoch: [155][385/391]\tLoss 0.0066 (0.0177)\tPrec@1 100.000 (99.510)\n",
            "Test\t  Prec@1: 92.060 (Err: 7.940 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [156][0/391]\tLoss 0.0507 (0.0507)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [156][55/391]\tLoss 0.0111 (0.0187)\tPrec@1 100.000 (99.554)\n",
            "Epoch: [156][110/391]\tLoss 0.0150 (0.0182)\tPrec@1 100.000 (99.571)\n",
            "Epoch: [156][165/391]\tLoss 0.0075 (0.0188)\tPrec@1 100.000 (99.506)\n",
            "Epoch: [156][220/391]\tLoss 0.0210 (0.0182)\tPrec@1 99.219 (99.512)\n",
            "Epoch: [156][275/391]\tLoss 0.0210 (0.0183)\tPrec@1 99.219 (99.507)\n",
            "Epoch: [156][330/391]\tLoss 0.0248 (0.0182)\tPrec@1 99.219 (99.523)\n",
            "Epoch: [156][385/391]\tLoss 0.0097 (0.0180)\tPrec@1 100.000 (99.528)\n",
            "Test\t  Prec@1: 91.950 (Err: 8.050 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [157][0/391]\tLoss 0.0186 (0.0186)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [157][55/391]\tLoss 0.0306 (0.0157)\tPrec@1 98.438 (99.554)\n",
            "Epoch: [157][110/391]\tLoss 0.0065 (0.0154)\tPrec@1 100.000 (99.585)\n",
            "Epoch: [157][165/391]\tLoss 0.0128 (0.0153)\tPrec@1 100.000 (99.600)\n",
            "Epoch: [157][220/391]\tLoss 0.0109 (0.0151)\tPrec@1 100.000 (99.615)\n",
            "Epoch: [157][275/391]\tLoss 0.0136 (0.0155)\tPrec@1 99.219 (99.590)\n",
            "Epoch: [157][330/391]\tLoss 0.0068 (0.0156)\tPrec@1 100.000 (99.599)\n",
            "Epoch: [157][385/391]\tLoss 0.0121 (0.0157)\tPrec@1 100.000 (99.587)\n",
            "Test\t  Prec@1: 92.000 (Err: 8.000 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [158][0/391]\tLoss 0.0214 (0.0214)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [158][55/391]\tLoss 0.0091 (0.0137)\tPrec@1 100.000 (99.651)\n",
            "Epoch: [158][110/391]\tLoss 0.0425 (0.0156)\tPrec@1 98.438 (99.592)\n",
            "Epoch: [158][165/391]\tLoss 0.0143 (0.0160)\tPrec@1 100.000 (99.576)\n",
            "Epoch: [158][220/391]\tLoss 0.0071 (0.0160)\tPrec@1 100.000 (99.572)\n",
            "Epoch: [158][275/391]\tLoss 0.0060 (0.0168)\tPrec@1 100.000 (99.530)\n",
            "Epoch: [158][330/391]\tLoss 0.0113 (0.0167)\tPrec@1 100.000 (99.542)\n",
            "Epoch: [158][385/391]\tLoss 0.0095 (0.0170)\tPrec@1 100.000 (99.518)\n",
            "Test\t  Prec@1: 92.020 (Err: 7.980 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [159][0/391]\tLoss 0.0238 (0.0238)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [159][55/391]\tLoss 0.0116 (0.0156)\tPrec@1 100.000 (99.595)\n",
            "Epoch: [159][110/391]\tLoss 0.0228 (0.0154)\tPrec@1 99.219 (99.613)\n",
            "Epoch: [159][165/391]\tLoss 0.0121 (0.0152)\tPrec@1 100.000 (99.595)\n",
            "Epoch: [159][220/391]\tLoss 0.0031 (0.0157)\tPrec@1 100.000 (99.586)\n",
            "Epoch: [159][275/391]\tLoss 0.0161 (0.0160)\tPrec@1 99.219 (99.581)\n",
            "Epoch: [159][330/391]\tLoss 0.0191 (0.0161)\tPrec@1 99.219 (99.575)\n",
            "Epoch: [159][385/391]\tLoss 0.0208 (0.0161)\tPrec@1 99.219 (99.575)\n",
            "Test\t  Prec@1: 92.090 (Err: 7.910 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [160][0/391]\tLoss 0.0204 (0.0204)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [160][55/391]\tLoss 0.0292 (0.0169)\tPrec@1 97.656 (99.526)\n",
            "Epoch: [160][110/391]\tLoss 0.0069 (0.0160)\tPrec@1 100.000 (99.592)\n",
            "Epoch: [160][165/391]\tLoss 0.0052 (0.0156)\tPrec@1 100.000 (99.609)\n",
            "Epoch: [160][220/391]\tLoss 0.0313 (0.0155)\tPrec@1 98.438 (99.615)\n",
            "Epoch: [160][275/391]\tLoss 0.0086 (0.0156)\tPrec@1 100.000 (99.615)\n",
            "Epoch: [160][330/391]\tLoss 0.0160 (0.0154)\tPrec@1 100.000 (99.625)\n",
            "Epoch: [160][385/391]\tLoss 0.0147 (0.0156)\tPrec@1 100.000 (99.615)\n",
            "Test\t  Prec@1: 92.020 (Err: 7.980 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [161][0/391]\tLoss 0.0430 (0.0430)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [161][55/391]\tLoss 0.0272 (0.0155)\tPrec@1 99.219 (99.707)\n",
            "Epoch: [161][110/391]\tLoss 0.0069 (0.0147)\tPrec@1 100.000 (99.711)\n",
            "Epoch: [161][165/391]\tLoss 0.0219 (0.0160)\tPrec@1 98.438 (99.600)\n",
            "Epoch: [161][220/391]\tLoss 0.0118 (0.0164)\tPrec@1 99.219 (99.565)\n",
            "Epoch: [161][275/391]\tLoss 0.0075 (0.0161)\tPrec@1 100.000 (99.581)\n",
            "Epoch: [161][330/391]\tLoss 0.0208 (0.0159)\tPrec@1 99.219 (99.589)\n",
            "Epoch: [161][385/391]\tLoss 0.0176 (0.0163)\tPrec@1 100.000 (99.553)\n",
            "Test\t  Prec@1: 91.980 (Err: 8.020 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [162][0/391]\tLoss 0.0143 (0.0143)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [162][55/391]\tLoss 0.0149 (0.0149)\tPrec@1 100.000 (99.623)\n",
            "Epoch: [162][110/391]\tLoss 0.0246 (0.0152)\tPrec@1 99.219 (99.634)\n",
            "Epoch: [162][165/391]\tLoss 0.0072 (0.0161)\tPrec@1 100.000 (99.595)\n",
            "Epoch: [162][220/391]\tLoss 0.0042 (0.0159)\tPrec@1 100.000 (99.601)\n",
            "Epoch: [162][275/391]\tLoss 0.0083 (0.0159)\tPrec@1 100.000 (99.590)\n",
            "Epoch: [162][330/391]\tLoss 0.0088 (0.0157)\tPrec@1 100.000 (99.596)\n",
            "Epoch: [162][385/391]\tLoss 0.0419 (0.0158)\tPrec@1 99.219 (99.593)\n",
            "Test\t  Prec@1: 91.960 (Err: 8.040 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [163][0/391]\tLoss 0.0063 (0.0063)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [163][55/391]\tLoss 0.0156 (0.0148)\tPrec@1 100.000 (99.595)\n",
            "Epoch: [163][110/391]\tLoss 0.0065 (0.0153)\tPrec@1 100.000 (99.613)\n",
            "Epoch: [163][165/391]\tLoss 0.0243 (0.0151)\tPrec@1 98.438 (99.633)\n",
            "Epoch: [163][220/391]\tLoss 0.0047 (0.0159)\tPrec@1 100.000 (99.569)\n",
            "Epoch: [163][275/391]\tLoss 0.0082 (0.0159)\tPrec@1 100.000 (99.567)\n",
            "Epoch: [163][330/391]\tLoss 0.0124 (0.0161)\tPrec@1 100.000 (99.563)\n",
            "Epoch: [163][385/391]\tLoss 0.0289 (0.0159)\tPrec@1 99.219 (99.577)\n",
            "Test\t  Prec@1: 91.950 (Err: 8.050 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [164][0/391]\tLoss 0.0285 (0.0285)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [164][55/391]\tLoss 0.0084 (0.0165)\tPrec@1 100.000 (99.581)\n",
            "Epoch: [164][110/391]\tLoss 0.0073 (0.0167)\tPrec@1 100.000 (99.557)\n",
            "Epoch: [164][165/391]\tLoss 0.0136 (0.0165)\tPrec@1 100.000 (99.558)\n",
            "Epoch: [164][220/391]\tLoss 0.0056 (0.0159)\tPrec@1 100.000 (99.597)\n",
            "Epoch: [164][275/391]\tLoss 0.0073 (0.0154)\tPrec@1 100.000 (99.621)\n",
            "Epoch: [164][330/391]\tLoss 0.0148 (0.0156)\tPrec@1 100.000 (99.601)\n",
            "Epoch: [164][385/391]\tLoss 0.0150 (0.0157)\tPrec@1 99.219 (99.593)\n",
            "Test\t  Prec@1: 92.020 (Err: 7.980 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [165][0/391]\tLoss 0.0151 (0.0151)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [165][55/391]\tLoss 0.0056 (0.0143)\tPrec@1 100.000 (99.568)\n",
            "Epoch: [165][110/391]\tLoss 0.0045 (0.0153)\tPrec@1 100.000 (99.528)\n",
            "Epoch: [165][165/391]\tLoss 0.0237 (0.0162)\tPrec@1 99.219 (99.529)\n",
            "Epoch: [165][220/391]\tLoss 0.0069 (0.0157)\tPrec@1 100.000 (99.565)\n",
            "Epoch: [165][275/391]\tLoss 0.0108 (0.0154)\tPrec@1 100.000 (99.587)\n",
            "Epoch: [165][330/391]\tLoss 0.0153 (0.0158)\tPrec@1 99.219 (99.570)\n",
            "Epoch: [165][385/391]\tLoss 0.0156 (0.0160)\tPrec@1 99.219 (99.567)\n",
            "Test\t  Prec@1: 92.020 (Err: 7.980 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [166][0/391]\tLoss 0.0221 (0.0221)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [166][55/391]\tLoss 0.0167 (0.0152)\tPrec@1 100.000 (99.637)\n",
            "Epoch: [166][110/391]\tLoss 0.0213 (0.0148)\tPrec@1 99.219 (99.634)\n",
            "Epoch: [166][165/391]\tLoss 0.0207 (0.0152)\tPrec@1 99.219 (99.619)\n",
            "Epoch: [166][220/391]\tLoss 0.0217 (0.0152)\tPrec@1 99.219 (99.618)\n",
            "Epoch: [166][275/391]\tLoss 0.0116 (0.0148)\tPrec@1 100.000 (99.635)\n",
            "Epoch: [166][330/391]\tLoss 0.0126 (0.0149)\tPrec@1 99.219 (99.639)\n",
            "Epoch: [166][385/391]\tLoss 0.0400 (0.0149)\tPrec@1 99.219 (99.638)\n",
            "Test\t  Prec@1: 92.030 (Err: 7.970 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [167][0/391]\tLoss 0.0289 (0.0289)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [167][55/391]\tLoss 0.0196 (0.0166)\tPrec@1 99.219 (99.498)\n",
            "Epoch: [167][110/391]\tLoss 0.0049 (0.0167)\tPrec@1 100.000 (99.535)\n",
            "Epoch: [167][165/391]\tLoss 0.0101 (0.0162)\tPrec@1 100.000 (99.572)\n",
            "Epoch: [167][220/391]\tLoss 0.0071 (0.0156)\tPrec@1 100.000 (99.593)\n",
            "Epoch: [167][275/391]\tLoss 0.0081 (0.0157)\tPrec@1 100.000 (99.598)\n",
            "Epoch: [167][330/391]\tLoss 0.0355 (0.0155)\tPrec@1 99.219 (99.613)\n",
            "Epoch: [167][385/391]\tLoss 0.0076 (0.0154)\tPrec@1 100.000 (99.607)\n",
            "Test\t  Prec@1: 92.090 (Err: 7.910 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [168][0/391]\tLoss 0.0063 (0.0063)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [168][55/391]\tLoss 0.0444 (0.0162)\tPrec@1 98.438 (99.568)\n",
            "Epoch: [168][110/391]\tLoss 0.0059 (0.0170)\tPrec@1 100.000 (99.514)\n",
            "Epoch: [168][165/391]\tLoss 0.0079 (0.0163)\tPrec@1 100.000 (99.572)\n",
            "Epoch: [168][220/391]\tLoss 0.0170 (0.0160)\tPrec@1 100.000 (99.590)\n",
            "Epoch: [168][275/391]\tLoss 0.0117 (0.0162)\tPrec@1 100.000 (99.578)\n",
            "Epoch: [168][330/391]\tLoss 0.0111 (0.0163)\tPrec@1 100.000 (99.578)\n",
            "Epoch: [168][385/391]\tLoss 0.0142 (0.0164)\tPrec@1 99.219 (99.559)\n",
            "Test\t  Prec@1: 91.970 (Err: 8.030 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [169][0/391]\tLoss 0.0134 (0.0134)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [169][55/391]\tLoss 0.0136 (0.0137)\tPrec@1 99.219 (99.679)\n",
            "Epoch: [169][110/391]\tLoss 0.0111 (0.0143)\tPrec@1 100.000 (99.655)\n",
            "Epoch: [169][165/391]\tLoss 0.0182 (0.0149)\tPrec@1 99.219 (99.628)\n",
            "Epoch: [169][220/391]\tLoss 0.0232 (0.0150)\tPrec@1 99.219 (99.618)\n",
            "Epoch: [169][275/391]\tLoss 0.0030 (0.0156)\tPrec@1 100.000 (99.590)\n",
            "Epoch: [169][330/391]\tLoss 0.0457 (0.0159)\tPrec@1 98.438 (99.587)\n",
            "Epoch: [169][385/391]\tLoss 0.0041 (0.0158)\tPrec@1 100.000 (99.579)\n",
            "Test\t  Prec@1: 91.960 (Err: 8.040 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [170][0/391]\tLoss 0.0055 (0.0055)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [170][55/391]\tLoss 0.0210 (0.0151)\tPrec@1 99.219 (99.526)\n",
            "Epoch: [170][110/391]\tLoss 0.0236 (0.0148)\tPrec@1 99.219 (99.592)\n",
            "Epoch: [170][165/391]\tLoss 0.0026 (0.0145)\tPrec@1 100.000 (99.614)\n",
            "Epoch: [170][220/391]\tLoss 0.0154 (0.0153)\tPrec@1 99.219 (99.590)\n",
            "Epoch: [170][275/391]\tLoss 0.0071 (0.0152)\tPrec@1 100.000 (99.590)\n",
            "Epoch: [170][330/391]\tLoss 0.0102 (0.0149)\tPrec@1 99.219 (99.603)\n",
            "Epoch: [170][385/391]\tLoss 0.0417 (0.0151)\tPrec@1 98.438 (99.593)\n",
            "Test\t  Prec@1: 92.040 (Err: 7.960 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [171][0/391]\tLoss 0.0244 (0.0244)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [171][55/391]\tLoss 0.0144 (0.0192)\tPrec@1 99.219 (99.358)\n",
            "Epoch: [171][110/391]\tLoss 0.0127 (0.0162)\tPrec@1 99.219 (99.521)\n",
            "Epoch: [171][165/391]\tLoss 0.0042 (0.0159)\tPrec@1 100.000 (99.543)\n",
            "Epoch: [171][220/391]\tLoss 0.0098 (0.0153)\tPrec@1 100.000 (99.583)\n",
            "Epoch: [171][275/391]\tLoss 0.0117 (0.0155)\tPrec@1 100.000 (99.598)\n",
            "Epoch: [171][330/391]\tLoss 0.0063 (0.0153)\tPrec@1 100.000 (99.599)\n",
            "Epoch: [171][385/391]\tLoss 0.0131 (0.0152)\tPrec@1 99.219 (99.593)\n",
            "Test\t  Prec@1: 91.970 (Err: 8.030 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [172][0/391]\tLoss 0.0093 (0.0093)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [172][55/391]\tLoss 0.0460 (0.0182)\tPrec@1 97.656 (99.512)\n",
            "Epoch: [172][110/391]\tLoss 0.0085 (0.0163)\tPrec@1 100.000 (99.578)\n",
            "Epoch: [172][165/391]\tLoss 0.0111 (0.0150)\tPrec@1 100.000 (99.642)\n",
            "Epoch: [172][220/391]\tLoss 0.0688 (0.0156)\tPrec@1 96.875 (99.604)\n",
            "Epoch: [172][275/391]\tLoss 0.0228 (0.0156)\tPrec@1 99.219 (99.609)\n",
            "Epoch: [172][330/391]\tLoss 0.0149 (0.0156)\tPrec@1 99.219 (99.592)\n",
            "Epoch: [172][385/391]\tLoss 0.0195 (0.0155)\tPrec@1 99.219 (99.605)\n",
            "Test\t  Prec@1: 91.990 (Err: 8.010 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [173][0/391]\tLoss 0.0133 (0.0133)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [173][55/391]\tLoss 0.0268 (0.0167)\tPrec@1 99.219 (99.498)\n",
            "Epoch: [173][110/391]\tLoss 0.0462 (0.0166)\tPrec@1 98.438 (99.543)\n",
            "Epoch: [173][165/391]\tLoss 0.0152 (0.0160)\tPrec@1 100.000 (99.567)\n",
            "Epoch: [173][220/391]\tLoss 0.0092 (0.0157)\tPrec@1 100.000 (99.590)\n",
            "Epoch: [173][275/391]\tLoss 0.0250 (0.0157)\tPrec@1 99.219 (99.578)\n",
            "Epoch: [173][330/391]\tLoss 0.0223 (0.0160)\tPrec@1 99.219 (99.570)\n",
            "Epoch: [173][385/391]\tLoss 0.0213 (0.0160)\tPrec@1 99.219 (99.577)\n",
            "Test\t  Prec@1: 91.940 (Err: 8.060 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [174][0/391]\tLoss 0.0121 (0.0121)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [174][55/391]\tLoss 0.0130 (0.0131)\tPrec@1 99.219 (99.707)\n",
            "Epoch: [174][110/391]\tLoss 0.0144 (0.0146)\tPrec@1 99.219 (99.620)\n",
            "Epoch: [174][165/391]\tLoss 0.0220 (0.0145)\tPrec@1 99.219 (99.609)\n",
            "Epoch: [174][220/391]\tLoss 0.0078 (0.0146)\tPrec@1 100.000 (99.597)\n",
            "Epoch: [174][275/391]\tLoss 0.0090 (0.0148)\tPrec@1 100.000 (99.607)\n",
            "Epoch: [174][330/391]\tLoss 0.0033 (0.0150)\tPrec@1 100.000 (99.596)\n",
            "Epoch: [174][385/391]\tLoss 0.0105 (0.0150)\tPrec@1 100.000 (99.595)\n",
            "Test\t  Prec@1: 91.940 (Err: 8.060 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [175][0/391]\tLoss 0.0412 (0.0412)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [175][55/391]\tLoss 0.0235 (0.0154)\tPrec@1 99.219 (99.637)\n",
            "Epoch: [175][110/391]\tLoss 0.0097 (0.0150)\tPrec@1 100.000 (99.627)\n",
            "Epoch: [175][165/391]\tLoss 0.0086 (0.0147)\tPrec@1 100.000 (99.647)\n",
            "Epoch: [175][220/391]\tLoss 0.0117 (0.0146)\tPrec@1 100.000 (99.650)\n",
            "Epoch: [175][275/391]\tLoss 0.0108 (0.0145)\tPrec@1 100.000 (99.652)\n",
            "Epoch: [175][330/391]\tLoss 0.0230 (0.0148)\tPrec@1 98.438 (99.625)\n",
            "Epoch: [175][385/391]\tLoss 0.0065 (0.0148)\tPrec@1 100.000 (99.628)\n",
            "Test\t  Prec@1: 91.890 (Err: 8.110 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [176][0/391]\tLoss 0.0241 (0.0241)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [176][55/391]\tLoss 0.0147 (0.0152)\tPrec@1 100.000 (99.581)\n",
            "Epoch: [176][110/391]\tLoss 0.0063 (0.0166)\tPrec@1 100.000 (99.535)\n",
            "Epoch: [176][165/391]\tLoss 0.0275 (0.0165)\tPrec@1 99.219 (99.525)\n",
            "Epoch: [176][220/391]\tLoss 0.0114 (0.0159)\tPrec@1 100.000 (99.533)\n",
            "Epoch: [176][275/391]\tLoss 0.0170 (0.0158)\tPrec@1 99.219 (99.544)\n",
            "Epoch: [176][330/391]\tLoss 0.0247 (0.0156)\tPrec@1 99.219 (99.549)\n",
            "Epoch: [176][385/391]\tLoss 0.0100 (0.0158)\tPrec@1 100.000 (99.534)\n",
            "Test\t  Prec@1: 91.860 (Err: 8.140 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [177][0/391]\tLoss 0.0246 (0.0246)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [177][55/391]\tLoss 0.0106 (0.0144)\tPrec@1 100.000 (99.651)\n",
            "Epoch: [177][110/391]\tLoss 0.0068 (0.0140)\tPrec@1 100.000 (99.676)\n",
            "Epoch: [177][165/391]\tLoss 0.0065 (0.0145)\tPrec@1 100.000 (99.642)\n",
            "Epoch: [177][220/391]\tLoss 0.0179 (0.0150)\tPrec@1 100.000 (99.622)\n",
            "Epoch: [177][275/391]\tLoss 0.0265 (0.0148)\tPrec@1 99.219 (99.621)\n",
            "Epoch: [177][330/391]\tLoss 0.0071 (0.0143)\tPrec@1 100.000 (99.651)\n",
            "Epoch: [177][385/391]\tLoss 0.0076 (0.0142)\tPrec@1 100.000 (99.662)\n",
            "Test\t  Prec@1: 91.930 (Err: 8.070 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [178][0/391]\tLoss 0.0126 (0.0126)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [178][55/391]\tLoss 0.0078 (0.0126)\tPrec@1 100.000 (99.693)\n",
            "Epoch: [178][110/391]\tLoss 0.0561 (0.0134)\tPrec@1 98.438 (99.676)\n",
            "Epoch: [178][165/391]\tLoss 0.0103 (0.0142)\tPrec@1 99.219 (99.633)\n",
            "Epoch: [178][220/391]\tLoss 0.0077 (0.0145)\tPrec@1 100.000 (99.629)\n",
            "Epoch: [178][275/391]\tLoss 0.0059 (0.0151)\tPrec@1 100.000 (99.601)\n",
            "Epoch: [178][330/391]\tLoss 0.0056 (0.0145)\tPrec@1 100.000 (99.639)\n",
            "Epoch: [178][385/391]\tLoss 0.0177 (0.0144)\tPrec@1 100.000 (99.638)\n",
            "Test\t  Prec@1: 92.050 (Err: 7.950 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [179][0/391]\tLoss 0.0171 (0.0171)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [179][55/391]\tLoss 0.0082 (0.0153)\tPrec@1 100.000 (99.637)\n",
            "Epoch: [179][110/391]\tLoss 0.0098 (0.0141)\tPrec@1 100.000 (99.676)\n",
            "Epoch: [179][165/391]\tLoss 0.0021 (0.0138)\tPrec@1 100.000 (99.680)\n",
            "Epoch: [179][220/391]\tLoss 0.0118 (0.0139)\tPrec@1 99.219 (99.654)\n",
            "Epoch: [179][275/391]\tLoss 0.0190 (0.0139)\tPrec@1 100.000 (99.666)\n",
            "Epoch: [179][330/391]\tLoss 0.0287 (0.0146)\tPrec@1 99.219 (99.637)\n",
            "Epoch: [179][385/391]\tLoss 0.0302 (0.0143)\tPrec@1 98.438 (99.654)\n",
            "Test\t  Prec@1: 92.020 (Err: 7.980 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [180][0/391]\tLoss 0.0154 (0.0154)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [180][55/391]\tLoss 0.0087 (0.0148)\tPrec@1 100.000 (99.609)\n",
            "Epoch: [180][110/391]\tLoss 0.0191 (0.0146)\tPrec@1 99.219 (99.620)\n",
            "Epoch: [180][165/391]\tLoss 0.0115 (0.0147)\tPrec@1 100.000 (99.638)\n",
            "Epoch: [180][220/391]\tLoss 0.0211 (0.0153)\tPrec@1 99.219 (99.597)\n",
            "Epoch: [180][275/391]\tLoss 0.0128 (0.0152)\tPrec@1 99.219 (99.595)\n",
            "Epoch: [180][330/391]\tLoss 0.0163 (0.0150)\tPrec@1 99.219 (99.608)\n",
            "Epoch: [180][385/391]\tLoss 0.0123 (0.0149)\tPrec@1 100.000 (99.613)\n",
            "Test\t  Prec@1: 92.080 (Err: 7.920 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [181][0/391]\tLoss 0.0098 (0.0098)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [181][55/391]\tLoss 0.0314 (0.0147)\tPrec@1 98.438 (99.623)\n",
            "Epoch: [181][110/391]\tLoss 0.0120 (0.0164)\tPrec@1 99.219 (99.521)\n",
            "Epoch: [181][165/391]\tLoss 0.0226 (0.0154)\tPrec@1 99.219 (99.581)\n",
            "Epoch: [181][220/391]\tLoss 0.0221 (0.0155)\tPrec@1 99.219 (99.583)\n",
            "Epoch: [181][275/391]\tLoss 0.0087 (0.0156)\tPrec@1 100.000 (99.561)\n",
            "Epoch: [181][330/391]\tLoss 0.0145 (0.0156)\tPrec@1 99.219 (99.563)\n",
            "Epoch: [181][385/391]\tLoss 0.0133 (0.0157)\tPrec@1 100.000 (99.563)\n",
            "Test\t  Prec@1: 91.960 (Err: 8.040 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [182][0/391]\tLoss 0.0500 (0.0500)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [182][55/391]\tLoss 0.0076 (0.0162)\tPrec@1 100.000 (99.540)\n",
            "Epoch: [182][110/391]\tLoss 0.0156 (0.0149)\tPrec@1 99.219 (99.606)\n",
            "Epoch: [182][165/391]\tLoss 0.0078 (0.0148)\tPrec@1 100.000 (99.614)\n",
            "Epoch: [182][220/391]\tLoss 0.0402 (0.0145)\tPrec@1 98.438 (99.654)\n",
            "Epoch: [182][275/391]\tLoss 0.0059 (0.0143)\tPrec@1 100.000 (99.663)\n",
            "Epoch: [182][330/391]\tLoss 0.0069 (0.0142)\tPrec@1 100.000 (99.672)\n",
            "Epoch: [182][385/391]\tLoss 0.0432 (0.0140)\tPrec@1 97.656 (99.676)\n",
            "Test\t  Prec@1: 91.970 (Err: 8.030 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [183][0/391]\tLoss 0.0118 (0.0118)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [183][55/391]\tLoss 0.0037 (0.0146)\tPrec@1 100.000 (99.512)\n",
            "Epoch: [183][110/391]\tLoss 0.0133 (0.0143)\tPrec@1 100.000 (99.634)\n",
            "Epoch: [183][165/391]\tLoss 0.0174 (0.0146)\tPrec@1 99.219 (99.628)\n",
            "Epoch: [183][220/391]\tLoss 0.0094 (0.0142)\tPrec@1 100.000 (99.639)\n",
            "Epoch: [183][275/391]\tLoss 0.0094 (0.0142)\tPrec@1 100.000 (99.638)\n",
            "Epoch: [183][330/391]\tLoss 0.0246 (0.0140)\tPrec@1 99.219 (99.655)\n",
            "Epoch: [183][385/391]\tLoss 0.0092 (0.0142)\tPrec@1 100.000 (99.644)\n",
            "Test\t  Prec@1: 92.080 (Err: 7.920 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [184][0/391]\tLoss 0.0640 (0.0640)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [184][55/391]\tLoss 0.0234 (0.0145)\tPrec@1 100.000 (99.637)\n",
            "Epoch: [184][110/391]\tLoss 0.0078 (0.0139)\tPrec@1 100.000 (99.634)\n",
            "Epoch: [184][165/391]\tLoss 0.0086 (0.0141)\tPrec@1 100.000 (99.647)\n",
            "Epoch: [184][220/391]\tLoss 0.0127 (0.0139)\tPrec@1 100.000 (99.664)\n",
            "Epoch: [184][275/391]\tLoss 0.0046 (0.0146)\tPrec@1 100.000 (99.641)\n",
            "Epoch: [184][330/391]\tLoss 0.0050 (0.0146)\tPrec@1 100.000 (99.641)\n",
            "Epoch: [184][385/391]\tLoss 0.0303 (0.0147)\tPrec@1 98.438 (99.626)\n",
            "Test\t  Prec@1: 91.950 (Err: 8.050 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [185][0/391]\tLoss 0.0059 (0.0059)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [185][55/391]\tLoss 0.0210 (0.0142)\tPrec@1 99.219 (99.651)\n",
            "Epoch: [185][110/391]\tLoss 0.0167 (0.0133)\tPrec@1 100.000 (99.704)\n",
            "Epoch: [185][165/391]\tLoss 0.0159 (0.0143)\tPrec@1 100.000 (99.666)\n",
            "Epoch: [185][220/391]\tLoss 0.0053 (0.0140)\tPrec@1 100.000 (99.682)\n",
            "Epoch: [185][275/391]\tLoss 0.0082 (0.0138)\tPrec@1 100.000 (99.686)\n",
            "Epoch: [185][330/391]\tLoss 0.0392 (0.0141)\tPrec@1 97.656 (99.665)\n",
            "Epoch: [185][385/391]\tLoss 0.0088 (0.0139)\tPrec@1 99.219 (99.674)\n",
            "Test\t  Prec@1: 91.930 (Err: 8.070 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [186][0/391]\tLoss 0.0063 (0.0063)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [186][55/391]\tLoss 0.0269 (0.0137)\tPrec@1 99.219 (99.721)\n",
            "Epoch: [186][110/391]\tLoss 0.0122 (0.0136)\tPrec@1 100.000 (99.697)\n",
            "Epoch: [186][165/391]\tLoss 0.0174 (0.0140)\tPrec@1 100.000 (99.680)\n",
            "Epoch: [186][220/391]\tLoss 0.0084 (0.0142)\tPrec@1 100.000 (99.661)\n",
            "Epoch: [186][275/391]\tLoss 0.0023 (0.0142)\tPrec@1 100.000 (99.646)\n",
            "Epoch: [186][330/391]\tLoss 0.0177 (0.0142)\tPrec@1 100.000 (99.658)\n",
            "Epoch: [186][385/391]\tLoss 0.0129 (0.0147)\tPrec@1 99.219 (99.632)\n",
            "Test\t  Prec@1: 92.020 (Err: 7.980 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [187][0/391]\tLoss 0.0114 (0.0114)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [187][55/391]\tLoss 0.0086 (0.0149)\tPrec@1 100.000 (99.568)\n",
            "Epoch: [187][110/391]\tLoss 0.0219 (0.0156)\tPrec@1 98.438 (99.578)\n",
            "Epoch: [187][165/391]\tLoss 0.0230 (0.0154)\tPrec@1 99.219 (99.600)\n",
            "Epoch: [187][220/391]\tLoss 0.0053 (0.0146)\tPrec@1 100.000 (99.636)\n",
            "Epoch: [187][275/391]\tLoss 0.0035 (0.0144)\tPrec@1 100.000 (99.652)\n",
            "Epoch: [187][330/391]\tLoss 0.0180 (0.0148)\tPrec@1 99.219 (99.629)\n",
            "Epoch: [187][385/391]\tLoss 0.0162 (0.0149)\tPrec@1 100.000 (99.634)\n",
            "Test\t  Prec@1: 92.010 (Err: 7.990 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [188][0/391]\tLoss 0.0116 (0.0116)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [188][55/391]\tLoss 0.0143 (0.0148)\tPrec@1 100.000 (99.637)\n",
            "Epoch: [188][110/391]\tLoss 0.0072 (0.0138)\tPrec@1 100.000 (99.690)\n",
            "Epoch: [188][165/391]\tLoss 0.0065 (0.0148)\tPrec@1 100.000 (99.628)\n",
            "Epoch: [188][220/391]\tLoss 0.0187 (0.0149)\tPrec@1 99.219 (99.615)\n",
            "Epoch: [188][275/391]\tLoss 0.0183 (0.0147)\tPrec@1 99.219 (99.624)\n",
            "Epoch: [188][330/391]\tLoss 0.0110 (0.0145)\tPrec@1 99.219 (99.629)\n",
            "Epoch: [188][385/391]\tLoss 0.0124 (0.0145)\tPrec@1 100.000 (99.638)\n",
            "Test\t  Prec@1: 91.990 (Err: 8.010 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [189][0/391]\tLoss 0.0057 (0.0057)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [189][55/391]\tLoss 0.0041 (0.0131)\tPrec@1 100.000 (99.665)\n",
            "Epoch: [189][110/391]\tLoss 0.0090 (0.0147)\tPrec@1 100.000 (99.641)\n",
            "Epoch: [189][165/391]\tLoss 0.0157 (0.0152)\tPrec@1 100.000 (99.591)\n",
            "Epoch: [189][220/391]\tLoss 0.0132 (0.0148)\tPrec@1 99.219 (99.611)\n",
            "Epoch: [189][275/391]\tLoss 0.0059 (0.0150)\tPrec@1 100.000 (99.624)\n",
            "Epoch: [189][330/391]\tLoss 0.0051 (0.0148)\tPrec@1 100.000 (99.639)\n",
            "Epoch: [189][385/391]\tLoss 0.0111 (0.0146)\tPrec@1 99.219 (99.650)\n",
            "Test\t  Prec@1: 92.080 (Err: 7.920 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [190][0/391]\tLoss 0.0057 (0.0057)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [190][55/391]\tLoss 0.0074 (0.0125)\tPrec@1 100.000 (99.721)\n",
            "Epoch: [190][110/391]\tLoss 0.0110 (0.0118)\tPrec@1 100.000 (99.761)\n",
            "Epoch: [190][165/391]\tLoss 0.0081 (0.0128)\tPrec@1 100.000 (99.727)\n",
            "Epoch: [190][220/391]\tLoss 0.0104 (0.0132)\tPrec@1 100.000 (99.717)\n",
            "Epoch: [190][275/391]\tLoss 0.0144 (0.0131)\tPrec@1 100.000 (99.723)\n",
            "Epoch: [190][330/391]\tLoss 0.0193 (0.0133)\tPrec@1 100.000 (99.705)\n",
            "Epoch: [190][385/391]\tLoss 0.0066 (0.0133)\tPrec@1 100.000 (99.702)\n",
            "Test\t  Prec@1: 92.030 (Err: 7.970 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [191][0/391]\tLoss 0.0179 (0.0179)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [191][55/391]\tLoss 0.0152 (0.0165)\tPrec@1 100.000 (99.609)\n",
            "Epoch: [191][110/391]\tLoss 0.0173 (0.0145)\tPrec@1 99.219 (99.662)\n",
            "Epoch: [191][165/391]\tLoss 0.0314 (0.0144)\tPrec@1 98.438 (99.647)\n",
            "Epoch: [191][220/391]\tLoss 0.0278 (0.0150)\tPrec@1 99.219 (99.629)\n",
            "Epoch: [191][275/391]\tLoss 0.0225 (0.0145)\tPrec@1 99.219 (99.646)\n",
            "Epoch: [191][330/391]\tLoss 0.0248 (0.0144)\tPrec@1 98.438 (99.646)\n",
            "Epoch: [191][385/391]\tLoss 0.0047 (0.0142)\tPrec@1 100.000 (99.656)\n",
            "Test\t  Prec@1: 91.940 (Err: 8.060 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [192][0/391]\tLoss 0.0047 (0.0047)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [192][55/391]\tLoss 0.0123 (0.0143)\tPrec@1 100.000 (99.679)\n",
            "Epoch: [192][110/391]\tLoss 0.0080 (0.0136)\tPrec@1 100.000 (99.669)\n",
            "Epoch: [192][165/391]\tLoss 0.0116 (0.0138)\tPrec@1 100.000 (99.652)\n",
            "Epoch: [192][220/391]\tLoss 0.0125 (0.0137)\tPrec@1 100.000 (99.654)\n",
            "Epoch: [192][275/391]\tLoss 0.0192 (0.0137)\tPrec@1 98.438 (99.643)\n",
            "Epoch: [192][330/391]\tLoss 0.0104 (0.0135)\tPrec@1 100.000 (99.639)\n",
            "Epoch: [192][385/391]\tLoss 0.0024 (0.0133)\tPrec@1 100.000 (99.660)\n",
            "Test\t  Prec@1: 91.980 (Err: 8.020 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [193][0/391]\tLoss 0.0165 (0.0165)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [193][55/391]\tLoss 0.0076 (0.0139)\tPrec@1 100.000 (99.637)\n",
            "Epoch: [193][110/391]\tLoss 0.0193 (0.0144)\tPrec@1 99.219 (99.613)\n",
            "Epoch: [193][165/391]\tLoss 0.0285 (0.0140)\tPrec@1 98.438 (99.638)\n",
            "Epoch: [193][220/391]\tLoss 0.0257 (0.0141)\tPrec@1 99.219 (99.622)\n",
            "Epoch: [193][275/391]\tLoss 0.0325 (0.0142)\tPrec@1 99.219 (99.621)\n",
            "Epoch: [193][330/391]\tLoss 0.0157 (0.0143)\tPrec@1 99.219 (99.627)\n",
            "Epoch: [193][385/391]\tLoss 0.0123 (0.0145)\tPrec@1 100.000 (99.624)\n",
            "Test\t  Prec@1: 92.060 (Err: 7.940 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [194][0/391]\tLoss 0.0050 (0.0050)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [194][55/391]\tLoss 0.0073 (0.0138)\tPrec@1 100.000 (99.651)\n",
            "Epoch: [194][110/391]\tLoss 0.0070 (0.0137)\tPrec@1 100.000 (99.655)\n",
            "Epoch: [194][165/391]\tLoss 0.0121 (0.0137)\tPrec@1 99.219 (99.638)\n",
            "Epoch: [194][220/391]\tLoss 0.0120 (0.0136)\tPrec@1 100.000 (99.643)\n",
            "Epoch: [194][275/391]\tLoss 0.0143 (0.0136)\tPrec@1 99.219 (99.646)\n",
            "Epoch: [194][330/391]\tLoss 0.0165 (0.0135)\tPrec@1 100.000 (99.644)\n",
            "Epoch: [194][385/391]\tLoss 0.0099 (0.0135)\tPrec@1 100.000 (99.638)\n",
            "Test\t  Prec@1: 92.120 (Err: 7.880 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [195][0/391]\tLoss 0.0229 (0.0229)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [195][55/391]\tLoss 0.0179 (0.0138)\tPrec@1 99.219 (99.637)\n",
            "Epoch: [195][110/391]\tLoss 0.0107 (0.0143)\tPrec@1 100.000 (99.627)\n",
            "Epoch: [195][165/391]\tLoss 0.0101 (0.0144)\tPrec@1 99.219 (99.623)\n",
            "Epoch: [195][220/391]\tLoss 0.0077 (0.0142)\tPrec@1 100.000 (99.643)\n",
            "Epoch: [195][275/391]\tLoss 0.0136 (0.0138)\tPrec@1 100.000 (99.657)\n",
            "Epoch: [195][330/391]\tLoss 0.0050 (0.0138)\tPrec@1 100.000 (99.665)\n",
            "Epoch: [195][385/391]\tLoss 0.0171 (0.0140)\tPrec@1 100.000 (99.660)\n",
            "Test\t  Prec@1: 92.020 (Err: 7.980 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [196][0/391]\tLoss 0.0044 (0.0044)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [196][55/391]\tLoss 0.0106 (0.0166)\tPrec@1 100.000 (99.540)\n",
            "Epoch: [196][110/391]\tLoss 0.0124 (0.0147)\tPrec@1 100.000 (99.641)\n",
            "Epoch: [196][165/391]\tLoss 0.0033 (0.0146)\tPrec@1 100.000 (99.652)\n",
            "Epoch: [196][220/391]\tLoss 0.0167 (0.0141)\tPrec@1 100.000 (99.678)\n",
            "Epoch: [196][275/391]\tLoss 0.0081 (0.0137)\tPrec@1 100.000 (99.686)\n",
            "Epoch: [196][330/391]\tLoss 0.0335 (0.0141)\tPrec@1 99.219 (99.679)\n",
            "Epoch: [196][385/391]\tLoss 0.0238 (0.0141)\tPrec@1 99.219 (99.672)\n",
            "Test\t  Prec@1: 92.110 (Err: 7.890 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [197][0/391]\tLoss 0.0053 (0.0053)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [197][55/391]\tLoss 0.0050 (0.0115)\tPrec@1 100.000 (99.791)\n",
            "Epoch: [197][110/391]\tLoss 0.0098 (0.0135)\tPrec@1 100.000 (99.676)\n",
            "Epoch: [197][165/391]\tLoss 0.0105 (0.0141)\tPrec@1 100.000 (99.633)\n",
            "Epoch: [197][220/391]\tLoss 0.0030 (0.0138)\tPrec@1 100.000 (99.646)\n",
            "Epoch: [197][275/391]\tLoss 0.0237 (0.0139)\tPrec@1 99.219 (99.646)\n",
            "Epoch: [197][330/391]\tLoss 0.0107 (0.0136)\tPrec@1 100.000 (99.662)\n",
            "Epoch: [197][385/391]\tLoss 0.0102 (0.0134)\tPrec@1 100.000 (99.662)\n",
            "Test\t  Prec@1: 92.060 (Err: 7.940 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [198][0/391]\tLoss 0.0152 (0.0152)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [198][55/391]\tLoss 0.0298 (0.0140)\tPrec@1 99.219 (99.665)\n",
            "Epoch: [198][110/391]\tLoss 0.0110 (0.0143)\tPrec@1 100.000 (99.683)\n",
            "Epoch: [198][165/391]\tLoss 0.0070 (0.0142)\tPrec@1 100.000 (99.656)\n",
            "Epoch: [198][220/391]\tLoss 0.0182 (0.0136)\tPrec@1 99.219 (99.675)\n",
            "Epoch: [198][275/391]\tLoss 0.0118 (0.0140)\tPrec@1 100.000 (99.643)\n",
            "Epoch: [198][330/391]\tLoss 0.0090 (0.0139)\tPrec@1 100.000 (99.648)\n",
            "Epoch: [198][385/391]\tLoss 0.0051 (0.0137)\tPrec@1 100.000 (99.656)\n",
            "Test\t  Prec@1: 92.060 (Err: 7.940 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [199][0/391]\tLoss 0.0130 (0.0130)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [199][55/391]\tLoss 0.0294 (0.0124)\tPrec@1 98.438 (99.735)\n",
            "Epoch: [199][110/391]\tLoss 0.0079 (0.0131)\tPrec@1 100.000 (99.704)\n",
            "Epoch: [199][165/391]\tLoss 0.0108 (0.0135)\tPrec@1 100.000 (99.694)\n",
            "Epoch: [199][220/391]\tLoss 0.0032 (0.0140)\tPrec@1 100.000 (99.685)\n",
            "Epoch: [199][275/391]\tLoss 0.0074 (0.0139)\tPrec@1 100.000 (99.672)\n",
            "Epoch: [199][330/391]\tLoss 0.0220 (0.0139)\tPrec@1 99.219 (99.662)\n",
            "Epoch: [199][385/391]\tLoss 0.0096 (0.0139)\tPrec@1 100.000 (99.658)\n",
            "Test\t  Prec@1: 92.110 (Err: 7.890 )\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "model = ResNet(ResidualBlock, [3, 3, 3])\n",
        "model.cuda()\n",
        "\n",
        "# define loss function and pptimizer\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), 0.1,\n",
        "                                momentum=0.9,\n",
        "                                weight_decay=1e-4)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], last_epoch=0 - 1)\n",
        "\n",
        "for epoch in range(0, 200):\n",
        "\n",
        "  # train for one epoch\n",
        "\n",
        "  print('Training resnet20 model')\n",
        "  print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
        "  train(train_loader, model, criterion, optimizer, epoch)\n",
        "  lr_scheduler.step()\n",
        "\n",
        "  # evaluate on validation set\n",
        "  prec1 = validate(val_loader, model, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model"
      ],
      "metadata": {
        "id": "xmSuCemfAQAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06GIJ-_rkFbT"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/content/net2.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the model"
      ],
      "metadata": {
        "id": "DLnNlfQeAZqc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGtQv2XHOGS9",
        "outputId": "9375d0d9-44d5-4cae-fbf1-0721e1826a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test\t  Prec@1: 92.110 (Err: 7.890 )\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92.11"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model = ResNet(ResidualBlock, [3, 3, 3])\n",
        "model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model.load_state_dict(torch.load('/content/net2.pth'))\n",
        "model.eval()\n",
        "validate(val_loader, model, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получилось добиться точности в 92.11%"
      ],
      "metadata": {
        "id": "icFhSQNTAr8u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQnNhao6JlD1"
      },
      "source": [
        "# Quantized Resnet20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dldvEOWVJj8r"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "    \n",
        "    # return 3x3 Conv2d\n",
        "   \n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3,stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class QuantizedResidualBlock(nn.Module):\n",
        "    \n",
        "    # Initialize basic QuantizedResidualBlock with forward propogation\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None,quantized=True):\n",
        "        super(QuantizedResidualBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(out_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "        self.quantized = quantized\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        if self.quantized:\n",
        "            out = self.skip_add.add(out, residual)\n",
        "        else:\n",
        "            out += residual  \n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class QuantizedResNet(nn.Module):\n",
        "    \n",
        "    # Initialize  ResNet with forward propogation\n",
        "    \n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(QuantizedResNet, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv = conv3x3(3, 16)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
        "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
        "        self.avg_pool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "        # QuantStub converts tensors from floating point to quantized\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        # DeQuantStub converts tensors from quantized to floating point\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        \n",
        "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if (stride != 1) or (self.in_channels != out_channels):\n",
        "            downsample = nn.Sequential(\n",
        "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels))\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # tensors will be converted from floating point to quantized\n",
        "        out = self.quant(x)\n",
        "\n",
        "        out = self.conv(out)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # tensors will be converted from quantized to floating point\n",
        "        out = self.dequant(out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's look at the model"
      ],
      "metadata": {
        "id": "zdyBzfoWL5Yn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VtSOZKSA2hR",
        "outputId": "713783de-5e18-4ad7-d158-274ef16dd41b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): ResidualBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): ResidualBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): ResidualBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): ResidualBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
              "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ResNet(ResidualBlock, [3, 3, 3]).to('cuda')\n",
        "model.load_state_dict(torch.load('/content/net2.pth'))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's look at the quantized model"
      ],
      "metadata": {
        "id": "B11k34uPMHMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdX5a4kSJ5Pp",
        "outputId": "b1eaa6d9-d750-4365-d4f6-154ffd0ee1f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedResNet(\n",
              "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): QuantizedResidualBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (1): QuantizedResidualBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (2): QuantizedResidualBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): QuantizedResidualBlock(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (1): QuantizedResidualBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (2): QuantizedResidualBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): QuantizedResidualBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (1): QuantizedResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (2): QuantizedResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "  )\n",
              "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
              "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (quant): QuantStub()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model_quantized = QuantizedResNet(QuantizedResidualBlock, [3, 3, 3]).to('cpu')\n",
        "model_quantized.load_state_dict(torch.load('/content/net2.pth'))\n",
        "model_quantized.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post Training Static Quantization"
      ],
      "metadata": {
        "id": "-Df6WWV8MTs8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdfpGOEbJ8L7",
        "outputId": "f6fc70f7-90db-4fb5-efca-ff7d516bfbaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:178: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  reduce_range will be deprecated in a future release of PyTorch.\"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:1137: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
            "  Returning default scale and zero point \"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedResNet(\n",
              "  (conv): QuantizedConv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.08240721374750137, zero_point=56, padding=(1, 1))\n",
              "  (bn): Identity()\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): QuantizedResidualBlock(\n",
              "      (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.09511621296405792, zero_point=63, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.08836288005113602, zero_point=69, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (skip_add): QFunctional(\n",
              "        scale=0.11665356159210205, zero_point=52\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "      (dequant): DeQuantize()\n",
              "    )\n",
              "    (1): QuantizedResidualBlock(\n",
              "      (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.08194230496883392, zero_point=58, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.08847644925117493, zero_point=75, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (skip_add): QFunctional(\n",
              "        scale=0.12011585384607315, zero_point=49\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "      (dequant): DeQuantize()\n",
              "    )\n",
              "    (2): QuantizedResidualBlock(\n",
              "      (conv1): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.07156660407781601, zero_point=69, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.09074089676141739, zero_point=71, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (skip_add): QFunctional(\n",
              "        scale=0.10911647230386734, zero_point=38\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "      (dequant): DeQuantize()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): QuantizedResidualBlock(\n",
              "      (conv1): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.055965304374694824, zero_point=68, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.0604582205414772, zero_point=66, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (downsample): Sequential(\n",
              "        (0): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.08389363437891006, zero_point=54, padding=(1, 1))\n",
              "        (1): Identity()\n",
              "      )\n",
              "      (skip_add): QFunctional(\n",
              "        scale=0.0962001159787178, zero_point=60\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "      (dequant): DeQuantize()\n",
              "    )\n",
              "    (1): QuantizedResidualBlock(\n",
              "      (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.05516471341252327, zero_point=72, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.05735313147306442, zero_point=64, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (skip_add): QFunctional(\n",
              "        scale=0.09286379814147949, zero_point=43\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "      (dequant): DeQuantize()\n",
              "    )\n",
              "    (2): QuantizedResidualBlock(\n",
              "      (conv1): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.06967445462942123, zero_point=72, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.07233656197786331, zero_point=62, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (skip_add): QFunctional(\n",
              "        scale=0.10231941938400269, zero_point=39\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "      (dequant): DeQuantize()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): QuantizedResidualBlock(\n",
              "      (conv1): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.05785501003265381, zero_point=64, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.06639479100704193, zero_point=70, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (downsample): Sequential(\n",
              "        (0): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.06530023366212845, zero_point=65, padding=(1, 1))\n",
              "        (1): Identity()\n",
              "      )\n",
              "      (skip_add): QFunctional(\n",
              "        scale=0.09434260427951813, zero_point=66\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "      (dequant): DeQuantize()\n",
              "    )\n",
              "    (1): QuantizedResidualBlock(\n",
              "      (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.06586959213018417, zero_point=71, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.08256948739290237, zero_point=64, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (skip_add): QFunctional(\n",
              "        scale=0.10036157816648483, zero_point=55\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "      (dequant): DeQuantize()\n",
              "    )\n",
              "    (2): QuantizedResidualBlock(\n",
              "      (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.07035801559686661, zero_point=72, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.1820906400680542, zero_point=50, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (skip_add): QFunctional(\n",
              "        scale=0.207999587059021, zero_point=46\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "      (dequant): DeQuantize()\n",
              "    )\n",
              "  )\n",
              "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
              "  (fc): QuantizedLinear(in_features=64, out_features=10, scale=0.40752774477005005, zero_point=36, qscheme=torch.per_channel_affine)\n",
              "  (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)\n",
              "  (dequant): DeQuantize()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "#Layers to fuse\n",
        "modules_to_fuse = [['conv', 'bn'],\n",
        "                   ['layer1.0.conv1', 'layer1.0.bn1'],\n",
        "                   ['layer1.0.conv2', 'layer1.0.bn2'],\n",
        "                   ['layer1.1.conv1', 'layer1.1.bn1'],\n",
        "                   ['layer1.1.conv2', 'layer1.1.bn2'],\n",
        "                   ['layer1.2.conv1', 'layer1.2.bn1'],\n",
        "                   ['layer1.2.conv2', 'layer1.2.bn2'],\n",
        "                   \n",
        "                   ['layer2.0.conv1', 'layer2.0.bn1'],\n",
        "                   ['layer2.0.conv2', 'layer2.0.bn2'],\n",
        "                   ['layer2.0.downsample.0', 'layer2.0.downsample.1'],\n",
        "                   ['layer2.1.conv1', 'layer2.1.bn1'],\n",
        "                   ['layer2.1.conv2', 'layer2.1.bn2'],\n",
        "                   ['layer2.2.conv1', 'layer2.2.bn1'],\n",
        "                   ['layer2.2.conv2', 'layer2.2.bn2'],\n",
        "                   \n",
        "                   ['layer3.0.conv1', 'layer3.0.bn1'],\n",
        "                   ['layer3.0.conv2', 'layer3.0.bn2'],\n",
        "                   ['layer3.0.downsample.0', 'layer3.0.downsample.1'],\n",
        "                   ['layer3.1.conv1', 'layer3.1.bn1'],\n",
        "                   ['layer3.1.conv2', 'layer3.1.bn2'],\n",
        "                   ['layer3.2.conv1', 'layer3.2.bn1'],\n",
        "                   ['layer3.2.conv2', 'layer3.2.bn2']]\n",
        "\n",
        "# Fusing layers\n",
        "model_quantized = torch.quantization.fuse_modules(model_quantized, modules_to_fuse)\n",
        "# attach a global qconfig, which contains information about what kind of observers to attach\n",
        "model_quantized.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "# This inserts observers in the model that will observe activation tensors during calibration.\n",
        "torch.quantization.prepare(model_quantized, inplace=True)\n",
        "model_quantized.eval();\n",
        "\n",
        "for i, (images, labels) in enumerate(train_loader):\n",
        "    images = images.to('cpu')\n",
        "    model_quantized(images)\n",
        "# Finally, convert the observed model to a quantized model\n",
        "torch.quantization.convert(model_quantized, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to test accuracy"
      ],
      "metadata": {
        "id": "74BFe268R5XF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUN7jZN9cK_T"
      },
      "outputs": [],
      "source": [
        "def test(val_loader, model, criterion):\n",
        "    \n",
        "    # Run evaluation\n",
        "    \n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            target = target.to('cpu')\n",
        "            input_var = input.to('cpu')\n",
        "            target_var = target.to('cpu')\n",
        "\n",
        "            # compute output\n",
        "            output = model(input_var)\n",
        "            loss = criterion(output, target_var)\n",
        "\n",
        "            output = output.float()\n",
        "            loss = loss.float()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(output.data, target)[0]\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "\n",
        "    print('Test\\t  Prec@1: {top1.avg:.3f} (Err: {error:.3f} )\\n'\n",
        "          .format(top1=top1,error=100-top1.avg))\n",
        "\n",
        "    return top1.avg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the quantized model"
      ],
      "metadata": {
        "id": "VEOrhbHVR_Ku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-ciVMJuaEwu",
        "outputId": "171b5a79-88a9-4217-bed7-34e2f7ccef74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test\t  Prec@1: 91.820 (Err: 8.180 )\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91.82"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "#model_quantized.eval()\n",
        "test(val_loader, model_quantized, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Точность сжатой модели 91.82%. Это чуть хуже чем точность исходной модели (92.11%), но даже не смотря на это это высокая точность."
      ],
      "metadata": {
        "id": "5dkFCdokSQt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print size of the model"
      ],
      "metadata": {
        "id": "RFP5QxMLSS5l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDbonbVjd-pI"
      },
      "outputs": [],
      "source": [
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size of the model(MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGKidkkreFLL",
        "outputId": "79ffde02-968b-45d6-8148-3a1a0db4e1ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the model(MB): 0.359661\n",
            "Size of the model(MB): 1.219329\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print_size_of_model(model_quantized)\n",
        "print_size_of_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как мы можем увидеть размер сжатой модели 3,4 раза меньше размера исходной модели. Учитывая небольшую разницу в точности, можем сказать, что метод Post training static quantization успешно реализован."
      ],
      "metadata": {
        "id": "1bfXKzywTbKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build own PTQ"
      ],
      "metadata": {
        "id": "uQClChfjUlmq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua86iRX9SDXB"
      },
      "source": [
        "# Quantization functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7jpIPlbOF-6"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "QTensor = namedtuple('QTensor', ['tensor', 'scale', 'zero_point'])\n",
        "\n",
        "def calcScaleZeroPoint(min_val, max_val,num_bits=8):\n",
        "\n",
        "  # Calculate Scale and zero point of next \n",
        "\n",
        "  qmin = 0.\n",
        "  qmax = 2.**num_bits - 1.\n",
        "\n",
        "  scale = (max_val - min_val) / (qmax - qmin)\n",
        "\n",
        "  initial_zero_point = qmin - min_val / scale\n",
        "  \n",
        "  zero_point = 0\n",
        "  if initial_zero_point < qmin:\n",
        "      zero_point = qmin\n",
        "  elif initial_zero_point > qmax:\n",
        "      zero_point = qmax\n",
        "  else:\n",
        "      zero_point = initial_zero_point\n",
        "\n",
        "  zero_point = int(zero_point)\n",
        "\n",
        "  return scale, zero_point\n",
        "\n",
        "def quantize_tensor(x, num_bits=8, min_val=None, max_val=None):\n",
        "    \n",
        "    if not min_val and not max_val: \n",
        "      min_val, max_val = x.min(), x.max()\n",
        "\n",
        "    qmin = 0.\n",
        "    qmax = 2.**num_bits - 1.\n",
        "\n",
        "    scale, zero_point = calcScaleZeroPoint(min_val, max_val, num_bits)\n",
        "    q_x = zero_point + x / scale\n",
        "    q_x.clamp_(qmin, qmax).round_()\n",
        "    q_x = q_x.round().byte()\n",
        "    \n",
        "    return QTensor(tensor=q_x, scale=scale, zero_point=zero_point)\n",
        "\n",
        "def dequantize_tensor(q_x):\n",
        "    return q_x.scale * (q_x.tensor.float() - q_x.zero_point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVZc-jD4OF8E"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def quantizeLayer(x, layer, stat, scale_x, zp_x):\n",
        "\n",
        "  # cache old values\n",
        "  W = layer.weight.data\n",
        "  B = layer.bias.data\n",
        "\n",
        "  # quantise weights, activations are already quantised\n",
        "  w = quantize_tensor(layer.weight.data)\n",
        "  b = quantize_tensor(layer.bias.data)\n",
        "\n",
        "  layer.weight.data = w.tensor.float()\n",
        "  layer.bias.data = b.tensor.float()\n",
        "\n",
        "  # This is Quantisation\n",
        "  scale_w = w.scale\n",
        "  zp_w = w.zero_point\n",
        "  scale_b = b.scale\n",
        "  zp_b = b.zero_point\n",
        "  \n",
        "  scale_next, zero_point_next = calcScaleZeroPoint(min_val=stat['min'], max_val=stat['max'])\n",
        "\n",
        "  # Preparing input by shifting\n",
        "  X = x.float() - zp_x\n",
        "  layer.weight.data = scale_x * scale_w*(layer.weight.data - zp_w)\n",
        "  layer.bias.data = scale_b*(layer.bias.data + zp_b)\n",
        "\n",
        "  # All int computation\n",
        "  x = (layer(X)/ scale_next) + zero_point_next \n",
        "\n",
        "  # Reset weights for next forward pass\n",
        "  layer.weight.data = W\n",
        "  layer.bias.data = B\n",
        "  \n",
        "  return x, scale_next, zero_point_next"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGOlGWomTZhd"
      },
      "source": [
        "## Get Max and Min stats for quantising activations of Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDstN55uOF5J"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "# Get Min and max of x tensor, and stores it\n",
        "def updateStats(x, stats, key):\n",
        "  max_val, _ = torch.max(x, dim=1)\n",
        "  min_val, _ = torch.min(x, dim=1)\n",
        "  \n",
        "  \n",
        "  if key not in stats:\n",
        "    stats[key] = {\"max\": max_val.sum(), \"min\": min_val.sum(), \"total\": 1}\n",
        "  else:\n",
        "    stats[key]['max'] += max_val.sum().item()\n",
        "    stats[key]['min'] += min_val.sum().item()\n",
        "    stats[key]['total'] += 1\n",
        "  \n",
        "  return stats\n",
        "\n",
        "# Reworked Forward Pass to access activation Stats through updateStats function\n",
        "def gatherActivationStats(model, x, stats):\n",
        "\n",
        "  model_hooks = create_feature_extractor(model, return_nodes = ['conv', 'bn',\n",
        "                   'layer1.0.conv1', 'layer1.0.bn1',\n",
        "                   'layer1.0.conv2', 'layer1.0.bn2',\n",
        "                   'layer1.1.conv1', 'layer1.1.bn1',\n",
        "                   'layer1.1.conv2', 'layer1.1.bn2',\n",
        "                   'layer1.2.conv1', 'layer1.2.bn1',\n",
        "                   'layer1.2.conv2', 'layer1.2.bn2',\n",
        "                   \n",
        "                   'layer2.0.conv1', 'layer2.0.bn1',\n",
        "                   'layer2.0.conv2', 'layer2.0.bn2',\n",
        "                   'layer2.0.downsample.0', 'layer2.0.downsample.1',\n",
        "                   'layer2.1.conv1', 'layer2.1.bn1',\n",
        "                   'layer2.1.conv2', 'layer2.1.bn2',\n",
        "                   'layer2.2.conv1', 'layer2.2.bn1',\n",
        "                   'layer2.2.conv2', 'layer2.2.bn2',\n",
        "                   \n",
        "                   'layer3.0.conv1', 'layer3.0.bn1',\n",
        "                   'layer3.0.conv2', 'layer3.0.bn2',\n",
        "                   'layer3.0.downsample.0', 'layer3.0.downsample.1',\n",
        "                   'layer3.1.conv1', 'layer3.1.bn1',\n",
        "                   'layer3.1.conv2', 'layer3.1.bn2',\n",
        "                   'layer3.2.conv1', 'layer3.2.bn1',\n",
        "                   'layer3.2.conv2', 'layer3.2.bn2'])\n",
        "  \n",
        "  outputs = model_hooks(x)\n",
        "\n",
        "  for layer_name, out in outputs.items():\n",
        "    stats = updateStats(out.clone().view(out.shape[0], -1), stats, layer_name)\n",
        "\n",
        "  return stats\n",
        "\n",
        "# Entry function to get stats of all functions.\n",
        "def gatherStats(model, test_loader):\n",
        "    device = 'cuda'\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    stats = {}\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            stats = gatherActivationStats(model, data, stats)\n",
        "\n",
        "    final_stats = {}\n",
        "    for key, value in stats.items():\n",
        "      final_stats[key] = { \"max\" : value[\"max\"] / value[\"total\"], \"min\" : value[\"min\"] / value[\"total\"] }\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WLOLFldywsG"
      },
      "source": [
        "# Forward pass for quantised model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def quantForward(model, x, stats):\n",
        "  \n",
        "  # Quantise before inputting into incoming layers\n",
        "  x = quantize_tensor(x, min_val=stats['conv']['min'], max_val=stats['conv']['max'])\n",
        "\n",
        "  x1, scale_next, zero_point_next = quantizeLayer(x.tensor, model.conv, stats['bn'], x.scale, x.zero_point)\n",
        "\n",
        "  x1 = F.relu(x1)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x1, model.bn, stats['layer1.0.conv1'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[0].conv1, stats['layer1.0.bn1'],scale_next,zero_point_next)\n",
        "\n",
        "  x = F.relu(x)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[0].bn1, stats['layer1.0.conv2'],scale_next,zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[0].conv2, stats['layer1.0.bn2'],scale_next,zero_point_next)\n",
        "\n",
        "  x += x1 \n",
        "  x1 = F.relu(x)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x1, model.layer1[0].bn2, stats['layer1.1.conv1'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[1].conv1, stats['layer1.1.bn1'],scale_next,zero_point_next)\n",
        "\n",
        "  x = F.relu(x)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[1].bn1, stats['layer1.1.conv2'],scale_next,zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[1].conv2, stats['layer1.1.bn2'],scale_next,zero_point_next)\n",
        "\n",
        "  x += x1 \n",
        "  x1 = F.relu(x)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x1, model.layer1[1].bn2, stats['layer1.2.conv1'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[2].conv1, stats['layer1.2.bn1'], scale_next, zero_point_next)\n",
        "\n",
        "  x = F.relu(x)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[2].bn1, stats['layer1.2.conv2'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x,model.layer1[2].conv2, stats['layer1.2.bn2'], scale_next, zero_point_next)\n",
        "\n",
        "  x += x1 \n",
        "  x1 = F.relu(x)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x1, model.layer1[2].bn2, stats['layer2.0.conv1'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[0].conv1, stats['layer2.0.bn1'], scale_next, zero_point_next)\n",
        "\n",
        "  x = F.relu(x)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[0].bn1, stats['layer2.0.conv2'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[0].conv2, stats['layer2.0.bn2'], scale_next, zero_point_next)\n",
        "\n",
        "  x2, scale_next, zero_point_next = quantizeLayer(x, model.layer2[0].bn2, stats['layer2.0.downsample.0'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x1, model.layer2[0].downsample[0], stats['layer2.0.downsample.1'], scale_next, zero_point_next)\n",
        "\n",
        "  x2 += x \n",
        "  x1 = F.relu(x2)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x1, model.layer2[0].downsample[1], stats['layer2.1.conv1'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[1].conv1, stats['layer2.1.bn1'], scale_next, zero_point_next)\n",
        "\n",
        "  x = F.relu(x)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[1].bn1, stats['layer2.1.conv2'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[1].conv2, stats['layer2.1.bn2'], scale_next, zero_point_next)\n",
        "\n",
        "  x += x1 \n",
        "  x1 = F.relu(x)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x1, model.layer2[1].bn2, stats['layer2.2.conv1'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[2].conv1, stats['layer2.2.bn1'], scale_next, zero_point_next)\n",
        "\n",
        "  x = F.relu(x)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[2].bn1, stats['layer2.2.conv2'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[2].conv2, stats['layer2.2.bn2'], scale_next, zero_point_next)\n",
        "\n",
        "  x += x1 \n",
        "  x1 = F.relu(x)###\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x1, model.layer2[2].bn2, stats['layer3.0.conv1'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[0].conv1, stats['layer3.0.bn1'], scale_next, zero_point_next)\n",
        "\n",
        "  x = F.relu(x)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[0].bn1, stats['layer3.0.conv2'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[0].conv2, stats['layer3.0.bn2'], scale_next, zero_point_next)\n",
        "\n",
        "  x2, scale_next, zero_point_next = quantizeLayer(x, model.layer3[0].bn2, stats['layer3.0.downsample.0'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x1, model.layer3[0].downsample[0], stats['layer3.0.downsample.1'], scale_next, zero_point_next)\n",
        "\n",
        "  x2 += x \n",
        "  x1 = F.relu(x2)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x1, model.layer3[0].downsample[1], stats['layer3.1.conv1'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[1].conv1, stats['layer3.1.bn1'], scale_next, zero_point_next)\n",
        "\n",
        "  x = F.relu(x)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[1].bn1, stats['layer3.1.conv2'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[1].conv2, stats['layer3.1.bn2'], scale_next, zero_point_next)\n",
        "\n",
        "  x += x1 \n",
        "  x1 = F.relu(x)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x1, model.layer3[1].bn2, stats['layer3.2.conv1'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[2].conv1, stats['layer3.2.bn1'], scale_next, zero_point_next)\n",
        "\n",
        "  x = F.relu(x)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[2].bn1, stats['layer3.2.conv2'], scale_next, zero_point_next)\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[2].conv2, stats['layer3.2.bn2'], scale_next, zero_point_next)\n",
        "\n",
        "  x += x1 \n",
        "  x1 = F.relu(x)\n",
        "\n",
        "  x = model.avg_pool(x1)\n",
        "  x = x.view(x.size(0), -1)\n",
        "\n",
        "  x = dequantize_tensor(QTensor(tensor=x, scale=scale_next, zero_point=zero_point_next))\n",
        "\n",
        "  x = model.fc(x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "3PmMigrlkQgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test function"
      ],
      "metadata": {
        "id": "Ip3tbRVSqN_s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I13deLm4oy8V"
      },
      "outputs": [],
      "source": [
        "def testQ(val_loader, model, criterion, stats = None):\n",
        "    \n",
        "    # Run evaluation\n",
        "    \n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for input, target in val_loader:\n",
        "            \n",
        "            target = target.to('cuda')\n",
        "            input_var = input.to('cuda')\n",
        "            target_var = target.to('cuda')\n",
        "\n",
        "            output = quantForward(model, input_var, stats)\n",
        "            loss = criterion(output, target_var)\n",
        "\n",
        "            output = output.float()\n",
        "            loss = loss.float()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(output.data, target)[0]\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "\n",
        "    print('Test\\t  Prec@1: {top1.avg:.3f} (Err: {error:.3f} )\\n'\n",
        "          .format(top1=top1,error=100-top1.avg))\n",
        "\n",
        "    return top1.avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojiKa4MozGl2"
      },
      "source": [
        "# Get Stats"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для этого случая я обучал сеть с bias = True"
      ],
      "metadata": {
        "id": "A25lJuxfqeKM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URseN8p0T3HW",
        "outputId": "ee158d38-6683-4cea-9f05-74e829380e07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'conv': {'max': tensor(111635.2656, device='cuda:0'), 'min': tensor(-111302.9844, device='cuda:0'), 'total': 79}, 'bn': {'max': tensor(37936.5156, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'total': 79}, 'layer1.0.conv1': {'max': tensor(63662.1211, device='cuda:0'), 'min': tensor(-99796.0625, device='cuda:0'), 'total': 79}, 'layer1.0.bn1': {'max': tensor(35085.7578, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'total': 79}, 'layer1.0.conv2': {'max': tensor(57952.6562, device='cuda:0'), 'min': tensor(-78232.3516, device='cuda:0'), 'total': 79}, 'layer1.0.bn2': {'max': tensor(46384.4102, device='cuda:0'), 'min': tensor(-42265.9258, device='cuda:0'), 'total': 79}, 'layer1.1.conv1': {'max': tensor(95075.2031, device='cuda:0'), 'min': tensor(-161501.1875, device='cuda:0'), 'total': 79}, 'layer1.1.bn1': {'max': tensor(33537.0469, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'total': 79}, 'layer1.1.conv2': {'max': tensor(59611.9336, device='cuda:0'), 'min': tensor(-62910.1914, device='cuda:0'), 'total': 79}, 'layer1.1.bn2': {'max': tensor(45189.8438, device='cuda:0'), 'min': tensor(-41911.6016, device='cuda:0'), 'total': 79}, 'layer1.2.conv1': {'max': tensor(161781.4844, device='cuda:0'), 'min': tensor(-284137.2500, device='cuda:0'), 'total': 79}, 'layer1.2.bn1': {'max': tensor(34793.0742, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'total': 79}, 'layer1.2.conv2': {'max': tensor(79658.3359, device='cuda:0'), 'min': tensor(-50645.0586, device='cuda:0'), 'total': 79}, 'layer1.2.bn2': {'max': tensor(45362.9062, device='cuda:0'), 'min': tensor(-40668.2148, device='cuda:0'), 'total': 79}, 'layer2.0.conv1': {'max': tensor(104670.0391, device='cuda:0'), 'min': tensor(-153595.9688, device='cuda:0'), 'total': 79}, 'layer2.0.bn1': {'max': tensor(24828.6738, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'total': 79}, 'layer2.0.conv2': {'max': tensor(35809.3203, device='cuda:0'), 'min': tensor(-46466.7070, device='cuda:0'), 'total': 79}, 'layer2.0.bn2': {'max': tensor(32855.1680, device='cuda:0'), 'min': tensor(-30772.4902, device='cuda:0'), 'total': 79}, 'layer2.0.downsample.0': {'max': tensor(163009.0469, device='cuda:0'), 'min': tensor(-195144.1875, device='cuda:0'), 'total': 79}, 'layer2.0.downsample.1': {'max': tensor(45826.9375, device='cuda:0'), 'min': tensor(-40026.5195, device='cuda:0'), 'total': 79}, 'layer2.1.conv1': {'max': tensor(86480.8750, device='cuda:0'), 'min': tensor(-118681.5000, device='cuda:0'), 'total': 79}, 'layer2.1.bn1': {'max': tensor(23801.0742, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'total': 79}, 'layer2.1.conv2': {'max': tensor(28317.2227, device='cuda:0'), 'min': tensor(-31051.1992, device='cuda:0'), 'total': 79}, 'layer2.1.bn2': {'max': tensor(26037.5410, device='cuda:0'), 'min': tensor(-28736.2969, device='cuda:0'), 'total': 79}, 'layer2.2.conv1': {'max': tensor(92992.9453, device='cuda:0'), 'min': tensor(-123495.1719, device='cuda:0'), 'total': 79}, 'layer2.2.bn1': {'max': tensor(23823.1152, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'total': 79}, 'layer2.2.conv2': {'max': tensor(24928.8340, device='cuda:0'), 'min': tensor(-23324.9707, device='cuda:0'), 'total': 79}, 'layer2.2.bn2': {'max': tensor(29789.7559, device='cuda:0'), 'min': tensor(-29363.4473, device='cuda:0'), 'total': 79}, 'layer3.0.conv1': {'max': tensor(110739.6484, device='cuda:0'), 'min': tensor(-122495.5625, device='cuda:0'), 'total': 79}, 'layer3.0.bn1': {'max': tensor(24287.9883, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'total': 79}, 'layer3.0.conv2': {'max': tensor(34885.0625, device='cuda:0'), 'min': tensor(-41367.2500, device='cuda:0'), 'total': 79}, 'layer3.0.bn2': {'max': tensor(29605.3516, device='cuda:0'), 'min': tensor(-30675.5117, device='cuda:0'), 'total': 79}, 'layer3.0.downsample.0': {'max': tensor(128715.5938, device='cuda:0'), 'min': tensor(-111902.8281, device='cuda:0'), 'total': 79}, 'layer3.0.downsample.1': {'max': tensor(29679.3203, device='cuda:0'), 'min': tensor(-24473.7617, device='cuda:0'), 'total': 79}, 'layer3.1.conv1': {'max': tensor(70183.0391, device='cuda:0'), 'min': tensor(-83755.2344, device='cuda:0'), 'total': 79}, 'layer3.1.bn1': {'max': tensor(24872.9902, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'total': 79}, 'layer3.1.conv2': {'max': tensor(27161.7656, device='cuda:0'), 'min': tensor(-34883.0977, device='cuda:0'), 'total': 79}, 'layer3.1.bn2': {'max': tensor(31411.4141, device='cuda:0'), 'min': tensor(-41414.7148, device='cuda:0'), 'total': 79}, 'layer3.2.conv1': {'max': tensor(64615.4219, device='cuda:0'), 'min': tensor(-71894.8906, device='cuda:0'), 'total': 79}, 'layer3.2.bn1': {'max': tensor(26338.1230, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'total': 79}, 'layer3.2.conv2': {'max': tensor(25918.4980, device='cuda:0'), 'min': tensor(-18162., device='cuda:0'), 'total': 79}, 'layer3.2.bn2': {'max': tensor(89673.0234, device='cuda:0'), 'min': tensor(-53432.1289, device='cuda:0'), 'total': 79}}\n"
          ]
        }
      ],
      "source": [
        "model = ResNet(ResidualBlock, [3, 3, 3])\n",
        "model.to('cuda')\n",
        "model.load_state_dict(torch.load('/content/net_bias.pth'))\n",
        "stats = gatherStats(model, val_loader)\n",
        "print(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUI2K7zPzMq8"
      },
      "source": [
        "# Test quantised model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwZa_f5sT3EP",
        "outputId": "f1902f93-9b8d-4997-d363-d2c9b48944c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test\t  Prec@1: 10.000 (Err: 90.000 )\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "testQ(val_loader, model, criterion, stats=stats)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_size_of_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCTF0zo7e1d2",
        "outputId": "268d3474-b828-4c4b-99c2-53fc01fef195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the model(MB): 1.228759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Исходя из точности и веса модели можно сказать, что не удалось в достаточной мере реализовать собственный Post training static quantization. Скорее всего я сделал в коде глупую ошибку, которую просто не замечаю. Несмотря на это, полный pipeline задания выполнил."
      ],
      "metadata": {
        "id": "VnZCLmKcn84p"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ResNet20 quantized.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}